
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Internal learning site for AI, LLMs, and a Rivet chatbot capstone">
      
      
      
      
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.5.3, mkdocs-material-9.5.18">
    
    
      
        <title>python.langchain.com-docs-concepts-streaming - AI Product Engineer Course</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.66ac8b77.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../assets/css/progress.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
   <link href="../../assets/stylesheets/glightbox.min.css" rel="stylesheet"/><style>
    html.glightbox-open { overflow: initial; height: 100%; }
    .gslide-title { margin-top: 0px; user-select: text; }
    .gslide-desc { color: #666; user-select: text; }
    .gslide-image img { background: white; }
    .gscrollbar-fixer { padding-right: 15px; }
    .gdesc-inner { font-size: 0.75rem; }
    body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color);}
    body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color);}
    body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color);}</style> <script src="../../assets/javascripts/glightbox.min.js"></script></head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#pythonlangchaincom-docs-concepts-streaming" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="AI Product Engineer Course" class="md-header__button md-logo" aria-label="AI Product Engineer Course" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            AI Product Engineer Course
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              python.langchain.com-docs-concepts-streaming
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



  

<nav class="md-nav md-nav--primary md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="AI Product Engineer Course" class="md-nav__button md-logo" aria-label="AI Product Engineer Course" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    AI Product Engineer Course
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Overview
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Modules
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Modules
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../modules/foundations/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Foundations
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../modules/transformers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Transformers
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../modules/token-context/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Tokenization & Context
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../modules/prompting-structured-outputs/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Prompting & Structured Outputs
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../modules/streaming-ux/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Streaming UX
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../modules/multimodality/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Multimodality
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../modules/memory-state/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Memory & State
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../modules/rag/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Retrieval (RAG)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../modules/agents-orchestration/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Agents & Orchestration
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../modules/safety-security/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Safety & Security
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../modules/evaluation-observability/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Evaluation & Observability
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../modules/cost-latency/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Cost & Latency
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../modules/productization-mlops/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Productization & MLOps
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../modules/ai-ux-behavior/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    AI UX & Behavior
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../modules/collaboration-with-engineers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Collaboration with Engineers
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../modules/ecosystem-deep-dives/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Ecosystem Deep Dives
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Perspectives
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Perspectives
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../perspectives/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Overview
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../perspectives/llm-fundamentals/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LLM Fundamentals
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Sources
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Sources
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Overview
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Capstone (Rivet Bot)
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Capstone (Rivet Bot)
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../capstone/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Overview
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../capstone/rag/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    RAG & Knowledge
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../capstone/rivet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Rivet Flows
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../capstone/evaluation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Evaluation & Observability
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../capstone/safety/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Safety & Privacy
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Competitors
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            Competitors
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../competitors/fin-intercom/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Intercom Fin
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../competitors/yellow-ai/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Yellow.ai
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../news/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    News
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../glossary/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Glossary
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../progress/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Progress
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="pythonlangchaincom-docs-concepts-streaming">python.langchain.com-docs-concepts-streaming<a class="headerlink" href="#pythonlangchaincom-docs-concepts-streaming" title="Permanent link">&para;</a></h1>
<blockquote>
<p>Synthesis: TODO</p>
</blockquote>
<h1 id="streaming">Streaming<a class="headerlink" href="#streaming" title="Permanent link">&para;</a></h1>
<p><strong>Streaming</strong> is crucial for enhancing the responsiveness of applications built on LLMs. By displaying output progressively, even before a complete response is ready, streaming significantly improves user experience (UX), particularly when dealing with the latency of LLMs.</p>
<h2 id="overview">Overview<a class="headerlink" href="#overview" title="Permanent link">&para;</a></h2>
<p>Generating full responses from LLMs often incurs a delay of several seconds, which becomes more noticeable in complex applications with multiple model calls. Fortunately, LLMs generate responses iteratively, allowing for intermediate results to be displayed as they are produced. By streaming these intermediate outputs, LangChain enables smoother UX in LLM-powered apps and offers built-in support for streaming at the core of its design.
In this guide, we'll discuss streaming in LLM applications and explore how LangChain's streaming APIs facilitate real-time output from various components in your application.</p>
<h2 id="what-to-stream-in-llm-applications">What to stream in LLM applications<a class="headerlink" href="#what-to-stream-in-llm-applications" title="Permanent link">&para;</a></h2>
<p>In applications involving LLMs, several types of data can be streamed to improve user experience by reducing perceived latency and increasing transparency. These include:</p>
<h3 id="1-streaming-llm-outputs">1. Streaming LLM outputs<a class="headerlink" href="#1-streaming-llm-outputs" title="Permanent link">&para;</a></h3>
<p>The most common and critical data to stream is the output generated by the LLM itself. LLMs often take time to generate full responses, and by streaming the output in real-time, users can see partial results as they are produced. This provides immediate feedback and helps reduce the wait time for users.</p>
<h3 id="2-streaming-pipeline-or-workflow-progress">2. Streaming pipeline or workflow progress<a class="headerlink" href="#2-streaming-pipeline-or-workflow-progress" title="Permanent link">&para;</a></h3>
<h2 id="beyond-just-streaming-llm-output-its-useful-to-stream-progress-through-more-complex-workflows-or-pipelines-giving-users-a-sense-of-how-the-application-is-progressing-overall-this-could-include">Beyond just streaming LLM output, it’s useful to stream progress through more complex workflows or pipelines, giving users a sense of how the application is progressing overall. This could include:<a class="headerlink" href="#beyond-just-streaming-llm-output-its-useful-to-stream-progress-through-more-complex-workflows-or-pipelines-giving-users-a-sense-of-how-the-application-is-progressing-overall-this-could-include" title="Permanent link">&para;</a></h2>
<h2 id="in-langgraph-workflowswith-langgraph-workflows-are-composed-of-nodes-and-edges-that-represent-various-steps-streaming-here-involves-tracking-changes-to-the-graph-stateas-individual-nodesrequest-updates-this-allows-for-more-granular-monitoring-of-which-node-in-the-workflow-is-currently-active-giving-real-time-updates-about-the-status-of-the-workflow-as-it-progresses-through-different-stages"><strong>In LangGraph Workflows:</strong>With LangGraph, workflows are composed of nodes and edges that represent various steps. Streaming here involves tracking changes to the <strong>graph state</strong>as individual <strong>nodes</strong>request updates. This allows for more granular monitoring of which node in the workflow is currently active, giving real-time updates about the status of the workflow as it progresses through different stages.<a class="headerlink" href="#in-langgraph-workflowswith-langgraph-workflows-are-composed-of-nodes-and-edges-that-represent-various-steps-streaming-here-involves-tracking-changes-to-the-graph-stateas-individual-nodesrequest-updates-this-allows-for-more-granular-monitoring-of-which-node-in-the-workflow-is-currently-active-giving-real-time-updates-about-the-status-of-the-workflow-as-it-progresses-through-different-stages" title="Permanent link">&para;</a></h2>
<p><strong>In LCEL Pipelines:</strong>Streaming updates from an LCEL pipeline involves capturing progress from individual <strong>sub-runnables</strong>. For example, as different steps or components of the pipeline execute, you can stream which sub-runnable is currently running, providing real-time insight into the overall pipeline's progress.
Streaming pipeline or workflow progress is essential in providing users with a clear picture of where the application is in the execution process.</p>
<h3 id="3-streaming-custom-data">3. Streaming custom data<a class="headerlink" href="#3-streaming-custom-data" title="Permanent link">&para;</a></h3>
<p>In some cases, you may need to stream
<strong>custom data</strong> that goes beyond the information provided by the pipeline or workflow structure. This custom information is injected within a specific step in the workflow, whether that step is a tool or a LangGraph node. For example, you could stream updates about what a tool is doing in real-time or the progress through a LangGraph node. This granular data, which is emitted directly from within the step, provides more detailed insights into the execution of the workflow and is especially useful in complex processes where more visibility is needed.</p>
<h2 id="streaming-apis">Streaming APIs<a class="headerlink" href="#streaming-apis" title="Permanent link">&para;</a></h2>
<p>LangChain has two main APIs for streaming output in real-time. These APIs are supported by any component that implements the Runnable Interface, including LLMs, compiled LangGraph graphs, and any Runnable generated with LCEL.
- sync stream and async astream: Use to stream outputs from individual Runnables (e.g., a chat model) as they are generated or stream any workflow created with LangGraph.
- The async only astream_events: Use this API to get access to custom events and intermediate outputs from LLM applications built entirely with LCEL. Note that this API is available, but not needed when working with LangGraph.
In addition, there is a
<strong>legacy</strong> async astream_log API. This API is not recommended for new projects it is more complex and less feature-rich than the other streaming APIs.
stream() and
astream()
The
stream() method returns an iterator that yields chunks of output synchronously as they are produced. You can use a
for loop to process each chunk in real-time. For example, when using an LLM, this allows the output to be streamed incrementally as it is generated, reducing the wait time for users.
The type of chunk yielded by the
stream() and
astream() methods depends on the component being streamed. For example, when streaming from an LLM each component will be an AIMessageChunk; however, for other components, the chunk may be different.
The
stream() method returns an iterator that yields these chunks as they are produced. For example,
for chunk in component.stream(some_input):</p>
<h1 id="important-keep-the-processing-of-each-chunk-as-efficient-as-possible">IMPORTANT: Keep the processing of each chunk as efficient as possible.<a class="headerlink" href="#important-keep-the-processing-of-each-chunk-as-efficient-as-possible" title="Permanent link">&para;</a></h1>
<h1 id="while-youre-processing-the-current-chunk-the-upstream-component-is">While you're processing the current chunk, the upstream component is<a class="headerlink" href="#while-youre-processing-the-current-chunk-the-upstream-component-is" title="Permanent link">&para;</a></h1>
<h1 id="waiting-to-produce-the-next-one-for-example-if-working-with-langgraph">waiting to produce the next one. For example, if working with LangGraph,<a class="headerlink" href="#waiting-to-produce-the-next-one-for-example-if-working-with-langgraph" title="Permanent link">&para;</a></h1>
<h1 id="graph-execution-is-paused-while-the-current-chunk-is-being-processed">graph execution is paused while the current chunk is being processed.<a class="headerlink" href="#graph-execution-is-paused-while-the-current-chunk-is-being-processed" title="Permanent link">&para;</a></h1>
<h1 id="in-extreme-cases-this-could-even-result-in-timeouts-eg-when-llm-outputs-are">In extreme cases, this could even result in timeouts (e.g., when llm outputs are<a class="headerlink" href="#in-extreme-cases-this-could-even-result-in-timeouts-eg-when-llm-outputs-are" title="Permanent link">&para;</a></h1>
<h1 id="streamed-from-an-api-that-has-a-timeout">streamed from an API that has a timeout).<a class="headerlink" href="#streamed-from-an-api-that-has-a-timeout" title="Permanent link">&para;</a></h1>
<p>print(chunk)
The asynchronous version,
astream(), works similarly but is designed for non-blocking workflows. You can use it in asynchronous code to achieve the same real-time streaming behavior.</p>
<h4 id="usage-with-chat-models">Usage with chat models<a class="headerlink" href="#usage-with-chat-models" title="Permanent link">&para;</a></h4>
<p>When using
stream() or
astream() with chat models, the output is streamed as AIMessageChunks as it is generated by the LLM. This allows you to present or process the LLM's output incrementally as it's being produced, which is particularly useful in interactive applications or interfaces.</p>
<h4 id="usage-with-langgraph">Usage with LangGraph<a class="headerlink" href="#usage-with-langgraph" title="Permanent link">&para;</a></h4>
<p>LangGraph compiled graphs are Runnables and support the standard streaming APIs.
When using the
<em>stream</em> and <em>astream</em> methods with LangGraph, you can choose <strong>one or more</strong> streaming mode which allow you to control the type of output that is streamed. The available streaming modes are: <strong>"values"</strong>: Emit all values of the state for each step. <strong>"updates"</strong>: Emit only the node name(s) and updates that were returned by the node(s) after each step. <strong>"debug"</strong>: Emit debug events for each step. <strong>"messages"</strong>: Emit LLM messages token-by-token. <strong>"custom"</strong>: Emit custom output written using LangGraph's StreamWriter.
For more information, please see:
- LangGraph streaming conceptual guide for more information on how to stream when working with LangGraph.
- LangGraph streaming how-to guides for specific examples of streaming in LangGraph.</p>
<h4 id="usage-with-lcel">Usage with LCEL<a class="headerlink" href="#usage-with-lcel" title="Permanent link">&para;</a></h4>
<p>If you compose multiple Runnables using LangChain’s Expression Language (LCEL), the
stream() and
astream() methods will, by convention, stream the output of the last step in the chain. This allows the final processed result to be streamed incrementally.
<strong>LCEL</strong> tries to optimize streaming latency in pipelines so that the streaming results from the last step are available as soon as possible.
astream_events
Use the
astream_events API to access custom data and intermediate outputs from LLM applications built entirely with LCEL.
While this API is available for use with LangGraph as well, it is usually not necessary when working with LangGraph, as the
stream and
astream methods provide comprehensive streaming capabilities for LangGraph graphs.
For chains constructed using
<strong>LCEL</strong>, the
.stream() method only streams the output of the final step from the chain. This might be sufficient for some applications, but as you build more complex chains of several LLM calls together, you may want to use the intermediate values of the chain alongside the final output. For example, you may want to return sources alongside the final generation when building a chat-over-documents app.
There are ways to do this using callbacks, or by constructing your chain in such a way that it passes intermediate
values to the end with something like chained
.assign() calls, but LangChain also includes an
.astream_events() method that combines the flexibility of callbacks with the ergonomics of
.stream(). When called, it returns an iterator
which yields various types of events that you can filter and process according
to the needs of your project.
Here's one small example that prints just events containing streamed chat model output:
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_anthropic import ChatAnthropic
model = ChatAnthropic(model="claude-3-7-sonnet-20250219")
prompt = ChatPromptTemplate.from_template("tell me a joke about {topic}")
parser = StrOutputParser()
chain = prompt | model | parser
async for event in chain.astream_events({"topic": "parrot"}):
kind = event["event"]
if kind == "on_chat_model_stream":
print(event, end="|", flush=True)
<strong>API Reference:</strong>StrOutputParser | ChatPromptTemplate
You can roughly think of it as an iterator over callback events (though the format differs) - and you can use it on almost all LangChain components!
See this guide for more detailed information on how to use
.astream_events(), including a table listing available events.</p>
<h2 id="writing-custom-data-to-the-stream">Writing custom data to the stream<a class="headerlink" href="#writing-custom-data-to-the-stream" title="Permanent link">&para;</a></h2>
<p>To write custom data to the stream, you will need to choose one of the following methods based on the component you are working with:
- LangGraph's StreamWriter can be used to write custom data that will surface through
<strong>stream</strong>and <strong>astream</strong>APIs when working with LangGraph. <strong>Important</strong>this is a LangGraph feature, so it is not available when working with pure LCEL. See how to streaming custom data for more information.
- dispatch_events / adispatch_events can be used to write custom data that will be surfaced through the
<strong>astream_events</strong>API. See how to dispatch custom callback events for more information.</p>
<h2 id="auto-streaming-chat-models">"Auto-Streaming" Chat Models<a class="headerlink" href="#auto-streaming-chat-models" title="Permanent link">&para;</a></h2>
<p>LangChain simplifies streaming from chat models by automatically enabling streaming mode in certain cases, even when you’re not explicitly calling the streaming methods. This is particularly useful when you use the non-streaming
invoke method but still want to stream the entire application, including intermediate results from the chat model.</p>
<h3 id="how-it-works">How It Works<a class="headerlink" href="#how-it-works" title="Permanent link">&para;</a></h3>
<p>When you call the
invoke (or
ainvoke) method on a chat model, LangChain will automatically switch to streaming mode if it detects that you are trying to stream the overall application.
Under the hood, it'll have
invoke (or
ainvoke) use the
stream (or
astream) method to generate its output. The result of the invocation will be the same as far as the code that was using
invoke is concerned; however, while the chat model is being streamed, LangChain will take care of invoking
on_llm_new_token events in LangChain's callback system. These callback events
allow LangGraph
stream/
astream and
astream_events to surface the chat model's output in real-time.
Example:
def node(state):
...</p>
<h1 id="the-code-below-uses-the-invoke-method-but-langchain-will">The code below uses the invoke method, but LangChain will<a class="headerlink" href="#the-code-below-uses-the-invoke-method-but-langchain-will" title="Permanent link">&para;</a></h1>
<h1 id="automatically-switch-to-streaming-mode">automatically switch to streaming mode<a class="headerlink" href="#automatically-switch-to-streaming-mode" title="Permanent link">&para;</a></h1>
<h1 id="when-it-detects-that-the-overall">when it detects that the overall<a class="headerlink" href="#when-it-detects-that-the-overall" title="Permanent link">&para;</a></h1>
<h1 id="application-is-being-streamed">application is being streamed.<a class="headerlink" href="#application-is-being-streamed" title="Permanent link">&para;</a></h1>
<p>ai_message = model.invoke(state["messages"])
...
for chunk in compiled_graph.stream(..., mode="messages"):
...</p>
<h2 id="async-programming">Async Programming<a class="headerlink" href="#async-programming" title="Permanent link">&para;</a></h2>
<p>LangChain offers both synchronous (sync) and asynchronous (async) versions of many of its methods. The async methods are typically prefixed with an "a" (e.g.,
ainvoke,
astream). When writing async code, it's crucial to consistently use these asynchronous methods to ensure non-blocking behavior and optimal performance.
If streaming data fails to appear in real-time, please ensure that you are using the correct async methods for your workflow.
Please review the async programming in LangChain guide for more information on writing async code with LangChain.</p>
<h2 id="related-resources">Related Resources<a class="headerlink" href="#related-resources" title="Permanent link">&para;</a></h2>
<p>Please see the following how-to guides for specific examples of streaming in LangChain:
- LangGraph conceptual guide on streaming
- LangGraph streaming how-to guides
- How to stream runnables: This how-to guide goes over common streaming patterns with LangChain components (e.g., chat models) and with LCEL.
- How to stream chat models
- How to stream tool calls
For writing custom data to the stream, please see the following resources:
- If using LangGraph, see how to stream custom data.
- If using LCEL, see how to dispatch custom callback events.</p>







  
    
  
  


  <aside class="md-source-file">
    
      
  <span class="md-source-file__fact">
    <span class="md-icon" title="Last update">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1-2.1-2M12.5 7v5.2l4 2.4-1 1L11 13V7h1.5M11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2v1.8Z"/></svg>
    </span>
    <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date">August 17, 2025</span>
  </span>

    
    
    
    
  </aside>





                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.top", "navigation.sections", "content.code.copy", "toc.integrate"], "search": "../../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.3220b9d7.min.js"></script>
      
        <script src="../../assets/js/progress.js"></script>
      
    
  <script id="init-glightbox">const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(() => { lightbox.reload() });
</script></body>
</html>