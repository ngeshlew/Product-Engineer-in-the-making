
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Internal learning site for AI, LLMs, and a Rivet chatbot capstone">
      
      
      
      
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.5.3, mkdocs-material-9.5.18">
    
    
      
        <title>langchain-ai.github.io-langgraph-how-tos-streaming - AI Product Engineer Course</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.66ac8b77.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../assets/css/progress.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
   <link href="../../assets/stylesheets/glightbox.min.css" rel="stylesheet"/><style>
    html.glightbox-open { overflow: initial; height: 100%; }
    .gslide-title { margin-top: 0px; user-select: text; }
    .gslide-desc { color: #666; user-select: text; }
    .gslide-image img { background: white; }
    .gscrollbar-fixer { padding-right: 15px; }
    .gdesc-inner { font-size: 0.75rem; }
    body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color);}
    body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color);}
    body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color);}</style> <script src="../../assets/javascripts/glightbox.min.js"></script></head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#langchain-aigithubio-langgraph-how-tos-streaming" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="AI Product Engineer Course" class="md-header__button md-logo" aria-label="AI Product Engineer Course" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            AI Product Engineer Course
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              langchain-ai.github.io-langgraph-how-tos-streaming
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



  

<nav class="md-nav md-nav--primary md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="AI Product Engineer Course" class="md-nav__button md-logo" aria-label="AI Product Engineer Course" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    AI Product Engineer Course
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Overview
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Modules
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Modules
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../modules/foundations/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Foundations
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../modules/transformers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Transformers
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../modules/token-context/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Tokenization & Context
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../modules/prompting-structured-outputs/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Prompting & Structured Outputs
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../modules/streaming-ux/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Streaming UX
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../modules/multimodality/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Multimodality
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../modules/memory-state/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Memory & State
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../modules/rag/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Retrieval (RAG)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../modules/agents-orchestration/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Agents & Orchestration
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../modules/safety-security/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Safety & Security
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../modules/evaluation-observability/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Evaluation & Observability
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../modules/cost-latency/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Cost & Latency
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../modules/productization-mlops/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Productization & MLOps
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../modules/ai-ux-behavior/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    AI UX & Behavior
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../modules/collaboration-with-engineers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Collaboration with Engineers
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../modules/ecosystem-deep-dives/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Ecosystem Deep Dives
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Perspectives
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Perspectives
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../perspectives/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Overview
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../perspectives/llm-fundamentals/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LLM Fundamentals
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Sources
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Sources
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Overview
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Capstone (Rivet Bot)
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Capstone (Rivet Bot)
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../capstone/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Overview
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../capstone/rag/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    RAG & Knowledge
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../capstone/rivet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Rivet Flows
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../capstone/evaluation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Evaluation & Observability
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../capstone/safety/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Safety & Privacy
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../capstone/demo/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Demo
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Competitors
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            Competitors
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../competitors/fin-intercom/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Intercom Fin
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../competitors/yellow-ai/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Yellow.ai
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../news/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    News
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../glossary/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Glossary
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../progress/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Progress
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="langchain-aigithubio-langgraph-how-tos-streaming">langchain-ai.github.io-langgraph-how-tos-streaming<a class="headerlink" href="#langchain-aigithubio-langgraph-how-tos-streaming" title="Permanent link">&para;</a></h1>
<blockquote>
<p>Synthesis: TODO</p>
</blockquote>
<h1 id="stream-outputs">Stream outputs¶<a class="headerlink" href="#stream-outputs" title="Permanent link">&para;</a></h1>
<p>You can stream outputs from a LangGraph agent or workflow.</p>
<h2 id="supported-stream-modes">Supported stream modes¶<a class="headerlink" href="#supported-stream-modes" title="Permanent link">&para;</a></h2>
<p>Pass one or more of the following stream modes as a list to the
stream() or
astream() methods:
|Mode
|Description
|
values
|Streams the full value of the state after each step of the graph.
|
updates
|Streams the updates to the state after each step of the graph. If multiple updates are made in the same step (e.g., multiple nodes are run), those updates are streamed separately.
|
custom
|Streams custom data from inside your graph nodes.
|
messages
|Streams 2-tuples (LLM token, metadata) from any graph nodes where an LLM is invoked.
|
debug
|Streams as much information as possible throughout the execution of the graph.</p>
<h2 id="stream-from-an-agent">Stream from an agent¶<a class="headerlink" href="#stream-from-an-agent" title="Permanent link">&para;</a></h2>
<h3 id="agent-progress">Agent progress¶<a class="headerlink" href="#agent-progress" title="Permanent link">&para;</a></h3>
<p>To stream agent progress, use the
stream() or
astream() methods with
stream_mode="updates". This emits an event after every agent step.
For example, if you have an agent that calls a tool once, you should see the following updates:
<strong>LLM node</strong>: AI message with tool call requests <strong>Tool node</strong>: Tool message with execution result <strong>LLM node</strong>: Final AI response</p>
<h3 id="llm-tokens">LLM tokens¶<a class="headerlink" href="#llm-tokens" title="Permanent link">&para;</a></h3>
<p>To stream tokens as they are produced by the LLM, use
stream_mode="messages":
agent = create_react_agent(
model="anthropic:claude-3-7-sonnet-latest",
tools=[get_weather],
)
async for token, metadata in agent.astream(
{"messages": [{"role": "user", "content": "what is the weather in sf"}]},
stream_mode="messages"
):
print("Token", token)
print("Metadata", metadata)
print("\n")</p>
<h3 id="tool-updates">Tool updates¶<a class="headerlink" href="#tool-updates" title="Permanent link">&para;</a></h3>
<p>To stream updates from tools as they are executed, you can use get_stream_writer.
from langgraph.config import get_stream_writer
def get_weather(city: str) -&gt; str:
"""Get weather for a given city."""
writer = get_stream_writer()</p>
<h1 id="stream-any-arbitrary-data">stream any arbitrary data<a class="headerlink" href="#stream-any-arbitrary-data" title="Permanent link">&para;</a></h1>
<p>writer(f"Looking up data for city: {city}")
return f"It's always sunny in {city}!"
agent = create_react_agent(
model="anthropic:claude-3-7-sonnet-latest",
tools=[get_weather],
)
for chunk in agent.stream(
{"messages": [{"role": "user", "content": "what is the weather in sf"}]},
stream_mode="custom"
):
print(chunk)
print("\n")
from langgraph.config import get_stream_writer
def get_weather(city: str) -&gt; str:
"""Get weather for a given city."""
writer = get_stream_writer()</p>
<h1 id="stream-any-arbitrary-data_1">stream any arbitrary data<a class="headerlink" href="#stream-any-arbitrary-data_1" title="Permanent link">&para;</a></h1>
<p>writer(f"Looking up data for city: {city}")
return f"It's always sunny in {city}!"
agent = create_react_agent(
model="anthropic:claude-3-7-sonnet-latest",
tools=[get_weather],
)
async for chunk in agent.astream(
{"messages": [{"role": "user", "content": "what is the weather in sf"}]},
stream_mode="custom"
):
print(chunk)
print("\n")
Note
If you add
get_stream_writer inside your tool, you won't be able to invoke the tool outside of a LangGraph execution context.</p>
<h3 id="stream-multiple-modes">Stream multiple modes¶<a class="headerlink" href="#stream-multiple-modes" title="Permanent link">&para;</a></h3>
<p>You can specify multiple streaming modes by passing stream mode as a list:
stream_mode=["updates", "messages", "custom"]:</p>
<h3 id="disable-streaming">Disable streaming¶<a class="headerlink" href="#disable-streaming" title="Permanent link">&para;</a></h3>
<p>In some applications you might need to disable streaming of individual tokens for a given model. This is useful in multi-agent systems to control which agents stream their output.
See the Models guide to learn how to disable streaming.</p>
<h2 id="stream-from-a-workflow">Stream from a workflow¶<a class="headerlink" href="#stream-from-a-workflow" title="Permanent link">&para;</a></h2>
<h3 id="basic-usage-example">Basic usage example¶<a class="headerlink" href="#basic-usage-example" title="Permanent link">&para;</a></h3>
<p>LangGraph graphs expose the
.stream() (sync) and
.astream() (async) methods to yield streamed outputs as iterators.</p>
<h2 id="extended-example-streaming-updates">Extended example: streaming updates<a class="headerlink" href="#extended-example-streaming-updates" title="Permanent link">&para;</a></h2>
<p>from typing import TypedDict
from langgraph.graph import StateGraph, START, END
class State(TypedDict):
topic: str
joke: str
def refine_topic(state: State):
return {"topic": state["topic"] + " and cats"}
def generate_joke(state: State):
return {"joke": f"This is a joke about {state['topic']}"}
graph = (
StateGraph(State)
.add_node(refine_topic)
.add_node(generate_joke)
.add_edge(START, "refine_topic")
.add_edge("refine_topic", "generate_joke")
.add_edge("generate_joke", END)
.compile()
)
for chunk in graph.stream( # (1)!
{"topic": "ice cream"},
stream_mode="updates", # (2)!
):
print(chunk)
- The
stream()method returns an iterator that yields streamed outputs.
- Set
stream_mode="updates"to stream only the updates to the graph state after each node. Other stream modes are also available. See supported stream modes for details.
output
{'refineTopic': {'topic': 'ice cream and cats'}}
{'generateJoke': {'joke': 'This is a joke about ice cream and cats'}} |</p>
<h3 id="stream-multiple-modes_1">Stream multiple modes¶<a class="headerlink" href="#stream-multiple-modes_1" title="Permanent link">&para;</a></h3>
<p>You can pass a list as the
stream_mode parameter to stream multiple modes at once.
The streamed outputs will be tuples of
(mode, chunk) where
mode is the name of the stream mode and
chunk is the data streamed by that mode.</p>
<h3 id="stream-graph-state">Stream graph state¶<a class="headerlink" href="#stream-graph-state" title="Permanent link">&para;</a></h3>
<p>Use the stream modes
updates and
values to stream the state of the graph as it executes.
updatesstreams the
<strong>updates</strong>to the state after each step of the graph.
valuesstreams the
<strong>full value</strong>of the state after each step of the graph. <em>API Reference: StateGraph | START | END</em>
from typing import TypedDict
from langgraph.graph import StateGraph, START, END
class State(TypedDict):
topic: str
joke: str
def refine_topic(state: State):
return {"topic": state["topic"] + " and cats"}
def generate_joke(state: State):
return {"joke": f"This is a joke about {state['topic']}"}
graph = (
StateGraph(State)
.add_node(refine_topic)
.add_node(generate_joke)
.add_edge(START, "refine_topic")
.add_edge("refine_topic", "generate_joke")
.add_edge("generate_joke", END)
.compile()
)
Use this to stream only the
<strong>state updates</strong> returned by the nodes after each step. The streamed outputs include the name of the node as well as the update.</p>
<h3 id="stream-subgraph-outputs">Stream subgraph outputs¶<a class="headerlink" href="#stream-subgraph-outputs" title="Permanent link">&para;</a></h3>
<p>To include outputs from subgraphs in the streamed outputs, you can set
subgraphs=True in the
.stream() method of the parent graph. This will stream outputs from both the parent graph and any subgraphs.
The outputs will be streamed as tuples
(namespace, data), where
namespace is a tuple with the path to the node where a subgraph is invoked, e.g.
("parent_node:<task_id>", "child_node:<task_id>").
for chunk in graph.stream(
{"foo": "foo"},
subgraphs=True, # (1)!
stream_mode="updates",
):
print(chunk)
- Set
subgraphs=Trueto stream outputs from subgraphs.</p>
<h2 id="extended-example-streaming-from-subgraphs">Extended example: streaming from subgraphs<a class="headerlink" href="#extended-example-streaming-from-subgraphs" title="Permanent link">&para;</a></h2>
<p>from langgraph.graph import START, StateGraph
from typing import TypedDict</p>
<h1 id="define-subgraph">Define subgraph<a class="headerlink" href="#define-subgraph" title="Permanent link">&para;</a></h1>
<p>class SubgraphState(TypedDict):
foo: str # note that this key is shared with the parent graph state
bar: str
def subgraph_node_1(state: SubgraphState):
return {"bar": "bar"}
def subgraph_node_2(state: SubgraphState):
return {"foo": state["foo"] + state["bar"]}
subgraph_builder = StateGraph(SubgraphState)
subgraph_builder.add_node(subgraph_node_1)
subgraph_builder.add_node(subgraph_node_2)
subgraph_builder.add_edge(START, "subgraph_node_1")
subgraph_builder.add_edge("subgraph_node_1", "subgraph_node_2")
subgraph = subgraph_builder.compile()</p>
<h1 id="define-parent-graph">Define parent graph<a class="headerlink" href="#define-parent-graph" title="Permanent link">&para;</a></h1>
<p>class ParentState(TypedDict):
foo: str
def node_1(state: ParentState):
return {"foo": "hi! " + state["foo"]}
builder = StateGraph(ParentState)
builder.add_node("node_1", node_1)
builder.add_node("node_2", subgraph)
builder.add_edge(START, "node_1")
builder.add_edge("node_1", "node_2")
graph = builder.compile()
for chunk in graph.stream(
{"foo": "foo"},
stream_mode="updates",
subgraphs=True, # (1)!
):
print(chunk)
- Set
subgraphs=Trueto stream outputs from subgraphs.
((), {'node_1': {'foo': 'hi! foo'}})
(('node_2:dfddc4ba-c3c5-6887-5012-a243b5b377c2',), {'subgraph_node_1': {'bar': 'bar'}})
(('node_2:dfddc4ba-c3c5-6887-5012-a243b5b377c2',), {'subgraph_node_2': {'foo': 'hi! foobar'}})
((), {'node_2': {'foo': 'hi! foobar'}})
<strong>Note</strong> that we are receiving not just the node updates, but we also the namespaces which tell us what graph (or subgraph) we are streaming from.</p>
<h3 id="debugging">Debugging¶<a class="headerlink" href="#debugging" title="Permanent link">&para;</a></h3>
<p>Use the
debug streaming mode to stream as much information as possible throughout the execution of the graph. The streamed outputs include the name of the node as well as the full state.</p>
<h3 id="llm-tokens_1">LLM tokens¶<a class="headerlink" href="#llm-tokens_1" title="Permanent link">&para;</a></h3>
<p>Use the
messages streaming mode to stream Large Language Model (LLM) outputs
<strong>token by token</strong> from any part of your graph, including nodes, tools, subgraphs, or tasks.
The streamed output from
messages mode is a tuple
(message_chunk, metadata) where:
message_chunk: the token or message segment from the LLM.
metadata: a dictionary containing details about the graph node and LLM invocation.
If your LLM is not available as a LangChain integration, you can stream its outputs using
custommode instead. See use with any LLM for details.
Manual config required for async in Python &lt; 3.11
When using Python &lt; 3.11 with async code, you must explicitly pass
RunnableConfig to
ainvoke() to enable proper streaming. See Async with Python &lt; 3.11 for details or upgrade to Python 3.11+.
<em>API Reference: init_chat_model | StateGraph | START</em>
from dataclasses import dataclass
from langchain.chat_models import init_chat_model
from langgraph.graph import StateGraph, START
@dataclass
class MyState:
topic: str
joke: str = ""
llm = init_chat_model(model="openai:gpt-4o-mini")
def call_model(state: MyState):
"""Call the LLM to generate a joke about a topic"""
llm_response = llm.invoke( # (1)!
[
{"role": "user", "content": f"Generate a joke about {state.topic}"}
]
)
return {"joke": llm_response.content}
graph = (
StateGraph(MyState)
.add_node(call_model)
.add_edge(START, "call_model")
.compile()
)
for message_chunk, metadata in graph.stream( # (2)!
{"topic": "ice cream"},
stream_mode="messages",
):
if message_chunk.content:
print(message_chunk.content, end="|", flush=True)
- Note that the message events are emitted even when the LLM is run using
.invokerather than
.stream.
- The "messages" stream mode returns an iterator of tuples
(message_chunk, metadata)where
message_chunkis the token streamed by the LLM and
metadatais a dictionary with information about the graph node where the LLM was called and other information.</p>
<h4 id="filter-by-llm-invocation">Filter by LLM invocation¶<a class="headerlink" href="#filter-by-llm-invocation" title="Permanent link">&para;</a></h4>
<p>You can associate
tags with LLM invocations to filter the streamed tokens by LLM invocation.
<em>API Reference: init_chat_model</em>
from langchain.chat_models import init_chat_model
llm_1 = init_chat_model(model="openai:gpt-4o-mini", tags=['joke']) # (1)!
llm_2 = init_chat_model(model="openai:gpt-4o-mini", tags=['poem']) # (2)!
graph = ... # define a graph that uses these LLMs
async for msg, metadata in graph.astream( # (3)!
{"topic": "cats"},
stream_mode="messages",
):
if metadata["tags"] == ["joke"]: # (4)!
print(msg.content, end="|", flush=True)
- llm_1 is tagged with "joke".
- llm_2 is tagged with "poem".
- The
stream_modeis set to "messages" to stream LLM tokens. The
metadatacontains information about the LLM invocation, including the tags.
- Filter the streamed tokens by the
tagsfield in the metadata to only include the tokens from the LLM invocation with the "joke" tag.</p>
<h2 id="extended-example-filtering-by-tags">Extended example: filtering by tags<a class="headerlink" href="#extended-example-filtering-by-tags" title="Permanent link">&para;</a></h2>
<p>from typing import TypedDict
from langchain.chat_models import init_chat_model
from langgraph.graph import START, StateGraph
joke_model = init_chat_model(model="openai:gpt-4o-mini", tags=["joke"]) # (1)!
poem_model = init_chat_model(model="openai:gpt-4o-mini", tags=["poem"]) # (2)!
class State(TypedDict):
topic: str
joke: str
poem: str
async def call_model(state, config):
topic = state["topic"]
print("Writing joke...")</p>
<h1 id="note-passing-the-config-through-explicitly-is-required-for-python-311">Note: Passing the config through explicitly is required for python &lt; 3.11<a class="headerlink" href="#note-passing-the-config-through-explicitly-is-required-for-python-311" title="Permanent link">&para;</a></h1>
<h1 id="since-context-var-support-wasnt-added-before-then-httpsdocspythonorg3libraryasyncio-taskhtmlcreating-tasks">Since context var support wasn't added before then: https://docs.python.org/3/library/asyncio-task.html#creating-tasks<a class="headerlink" href="#since-context-var-support-wasnt-added-before-then-httpsdocspythonorg3libraryasyncio-taskhtmlcreating-tasks" title="Permanent link">&para;</a></h1>
<p>joke_response = await joke_model.ainvoke(
[{"role": "user", "content": f"Write a joke about {topic}"}],
config, # (3)!
)
print("\n\nWriting poem...")
poem_response = await poem_model.ainvoke(
[{"role": "user", "content": f"Write a short poem about {topic}"}],
config, # (3)!
)
return {"joke": joke_response.content, "poem": poem_response.content}
graph = (
StateGraph(State)
.add_node(call_model)
.add_edge(START, "call_model")
.compile()
)
async for msg, metadata in graph.astream(
{"topic": "cats"},
stream_mode="messages", # (4)!
):
if metadata["tags"] == ["joke"]: # (4)!
print(msg.content, end="|", flush=True)
- The
joke_modelis tagged with "joke".
- The
poem_modelis tagged with "poem".
- The
configis passed through explicitly to ensure the context vars are propagated correctly. This is required for Python &lt; 3.11 when using async code. Please see the async section for more details.
- The
stream_modeis set to "messages" to stream LLM tokens. The
metadatacontains information about the LLM invocation, including the tags.</p>
<h4 id="filter-by-node">Filter by node¶<a class="headerlink" href="#filter-by-node" title="Permanent link">&para;</a></h4>
<p>To stream tokens only from specific nodes, use
stream_mode="messages" and filter the outputs by the
langgraph_node field in the streamed metadata:
for msg, metadata in graph.stream( # (1)!
inputs,
stream_mode="messages",
):
if msg.content and metadata["langgraph_node"] == "some_node_name": # (2)!
...
- The "messages" stream mode returns a tuple of
(message_chunk, metadata)where
message_chunkis the token streamed by the LLM and
metadatais a dictionary with information about the graph node where the LLM was called and other information.
- Filter the streamed tokens by the
langgraph_nodefield in the metadata to only include the tokens from the
write_poemnode.</p>
<h2 id="extended-example-streaming-llm-tokens-from-specific-nodes">Extended example: streaming LLM tokens from specific nodes<a class="headerlink" href="#extended-example-streaming-llm-tokens-from-specific-nodes" title="Permanent link">&para;</a></h2>
<p>from typing import TypedDict
from langgraph.graph import START, StateGraph
from langchain_openai import ChatOpenAI
model = ChatOpenAI(model="gpt-4o-mini")
class State(TypedDict):
topic: str
joke: str
poem: str
def write_joke(state: State):
topic = state["topic"]
joke_response = model.invoke(
[{"role": "user", "content": f"Write a joke about {topic}"}]
)
return {"joke": joke_response.content}
def write_poem(state: State):
topic = state["topic"]
poem_response = model.invoke(
[{"role": "user", "content": f"Write a short poem about {topic}"}]
)
return {"poem": poem_response.content}
graph = (
StateGraph(State)
.add_node(write_joke)
.add_node(write_poem)</p>
<h1 id="write-both-the-joke-and-the-poem-concurrently">write both the joke and the poem concurrently<a class="headerlink" href="#write-both-the-joke-and-the-poem-concurrently" title="Permanent link">&para;</a></h1>
<p>.add_edge(START, "write_joke")
.add_edge(START, "write_poem")
.compile()
)
for msg, metadata in graph.stream( # (1)!
{"topic": "cats"},
stream_mode="messages",
):
if msg.content and metadata["langgraph_node"] == "write_poem": # (2)!
print(msg.content, end="|", flush=True)
- The "messages" stream mode returns a tuple of
(message_chunk, metadata)where
message_chunkis the token streamed by the LLM and
metadatais a dictionary with information about the graph node where the LLM was called and other information.
- Filter the streamed tokens by the
langgraph_nodefield in the metadata to only include the tokens from the
write_poemnode.</p>
<h3 id="stream-custom-data">Stream custom data¶<a class="headerlink" href="#stream-custom-data" title="Permanent link">&para;</a></h3>
<p>To send
<strong>custom user-defined data</strong> from inside a LangGraph node or tool, follow these steps:
- Use
get_stream_writer()to access the stream writer and emit custom data.
- Set
stream_mode="custom"when calling
.stream()or
.astream()to get the custom data in the stream. You can combine multiple modes (e.g.,
["updates", "custom"]), but at least one must be
"custom".
No
get_stream_writer() in async for Python &lt; 3.11
In async code running on Python &lt; 3.11,
get_stream_writer() will not work.
Instead, add a
writer parameter to your node or tool and pass it manually.
See Async with Python &lt; 3.11 for usage examples.
from typing import TypedDict
from langgraph.config import get_stream_writer
from langgraph.graph import StateGraph, START
class State(TypedDict):
query: str
answer: str
def node(state: State):
writer = get_stream_writer() # (1)!
writer({"custom_key": "Generating custom data inside node"}) # (2)!
return {"answer": "some data"}
graph = (
StateGraph(State)
.add_node(node)
.add_edge(START, "node")
.compile()
)
inputs = {"query": "example"}</p>
<h1 id="usage">Usage<a class="headerlink" href="#usage" title="Permanent link">&para;</a></h1>
<p>for chunk in graph.stream(inputs, stream_mode="custom"): # (3)!
print(chunk)
- Get the stream writer to send custom data.
- Emit a custom key-value pair (e.g., progress update).
- Set
stream_mode="custom"to receive the custom data in the stream.
from langchain_core.tools import tool
from langgraph.config import get_stream_writer
@tool
def query_database(query: str) -&gt; str:
"""Query the database."""
writer = get_stream_writer() # (1)!
writer({"data": "Retrieved 0/100 records", "type": "progress"}) # (2)!</p>
<h1 id="perform-query">perform query<a class="headerlink" href="#perform-query" title="Permanent link">&para;</a></h1>
<p>writer({"data": "Retrieved 100/100 records", "type": "progress"}) # (3)!
return "some-answer"
graph = ... # define a graph that uses this tool
for chunk in graph.stream(inputs, stream_mode="custom"): # (4)!
print(chunk)
- Access the stream writer to send custom data.
- Emit a custom key-value pair (e.g., progress update).
- Emit another custom key-value pair.
- Set
stream_mode="custom"to receive the custom data in the stream.</p>
<h3 id="use-with-any-llm">Use with any LLM¶<a class="headerlink" href="#use-with-any-llm" title="Permanent link">&para;</a></h3>
<p>You can use
stream_mode="custom" to stream data from
<strong>any LLM API</strong> — even if that API does <strong>not</strong> implement the LangChain chat model interface.
This lets you integrate raw LLM clients or external services that provide their own streaming interfaces, making LangGraph highly flexible for custom setups.
<em>API Reference: get_stream_writer</em>
from langgraph.config import get_stream_writer
def call_arbitrary_model(state):
"""Example node that calls an arbitrary model and streams the output"""
writer = get_stream_writer() # (1)!</p>
<h1 id="assume-you-have-a-streaming-client-that-yields-chunks">Assume you have a streaming client that yields chunks<a class="headerlink" href="#assume-you-have-a-streaming-client-that-yields-chunks" title="Permanent link">&para;</a></h1>
<p>for chunk in your_custom_streaming_client(state["topic"]): # (2)!
writer({"custom_llm_chunk": chunk}) # (3)!
return {"result": "completed"}
graph = (
StateGraph(State)
.add_node(call_arbitrary_model)</p>
<h1 id="add-other-nodes-and-edges-as-needed">Add other nodes and edges as needed<a class="headerlink" href="#add-other-nodes-and-edges-as-needed" title="Permanent link">&para;</a></h1>
<p>.compile()
)
for chunk in graph.stream(
{"topic": "cats"},
stream_mode="custom", # (4)!
):</p>
<h1 id="the-chunk-will-contain-the-custom-data-streamed-from-the-llm">The chunk will contain the custom data streamed from the llm<a class="headerlink" href="#the-chunk-will-contain-the-custom-data-streamed-from-the-llm" title="Permanent link">&para;</a></h1>
<p>print(chunk)
- Get the stream writer to send custom data.
- Generate LLM tokens using your custom streaming client.
- Use the writer to send custom data to the stream.
- Set
stream_mode="custom"to receive the custom data in the stream.</p>
<h2 id="extended-example-streaming-arbitrary-chat-model">Extended example: streaming arbitrary chat model<a class="headerlink" href="#extended-example-streaming-arbitrary-chat-model" title="Permanent link">&para;</a></h2>
<p>import operator
import json
from typing import TypedDict
from typing_extensions import Annotated
from langgraph.graph import StateGraph, START
from openai import AsyncOpenAI
openai_client = AsyncOpenAI()
model_name = "gpt-4o-mini"
async def stream_tokens(model_name: str, messages: list[dict]):
response = await openai_client.chat.completions.create(
messages=messages, model=model_name, stream=True
)
role = None
async for chunk in response:
delta = chunk.choices[0].delta
if delta.role is not None:
role = delta.role
if delta.content:
yield {"role": role, "content": delta.content}</p>
<h1 id="this-is-our-tool">this is our tool<a class="headerlink" href="#this-is-our-tool" title="Permanent link">&para;</a></h1>
<p>async def get_items(place: str) -&gt; str:
"""Use this tool to list items one might find in a place you're asked about."""
writer = get_stream_writer()
response = ""
async for msg_chunk in stream_tokens(
model_name,
[
{
"role": "user",
"content": (
"Can you tell me what kind of items "
f"i might find in the following place: '{place}'. "
"List at least 3 such items separating them by a comma. "
"And include a brief description of each item."
),
}
],
):
response += msg_chunk["content"]
writer(msg_chunk)
return response
class State(TypedDict):
messages: Annotated[list[dict], operator.add]</p>
<h1 id="this-is-the-tool-calling-graph-node">this is the tool-calling graph node<a class="headerlink" href="#this-is-the-tool-calling-graph-node" title="Permanent link">&para;</a></h1>
<p>async def call_tool(state: State):
ai_message = state["messages"][-1]
tool_call = ai_message["tool_calls"][-1]
function_name = tool_call["function"]["name"]
if function_name != "get_items":
raise ValueError(f"Tool {function_name} not supported")
function_arguments = tool_call["function"]["arguments"]
arguments = json.loads(function_arguments)
function_response = await get_items(**arguments)
tool_message = {
"tool_call_id": tool_call["id"],
"role": "tool",
"name": function_name,
"content": function_response,
}
return {"messages": [tool_message]}
graph = (
StateGraph(State)
.add_node(call_tool)
.add_edge(START, "call_tool")
.compile()
)
Let's invoke the graph with an AI message that includes a tool call:
inputs = {
"messages": [
{
"content": None,
"role": "assistant",
"tool_calls": [
{
"id": "1",
"function": {
"arguments": '{"place":"bedroom"}',
"name": "get_items",
},
"type": "function",
}
],
}
]
}
async for chunk in graph.astream(
inputs,
stream_mode="custom",
):
print(chunk["content"], end="|", flush=True)</p>
<h3 id="disable-streaming-for-specific-chat-models">Disable streaming for specific chat models¶<a class="headerlink" href="#disable-streaming-for-specific-chat-models" title="Permanent link">&para;</a></h3>
<p>If your application mixes models that support streaming with those that do not, you may need to explicitly disable streaming for models that do not support it.
Set
disable_streaming=True when initializing the model.
from langchain.chat_models import init_chat_model
model = init_chat_model(
"anthropic:claude-3-7-sonnet-latest",
disable_streaming=True # (1)!
)
- Set
disable_streaming=Trueto disable streaming for the chat model.</p>
<h3 id="async-with-python-311">Async with Python &lt; 3.11¶<a class="headerlink" href="#async-with-python-311" title="Permanent link">&para;</a></h3>
<p>In Python versions &lt; 3.11, asyncio tasks do not support the
context parameter.
This limits LangGraph ability to automatically propagate context, and affects LangGraph's streaming mechanisms in two key ways:
- You
<strong>must</strong>explicitly pass
RunnableConfiginto async LLM calls (e.g.,
ainvoke()), as callbacks are not automatically propagated.
- You
<strong>cannot</strong>use
get_stream_writer()in async nodes or tools — you must pass a
writerargument directly.</p>
<h2 id="extended-example-async-llm-call-with-manual-config">Extended example: async LLM call with manual config<a class="headerlink" href="#extended-example-async-llm-call-with-manual-config" title="Permanent link">&para;</a></h2>
<p>from typing import TypedDict
from langgraph.graph import START, StateGraph
from langchain.chat_models import init_chat_model
llm = init_chat_model(model="openai:gpt-4o-mini")
class State(TypedDict):
topic: str
joke: str
async def call_model(state, config): # (1)!
topic = state["topic"]
print("Generating joke...")
joke_response = await llm.ainvoke(
[{"role": "user", "content": f"Write a joke about {topic}"}],
config, # (2)!
)
return {"joke": joke_response.content}
graph = (
StateGraph(State)
.add_node(call_model)
.add_edge(START, "call_model")
.compile()
)
async for chunk, metadata in graph.astream(
{"topic": "ice cream"},
stream_mode="messages", # (3)!
):
if chunk.content:
print(chunk.content, end="|", flush=True)
- Accept
configas an argument in the async node function.
- Pass
configto
llm.ainvoke()to ensure proper context propagation.
- Set
stream_mode="messages"to stream LLM tokens.</p>
<h2 id="extended-example-async-custom-streaming-with-stream-writer">Extended example: async custom streaming with stream writer<a class="headerlink" href="#extended-example-async-custom-streaming-with-stream-writer" title="Permanent link">&para;</a></h2>
<p>from typing import TypedDict
from langgraph.types import StreamWriter
class State(TypedDict):
topic: str
joke: str
async def generate_joke(state: State, writer: StreamWriter): # (1)!
writer({"custom_key": "Streaming custom data while generating a joke"})
return {"joke": f"This is a joke about {state['topic']}"}
graph = (
StateGraph(State)
.add_node(generate_joke)
.add_edge(START, "generate_joke")
.compile()
)
async for chunk in graph.astream(
{"topic": "ice cream"},
stream_mode="custom", # (2)!
):
print(chunk)
- Add
writeras an argument in the function signature of the async node or tool. LangGraph will automatically pass the stream writer to the function.
- Set
stream_mode="custom"to receive the custom data in the stream.</p>







  
    
  
  


  <aside class="md-source-file">
    
      
  <span class="md-source-file__fact">
    <span class="md-icon" title="Last update">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1-2.1-2M12.5 7v5.2l4 2.4-1 1L11 13V7h1.5M11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2v1.8Z"/></svg>
    </span>
    <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date">August 17, 2025</span>
  </span>

    
    
    
    
  </aside>





                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.top", "navigation.sections", "content.code.copy", "toc.integrate"], "search": "../../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.3220b9d7.min.js"></script>
      
        <script src="../../assets/js/progress.js"></script>
      
    
  <script id="init-glightbox">const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(() => { lightbox.reload() });
</script></body>
</html>