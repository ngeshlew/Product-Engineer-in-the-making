[{"path": "/workspace/my-app/docs/sources/langchain-ai.github.io-langgraph-how-tos-streaming.md", "title": "langchain-ai.github.io-langgraph-how-tos-streaming", "text": "---\ntitle: langchain-ai.github.io-langgraph-how-tos-streaming\nslug: langchain-ai.github.io-langgraph-how-tos-streaming\ntags:\n- source\nwhy_for_designers: TODO\nbot_application: TODO\ncollaboration_prompts:\n- What retrieval strategy applies here?\nsources:\n- url: https://langchain-ai.github.io/langgraph/how-tos/streaming/\n  title: langchain-ai.github.io-langgraph-how-tos-streaming\n  author: ''\n  license: internal-copy\n  retrieved_at: '2025-08-17'\n  policy: copy\nfigures: []\nupdated_at: '2025-08-17'\ncompleted: false\n---\n\n# langchain-ai.github.io-langgraph-how-tos-streaming\n\n> Synthesis: TODO\n\n# Stream outputs\u00b6\nYou can stream outputs from a LangGraph agent or workflow.\n## Supported stream modes\u00b6\nPass one or more of the following stream modes as a list to the\nstream() or\nastream() methods:\n|Mode\n|Description\n|\nvalues\n|Streams the full value of the state after each step of the graph.\n|\nupdates\n|Streams the updates to the state after each step of the graph. If multiple updates are made in the same step (e.g., multiple nodes are run), those updates are streamed separately.\n|\ncustom\n|Streams custom data from inside your graph nodes.\n|\nmessages\n|Streams 2-tuples (LLM token, metadata) from any graph nodes where an LLM is invoked.\n|\ndebug\n|Streams as much information as possible throughout the execution of the graph.\n## Stream from an agent\u00b6\n### Agent progress\u00b6\nTo stream agent progress, use the\nstream() or\nastream() methods with\nstream_mode=\"updates\". This emits an event after every agent step.\nFor example, if you have an agent that calls a tool once, you should see the following updates:\n**LLM node**: AI message with tool call requests **Tool node**: Tool message with execution result **LLM node**: Final AI response\n### LLM tokens\u00b6\nTo stream tokens as they are produced by the LLM, use\nstream_mode=\"messages\":\nagent = create_react_agent(\nmodel=\"anthropic:claude-3-7-sonnet-latest\",\ntools=[get_weather],\n)\nasync for token, metadata in agent.astream(\n{\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]},\nstream_mode=\"messages\"\n):\nprint(\"Token\", token)\nprint(\"Metadata\", metadata)\nprint(\"\\n\")\n### Tool updates\u00b6\nTo stream updates from tools as they are executed, you can use get_stream_writer.\nfrom langgraph.config import get_stream_writer\ndef get_weather(city: str) -> str:\n\"\"\"Get weather for a given city.\"\"\"\nwriter = get_stream_writer()\n# stream any arbitrary data\nwriter(f\"Looking up data for city: {city}\")\nreturn f\"It's always sunny in {city}!\"\nagent = create_react_agent(\nmodel=\"anthropic:claude-3-7-sonnet-latest\",\ntools=[get_weather],\n)\nfor chunk in agent.stream(\n{\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]},\nstream_mode=\"custom\"\n):\nprint(chunk)\nprint(\"\\n\")\nfrom langgraph.config import get_stream_writer\ndef get_weather(city: str) -> str:\n\"\"\"Get weather for a given city.\"\"\"\nwriter = get_stream_writer()\n# stream any arbitrary data\nwriter(f\"Looking up data for city: {city}\")\nreturn f\"It's always sunny in {city}!\"\nagent = create_react_agent(\nmodel=\"anthropic:claude-3-7-sonnet-latest\",\ntools=[get_weather],\n)\nasync for chunk in agent.astream(\n{\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]},\nstream_mode=\"custom\"\n):\nprint(chunk)\nprint(\"\\n\")\nNote\nIf you add\nget_stream_writer inside your tool, you won't be able to invoke the tool outside of a LangGraph execution context.\n### Stream multiple modes\u00b6\nYou can specify multiple streaming modes by passing stream mode as a list:\nstream_mode=[\"updates\", \"messages\", \"custom\"]:\n### Disable streaming\u00b6\nIn some applications you might need to disable streaming of individual tokens for a given model. This is useful in multi-agent systems to control which agents stream their output.\nSee the Models guide to learn how to disable streaming.\n## Stream from a workflow\u00b6\n### Basic usage example\u00b6\nLangGraph graphs expose the\n.stream() (sync) and\n.astream() (async) methods to yield streamed outputs as iterators.\n## Extended example: streaming updates\nfrom typing import TypedDict\nfrom langgraph.graph import StateGraph, START, END\nclass State(TypedDict):\ntopic: str\njoke: str\ndef refine_topic(state: State):\nreturn {\"topic\": state[\"topic\"] + \" and cats\"}\ndef generate_joke(state: State):\nreturn {\"joke\": f\"This is a joke about {state['topic']}\"}\ngraph = (\nStateGraph(State)\n.add_node(refine_topic)\n.add_node(generate_joke)\n.add_edge(START, \"refine_topic\")\n.add_edge(\"refine_topic\", \"generate_joke\")\n.add_edge(\"generate_joke\", END)\n.compile()\n)\nfor chunk in graph.stream( # (1)!\n{\"topic\": \"ice cream\"},\nstream_mode=\"updates\", # (2)!\n):\nprint(chunk)\n- The\nstream()method returns an iterator that yields streamed outputs.\n- Set\nstream_mode=\"updates\"to stream only the updates to the graph state after each node. Other stream modes are also available. See supported stream modes for details.\noutput\n{'refineTopic': {'topic': 'ice cream and cats'}}\n{'generateJoke': {'joke': 'This is a joke about ice cream and cats'}} |\n### Stream multiple modes\u00b6\nYou can pass a list as the\nstream_mode parameter to stream multiple modes at once.\nThe streamed outputs will be tuples of\n(mode, chunk) where\nmode is the name of the stream mode and\nchunk is the data streamed by that mode.\n### Stream graph state\u00b6\nUse the stream modes\nupdates and\nvalues to stream the state of the graph as it executes.\nupdatesstreams the\n**updates**to the state after each step of the graph.\nvaluesstreams the\n**full value**of the state after each step of the graph. *API Reference: StateGraph | START | END*\nfrom typing import TypedDict\nfrom langgraph.graph import StateGraph, START, END\nclass State(TypedDict):\ntopic: str\njoke: str\ndef refine_topic(state: State):\nreturn {\"topic\": state[\"topic\"] + \" and cats\"}\ndef generate_joke(state: State):\nreturn {\"joke\": f\"This is a joke about {state['topic']}\"}\ngraph = (\nStateGraph(State)\n.add_node(refine_topic)\n.add_node(generate_joke)\n.add_edge(START, \"refine_topic\")\n.add_edge(\"refine_topic\", \"generate_joke\")\n.add_edge(\"generate_joke\", END)\n.compile()\n)\nUse this to stream only the\n**state updates** returned by the nodes after each step. The streamed outputs include the name of the node as well as the update.\n### Stream subgraph outputs\u00b6\nTo include outputs from subgraphs in the streamed outputs, you can set\nsubgraphs=True in the\n.stream() method of the parent graph. This will stream outputs from both the parent graph and any subgraphs.\nThe outputs will be streamed as tuples\n(namespace, data), where\nnamespace is a tuple with the path to the node where a subgraph is invoked, e.g.\n(\"parent_node:<task_id>\", \"child_node:<task_id>\").\nfor chunk in graph.stream(\n{\"foo\": \"foo\"},\nsubgraphs=True, # (1)!\nstream_mode=\"updates\",\n):\nprint(chunk)\n- Set\nsubgraphs=Trueto stream outputs from subgraphs.\n## Extended example: streaming from subgraphs\nfrom langgraph.graph import START, StateGraph\nfrom typing import TypedDict\n# Define subgraph\nclass SubgraphState(TypedDict):\nfoo: str # note that this key is shared with the parent graph state\nbar: str\ndef subgraph_node_1(state: SubgraphState):\nreturn {\"bar\": \"bar\"}\ndef subgraph_node_2(state: SubgraphState):\nreturn {\"foo\": state[\"foo\"] + state[\"bar\"]}\nsubgraph_builder = StateGraph(SubgraphState)\nsubgraph_builder.add_node(subgraph_node_1)\nsubgraph_builder.add_node(subgraph_node_2)\nsubgraph_builder.add_edge(START, \"subgraph_node_1\")\nsubgraph_builder.add_edge(\"subgraph_node_1\", \"subgraph_node_2\")\nsubgraph = subgraph_builder.compile()\n# Define parent graph\nclass ParentState(TypedDict):\nfoo: str\ndef node_1(state: ParentState):\nreturn {\"foo\": \"hi! \" + state[\"foo\"]}\nbuilder = StateGraph(ParentState)\nbuilder.add_node(\"node_1\", node_1)\nbuilder.add_node(\"node_2\", subgraph)\nbuilder.add_edge(START, \"node_1\")\nbuilder.add_edge(\"node_1\", \"node_2\")\ngraph = builder.compile()\nfor chunk in graph.stream(\n{\"foo\": \"foo\"},\nstream_mode=\"updates\",\nsubgraphs=True, # (1)!\n):\nprint(chunk)\n- Set\nsubgraphs=Trueto stream outputs from subgraphs.\n((), {'node_1': {'foo': 'hi! foo'}})\n(('node_2:dfddc4ba-c3c5-6887-5012-a243b5b377c2',), {'subgraph_node_1': {'bar': 'bar'}})\n(('node_2:dfddc4ba-c3c5-6887-5012-a243b5b377c2',), {'subgraph_node_2': {'foo': 'hi! foobar'}})\n((), {'node_2': {'foo': 'hi! foobar'}})\n**Note** that we are receiving not just the node updates, but we also the namespaces which tell us what graph (or subgraph) we are streaming from.\n### Debugging\u00b6\nUse the\ndebug streaming mode to stream as much information as possible throughout the execution of the graph. The streamed outputs include the name of the node as well as the full state.\n### LLM tokens\u00b6\nUse the\nmessages streaming mode to stream Large Language Model (LLM) outputs\n**token by token** from any part of your graph, including nodes, tools, subgraphs, or tasks.\nThe streamed output from\nmessages mode is a tuple\n(message_chunk, metadata) where:\nmessage_chunk: the token or message segment from the LLM.\nmetadata: a dictionary containing details about the graph node and LLM invocation.\nIf your LLM is not available as a LangChain integration, you can stream its outputs using\ncustommode instead. See use with any LLM for details.\nManual config required for async in Python < 3.11\nWhen using Python < 3.11 with async code, you must explicitly pass\nRunnableConfig to\nainvoke() to enable proper streaming. See Async with Python < 3.11 for details or upgrade to Python 3.11+.\n*API Reference: init_chat_model | StateGraph | START*\nfrom dataclasses import dataclass\nfrom langchain.chat_models import init_chat_model\nfrom langgraph.graph import StateGraph, START\n@dataclass\nclass MyState:\ntopic: str\njoke: str = \"\"\nllm = init_chat_model(model=\"openai:gpt-4o-mini\")\ndef call_model(state: MyState):\n\"\"\"Call the LLM to generate a joke about a topic\"\"\"\nllm_response = llm.invoke( # (1)!\n[\n{\"role\": \"user\", \"content\": f\"Generate a joke about {state.topic}\"}\n]\n)\nreturn {\"joke\": llm_response.content}\ngraph = (\nStateGraph(MyState)\n.add_node(call_model)\n.add_edge(START, \"call_model\")\n.compile()\n)\nfor message_chunk, metadata in graph.stream( # (2)!\n{\"topic\": \"ice cream\"},\nstream_mode=\"messages\",\n):\nif message_chunk.content:\nprint(message_chunk.content, end=\"|\", flush=True)\n- Note that the message events are emitted even when the LLM is run using\n.invokerather than\n.stream.\n- The \"messages\" stream mode returns an iterator of tuples\n(message_chunk, metadata)where\nmessage_chunkis the token streamed by the LLM and\nmetadatais a dictionary with information about the graph node where the LLM was called and other information.\n#### Filter by LLM invocation\u00b6\nYou can associate\ntags with LLM invocations to filter the streamed tokens by LLM invocation.\n*API Reference: init_chat_model*\nfrom langchain.chat_models import init_chat_model\nllm_1 = init_chat_model(model=\"openai:gpt-4o-mini\", tags=['joke']) # (1)!\nllm_2 = init_chat_model(model=\"openai:gpt-4o-mini\", tags=['poem']) # (2)!\ngraph = ... # define a graph that uses these LLMs\nasync for msg, metadata in graph.astream( # (3)!\n{\"topic\": \"cats\"},\nstream_mode=\"messages\",\n):\nif metadata[\"tags\"] == [\"joke\"]: # (4)!\nprint(msg.content, end=\"|\", flush=True)\n- llm_1 is tagged with \"joke\".\n- llm_2 is tagged with \"poem\".\n- The\nstream_modeis set to \"messages\" to stream LLM tokens. The\nmetadatacontains information about the LLM invocation, including the tags.\n- Filter the streamed tokens by the\ntagsfield in the metadata to only include the tokens from the LLM invocation with the \"joke\" tag.\n## Extended example: filtering by tags\nfrom typing import TypedDict\nfrom langchain.chat_models import init_chat_model\nfrom langgraph.graph import START, StateGraph\njoke_model = init_chat_model(model=\"openai:gpt-4o-mini\", tags=[\"joke\"]) # (1)!\npoem_model = init_chat_model(model=\"openai:gpt-4o-mini\", tags=[\"poem\"]) # (2)!\nclass State(TypedDict):\ntopic: str\njoke: str\npoem: str\nasync def call_model(state, config):\ntopic = state[\"topic\"]\nprint(\"Writing joke...\")\n# Note: Passing the config through explicitly is required for python < 3.11\n# Since context var support wasn't added before then: https://docs.python.org/3/library/asyncio-task.html#creating-tasks\njoke_response = await joke_model.ainvoke(\n[{\"role\": \"user\", \"content\": f\"Write a joke about {topic}\"}],\nconfig, # (3)!\n)\nprint(\"\\n\\nWriting poem...\")\npoem_response = await poem_model.ainvoke(\n[{\"role\": \"user\", \"content\": f\"Write a short poem about {topic}\"}],\nconfig, # (3)!\n)\nreturn {\"joke\": joke_response.content, \"poem\": poem_response.content}\ngraph = (\nStateGraph(State)\n.add_node(call_model)\n.add_edge(START, \"call_model\")\n.compile()\n)\nasync for msg, metadata in graph.astream(\n{\"topic\": \"cats\"},\nstream_mode=\"messages\", # (4)!\n):\nif metadata[\"tags\"] == [\"joke\"]: # (4)!\nprint(msg.content, end=\"|\", flush=True)\n- The\njoke_modelis tagged with \"joke\".\n- The\npoem_modelis tagged with \"poem\".\n- The\nconfigis passed through explicitly to ensure the context vars are propagated correctly. This is required for Python < 3.11 when using async code. Please see the async section for more details.\n- The\nstream_modeis set to \"messages\" to stream LLM tokens. The\nmetadatacontains information about the LLM invocation, including the tags.\n#### Filter by node\u00b6\nTo stream tokens only from specific nodes, use\nstream_mode=\"messages\" and filter the outputs by the\nlanggraph_node field in the streamed metadata:\nfor msg, metadata in graph.stream( # (1)!\ninputs,\nstream_mode=\"messages\",\n):\nif msg.content and metadata[\"langgraph_node\"] == \"some_node_name\": # (2)!\n...\n- The \"messages\" stream mode returns a tuple of\n(message_chunk, metadata)where\nmessage_chunkis the token streamed by the LLM and\nmetadatais a dictionary with information about the graph node where the LLM was called and other information.\n- Filter the streamed tokens by the\nlanggraph_nodefield in the metadata to only include the tokens from the\nwrite_poemnode.\n## Extended example: streaming LLM tokens from specific nodes\nfrom typing import TypedDict\nfrom langgraph.graph import START, StateGraph\nfrom langchain_openai import ChatOpenAI\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\nclass State(TypedDict):\ntopic: str\njoke: str\npoem: str\ndef write_joke(state: State):\ntopic = state[\"topic\"]\njoke_response = model.invoke(\n[{\"role\": \"user\", \"content\": f\"Write a joke about {topic}\"}]\n)\nreturn {\"joke\": joke_response.content}\ndef write_poem(state: State):\ntopic = state[\"topic\"]\npoem_response = model.invoke(\n[{\"role\": \"user\", \"content\": f\"Write a short poem about {topic}\"}]\n)\nreturn {\"poem\": poem_response.content}\ngraph = (\nStateGraph(State)\n.add_node(write_joke)\n.add_node(write_poem)\n# write both the joke and the poem concurrently\n.add_edge(START, \"write_joke\")\n.add_edge(START, \"write_poem\")\n.compile()\n)\nfor msg, metadata in graph.stream( # (1)!\n{\"topic\": \"cats\"},\nstream_mode=\"messages\",\n):\nif msg.content and metadata[\"langgraph_node\"] == \"write_poem\": # (2)!\nprint(msg.content, end=\"|\", flush=True)\n- The \"messages\" stream mode returns a tuple of\n(message_chunk, metadata)where\nmessage_chunkis the token streamed by the LLM and\nmetadatais a dictionary with information about the graph node where the LLM was called and other information.\n- Filter the streamed tokens by the\nlanggraph_nodefield in the metadata to only include the tokens from the\nwrite_poemnode.\n### Stream custom data\u00b6\nTo send\n**custom user-defined data** from inside a LangGraph node or tool, follow these steps:\n- Use\nget_stream_writer()to access the stream writer and emit custom data.\n- Set\nstream_mode=\"custom\"when calling\n.stream()or\n.astream()to get the custom data in the stream. You can combine multiple modes (e.g.,\n[\"updates\", \"custom\"]), but at least one must be\n\"custom\".\nNo\nget_stream_writer() in async for Python < 3.11\nIn async code running on Python < 3.11,\nget_stream_writer() will not work.\nInstead, add a\nwriter parameter to your node or tool and pass it manually.\nSee Async with Python < 3.11 for usage examples.\nfrom typing import TypedDict\nfrom langgraph.config import get_stream_writer\nfrom langgraph.graph import StateGraph, START\nclass State(TypedDict):\nquery: str\nanswer: str\ndef node(state: State):\nwriter = get_stream_writer() # (1)!\nwriter({\"custom_key\": \"Generating custom data inside node\"}) # (2)!\nreturn {\"answer\": \"some data\"}\ngraph = (\nStateGraph(State)\n.add_node(node)\n.add_edge(START, \"node\")\n.compile()\n)\ninputs = {\"query\": \"example\"}\n# Usage\nfor chunk in graph.stream(inputs, stream_mode=\"custom\"): # (3)!\nprint(chunk)\n- Get the stream writer to send custom data.\n- Emit a custom key-value pair (e.g., progress update).\n- Set\nstream_mode=\"custom\"to receive the custom data in the stream.\nfrom langchain_core.tools import tool\nfrom langgraph.config import get_stream_writer\n@tool\ndef query_database(query: str) -> str:\n\"\"\"Query the database.\"\"\"\nwriter = get_stream_writer() # (1)!\nwriter({\"data\": \"Retrieved 0/100 records\", \"type\": \"progress\"}) # (2)!\n# perform query\nwriter({\"data\": \"Retrieved 100/100 records\", \"type\": \"progress\"}) # (3)!\nreturn \"some-answer\"\ngraph = ... # define a graph that uses this tool\nfor chunk in graph.stream(inputs, stream_mode=\"custom\"): # (4)!\nprint(chunk)\n- Access the stream writer to send custom data.\n- Emit a custom key-value pair (e.g., progress update).\n- Emit another custom key-value pair.\n- Set\nstream_mode=\"custom\"to receive the custom data in the stream.\n### Use with any LLM\u00b6\nYou can use\nstream_mode=\"custom\" to stream data from\n**any LLM API** \u2014 even if that API does **not** implement the LangChain chat model interface.\nThis lets you integrate raw LLM clients or external services that provide their own streaming interfaces, making LangGraph highly flexible for custom setups.\n*API Reference: get_stream_writer*\nfrom langgraph.config import get_stream_writer\ndef call_arbitrary_model(state):\n\"\"\"Example node that calls an arbitrary model and streams the output\"\"\"\nwriter = get_stream_writer() # (1)!\n# Assume you have a streaming client that yields chunks\nfor chunk in your_custom_streaming_client(state[\"topic\"]): # (2)!\nwriter({\"custom_llm_chunk\": chunk}) # (3)!\nreturn {\"result\": \"completed\"}\ngraph = (\nStateGraph(State)\n.add_node(call_arbitrary_model)\n# Add other nodes and edges as needed\n.compile()\n)\nfor chunk in graph.stream(\n{\"topic\": \"cats\"},\nstream_mode=\"custom\", # (4)!\n):\n# The chunk will contain the custom data streamed from the llm\nprint(chunk)\n- Get the stream writer to send custom data.\n- Generate LLM tokens using your custom streaming client.\n- Use the writer to send custom data to the stream.\n- Set\nstream_mode=\"custom\"to receive the custom data in the stream.\n## Extended example: streaming arbitrary chat model\nimport operator\nimport json\nfrom typing import TypedDict\nfrom typing_extensions import Annotated\nfrom langgraph.graph import StateGraph, START\nfrom openai import AsyncOpenAI\nopenai_client = AsyncOpenAI()\nmodel_name = \"gpt-4o-mini\"\nasync def stream_tokens(model_name: str, messages: list[dict]):\nresponse = await openai_client.chat.completions.create(\nmessages=messages, model=model_name, stream=True\n)\nrole = None\nasync for chunk in response:\ndelta = chunk.choices[0].delta\nif delta.role is not None:\nrole = delta.role\nif delta.content:\nyield {\"role\": role, \"content\": delta.content}\n# this is our tool\nasync def get_items(place: str) -> str:\n\"\"\"Use this tool to list items one might find in a place you're asked about.\"\"\"\nwriter = get_stream_writer()\nresponse = \"\"\nasync for msg_chunk in stream_tokens(\nmodel_name,\n[\n{\n\"role\": \"user\",\n\"content\": (\n\"Can you tell me what kind of items \"\nf\"i might find in the following place: '{place}'. \"\n\"List at least 3 such items separating them by a comma. \"\n\"And include a brief description of each item.\"\n),\n}\n],\n):\nresponse += msg_chunk[\"content\"]\nwriter(msg_chunk)\nreturn response\nclass State(TypedDict):\nmessages: Annotated[list[dict], operator.add]\n# this is the tool-calling graph node\nasync def call_tool(state: State):\nai_message = state[\"messages\"][-1]\ntool_call = ai_message[\"tool_calls\"][-1]\nfunction_name = tool_call[\"function\"][\"name\"]\nif function_name != \"get_items\":\nraise ValueError(f\"Tool {function_name} not supported\")\nfunction_arguments = tool_call[\"function\"][\"arguments\"]\narguments = json.loads(function_arguments)\nfunction_response = await get_items(**arguments)\ntool_message = {\n\"tool_call_id\": tool_call[\"id\"],\n\"role\": \"tool\",\n\"name\": function_name,\n\"content\": function_response,\n}\nreturn {\"messages\": [tool_message]}\ngraph = (\nStateGraph(State)\n.add_node(call_tool)\n.add_edge(START, \"call_tool\")\n.compile()\n)\nLet's invoke the graph with an AI message that includes a tool call:\ninputs = {\n\"messages\": [\n{\n\"content\": None,\n\"role\": \"assistant\",\n\"tool_calls\": [\n{\n\"id\": \"1\",\n\"function\": {\n\"arguments\": '{\"place\":\"bedroom\"}',\n\"name\": \"get_items\",\n},\n\"type\": \"function\",\n}\n],\n}\n]\n}\nasync for chunk in graph.astream(\ninputs,\nstream_mode=\"custom\",\n):\nprint(chunk[\"content\"], end=\"|\", flush=True)\n### Disable streaming for specific chat models\u00b6\nIf your application mixes models that support streaming with those that do not, you may need to explicitly disable streaming for models that do not support it.\nSet\ndisable_streaming=True when initializing the model.\nfrom langchain.chat_models import init_chat_model\nmodel = init_chat_model(\n\"anthropic:claude-3-7-sonnet-latest\",\ndisable_streaming=True # (1)!\n)\n- Set\ndisable_streaming=Trueto disable streaming for the chat model.\n### Async with Python < 3.11\u00b6\nIn Python versions < 3.11, asyncio tasks do not support the\ncontext parameter.\nThis limits LangGraph ability to automatically propagate context, and affects LangGraph's streaming mechanisms in two key ways:\n- You\n**must**explicitly pass\nRunnableConfiginto async LLM calls (e.g.,\nainvoke()), as callbacks are not automatically propagated.\n- You\n**cannot**use\nget_stream_writer()in async nodes or tools \u2014 you must pass a\nwriterargument directly.\n## Extended example: async LLM call with manual config\nfrom typing import TypedDict\nfrom langgraph.graph import START, StateGraph\nfrom langchain.chat_models import init_chat_model\nllm = init_chat_model(model=\"openai:gpt-4o-mini\")\nclass State(TypedDict):\ntopic: str\njoke: str\nasync def call_model(state, config): # (1)!\ntopic = state[\"topic\"]\nprint(\"Generating joke...\")\njoke_response = await llm.ainvoke(\n[{\"role\": \"user\", \"content\": f\"Write a joke about {topic}\"}],\nconfig, # (2)!\n)\nreturn {\"joke\": joke_response.content}\ngraph = (\nStateGraph(State)\n.add_node(call_model)\n.add_edge(START, \"call_model\")\n.compile()\n)\nasync for chunk, metadata in graph.astream(\n{\"topic\": \"ice cream\"},\nstream_mode=\"messages\", # (3)!\n):\nif chunk.content:\nprint(chunk.content, end=\"|\", flush=True)\n- Accept\nconfigas an argument in the async node function.\n- Pass\nconfigto\nllm.ainvoke()to ensure proper context propagation.\n- Set\nstream_mode=\"messages\"to stream LLM tokens.\n## Extended example: async custom streaming with stream writer\nfrom typing import TypedDict\nfrom langgraph.types import StreamWriter\nclass State(TypedDict):\ntopic: str\njoke: str\nasync def generate_joke(state: State, writer: StreamWriter): # (1)!\nwriter({\"custom_key\": \"Streaming custom data while generating a joke\"})\nreturn {\"joke\": f\"This is a joke about {state['topic']}\"}\ngraph = (\nStateGraph(State)\n.add_node(generate_joke)\n.add_edge(START, \"generate_joke\")\n.compile()\n)\nasync for chunk in graph.astream(\n{\"topic\": \"ice cream\"},\nstream_mode=\"custom\", # (2)!\n):\nprint(chunk)\n- Add\nwriteras an argument in the function signature of the async node or tool. LangGraph will automatically pass the stream writer to the function.\n- Set\nstream_mode=\"custom\"to receive the custom data in the stream.\n\n\n"}, {"path": "/workspace/my-app/docs/sources/python.langchain.com-docs-concepts-streaming.md", "title": "python.langchain.com-docs-concepts-streaming", "text": "---\ntitle: python.langchain.com-docs-concepts-streaming\nslug: python.langchain.com-docs-concepts-streaming\ntags:\n- source\nwhy_for_designers: TODO\nbot_application: TODO\ncollaboration_prompts:\n- What retrieval strategy applies here?\nsources:\n- url: https://python.langchain.com/docs/concepts/streaming/\n  title: python.langchain.com-docs-concepts-streaming\n  author: ''\n  license: internal-copy\n  retrieved_at: '2025-08-17'\n  policy: copy\nfigures: []\nupdated_at: '2025-08-17'\ncompleted: false\n---\n\n# python.langchain.com-docs-concepts-streaming\n\n> Synthesis: TODO\n\n# Streaming\n**Streaming** is crucial for enhancing the responsiveness of applications built on LLMs. By displaying output progressively, even before a complete response is ready, streaming significantly improves user experience (UX), particularly when dealing with the latency of LLMs.\n## Overview\nGenerating full responses from LLMs often incurs a delay of several seconds, which becomes more noticeable in complex applications with multiple model calls. Fortunately, LLMs generate responses iteratively, allowing for intermediate results to be displayed as they are produced. By streaming these intermediate outputs, LangChain enables smoother UX in LLM-powered apps and offers built-in support for streaming at the core of its design.\nIn this guide, we'll discuss streaming in LLM applications and explore how LangChain's streaming APIs facilitate real-time output from various components in your application.\n## What to stream in LLM applications\nIn applications involving LLMs, several types of data can be streamed to improve user experience by reducing perceived latency and increasing transparency. These include:\n### 1. Streaming LLM outputs\nThe most common and critical data to stream is the output generated by the LLM itself. LLMs often take time to generate full responses, and by streaming the output in real-time, users can see partial results as they are produced. This provides immediate feedback and helps reduce the wait time for users.\n### 2. Streaming pipeline or workflow progress\nBeyond just streaming LLM output, it\u2019s useful to stream progress through more complex workflows or pipelines, giving users a sense of how the application is progressing overall. This could include:\n-\n**In LangGraph Workflows:**With LangGraph, workflows are composed of nodes and edges that represent various steps. Streaming here involves tracking changes to the **graph state**as individual **nodes**request updates. This allows for more granular monitoring of which node in the workflow is currently active, giving real-time updates about the status of the workflow as it progresses through different stages.\n-\n**In LCEL Pipelines:**Streaming updates from an LCEL pipeline involves capturing progress from individual **sub-runnables**. For example, as different steps or components of the pipeline execute, you can stream which sub-runnable is currently running, providing real-time insight into the overall pipeline's progress.\nStreaming pipeline or workflow progress is essential in providing users with a clear picture of where the application is in the execution process.\n### 3. Streaming custom data\nIn some cases, you may need to stream\n**custom data** that goes beyond the information provided by the pipeline or workflow structure. This custom information is injected within a specific step in the workflow, whether that step is a tool or a LangGraph node. For example, you could stream updates about what a tool is doing in real-time or the progress through a LangGraph node. This granular data, which is emitted directly from within the step, provides more detailed insights into the execution of the workflow and is especially useful in complex processes where more visibility is needed.\n## Streaming APIs\nLangChain has two main APIs for streaming output in real-time. These APIs are supported by any component that implements the Runnable Interface, including LLMs, compiled LangGraph graphs, and any Runnable generated with LCEL.\n- sync stream and async astream: Use to stream outputs from individual Runnables (e.g., a chat model) as they are generated or stream any workflow created with LangGraph.\n- The async only astream_events: Use this API to get access to custom events and intermediate outputs from LLM applications built entirely with LCEL. Note that this API is available, but not needed when working with LangGraph.\nIn addition, there is a\n**legacy** async astream_log API. This API is not recommended for new projects it is more complex and less feature-rich than the other streaming APIs.\nstream() and\nastream()\nThe\nstream() method returns an iterator that yields chunks of output synchronously as they are produced. You can use a\nfor loop to process each chunk in real-time. For example, when using an LLM, this allows the output to be streamed incrementally as it is generated, reducing the wait time for users.\nThe type of chunk yielded by the\nstream() and\nastream() methods depends on the component being streamed. For example, when streaming from an LLM each component will be an AIMessageChunk; however, for other components, the chunk may be different.\nThe\nstream() method returns an iterator that yields these chunks as they are produced. For example,\nfor chunk in component.stream(some_input):\n# IMPORTANT: Keep the processing of each chunk as efficient as possible.\n# While you're processing the current chunk, the upstream component is\n# waiting to produce the next one. For example, if working with LangGraph,\n# graph execution is paused while the current chunk is being processed.\n# In extreme cases, this could even result in timeouts (e.g., when llm outputs are\n# streamed from an API that has a timeout).\nprint(chunk)\nThe asynchronous version,\nastream(), works similarly but is designed for non-blocking workflows. You can use it in asynchronous code to achieve the same real-time streaming behavior.\n#### Usage with chat models\nWhen using\nstream() or\nastream() with chat models, the output is streamed as AIMessageChunks as it is generated by the LLM. This allows you to present or process the LLM's output incrementally as it's being produced, which is particularly useful in interactive applications or interfaces.\n#### Usage with LangGraph\nLangGraph compiled graphs are Runnables and support the standard streaming APIs.\nWhen using the\n*stream* and *astream* methods with LangGraph, you can choose **one or more** streaming mode which allow you to control the type of output that is streamed. The available streaming modes are: **\"values\"**: Emit all values of the state for each step. **\"updates\"**: Emit only the node name(s) and updates that were returned by the node(s) after each step. **\"debug\"**: Emit debug events for each step. **\"messages\"**: Emit LLM messages token-by-token. **\"custom\"**: Emit custom output written using LangGraph's StreamWriter.\nFor more information, please see:\n- LangGraph streaming conceptual guide for more information on how to stream when working with LangGraph.\n- LangGraph streaming how-to guides for specific examples of streaming in LangGraph.\n#### Usage with LCEL\nIf you compose multiple Runnables using LangChain\u2019s Expression Language (LCEL), the\nstream() and\nastream() methods will, by convention, stream the output of the last step in the chain. This allows the final processed result to be streamed incrementally.\n**LCEL** tries to optimize streaming latency in pipelines so that the streaming results from the last step are available as soon as possible.\nastream_events\nUse the\nastream_events API to access custom data and intermediate outputs from LLM applications built entirely with LCEL.\nWhile this API is available for use with LangGraph as well, it is usually not necessary when working with LangGraph, as the\nstream and\nastream methods provide comprehensive streaming capabilities for LangGraph graphs.\nFor chains constructed using\n**LCEL**, the\n.stream() method only streams the output of the final step from the chain. This might be sufficient for some applications, but as you build more complex chains of several LLM calls together, you may want to use the intermediate values of the chain alongside the final output. For example, you may want to return sources alongside the final generation when building a chat-over-documents app.\nThere are ways to do this using callbacks, or by constructing your chain in such a way that it passes intermediate\nvalues to the end with something like chained\n.assign() calls, but LangChain also includes an\n.astream_events() method that combines the flexibility of callbacks with the ergonomics of\n.stream(). When called, it returns an iterator\nwhich yields various types of events that you can filter and process according\nto the needs of your project.\nHere's one small example that prints just events containing streamed chat model output:\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_anthropic import ChatAnthropic\nmodel = ChatAnthropic(model=\"claude-3-7-sonnet-20250219\")\nprompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\nparser = StrOutputParser()\nchain = prompt | model | parser\nasync for event in chain.astream_events({\"topic\": \"parrot\"}):\nkind = event[\"event\"]\nif kind == \"on_chat_model_stream\":\nprint(event, end=\"|\", flush=True)\n**API Reference:**StrOutputParser | ChatPromptTemplate\nYou can roughly think of it as an iterator over callback events (though the format differs) - and you can use it on almost all LangChain components!\nSee this guide for more detailed information on how to use\n.astream_events(), including a table listing available events.\n## Writing custom data to the stream\nTo write custom data to the stream, you will need to choose one of the following methods based on the component you are working with:\n- LangGraph's StreamWriter can be used to write custom data that will surface through\n**stream**and **astream**APIs when working with LangGraph. **Important**this is a LangGraph feature, so it is not available when working with pure LCEL. See how to streaming custom data for more information.\n- dispatch_events / adispatch_events can be used to write custom data that will be surfaced through the\n**astream_events**API. See how to dispatch custom callback events for more information.\n## \"Auto-Streaming\" Chat Models\nLangChain simplifies streaming from chat models by automatically enabling streaming mode in certain cases, even when you\u2019re not explicitly calling the streaming methods. This is particularly useful when you use the non-streaming\ninvoke method but still want to stream the entire application, including intermediate results from the chat model.\n### How It Works\nWhen you call the\ninvoke (or\nainvoke) method on a chat model, LangChain will automatically switch to streaming mode if it detects that you are trying to stream the overall application.\nUnder the hood, it'll have\ninvoke (or\nainvoke) use the\nstream (or\nastream) method to generate its output. The result of the invocation will be the same as far as the code that was using\ninvoke is concerned; however, while the chat model is being streamed, LangChain will take care of invoking\non_llm_new_token events in LangChain's callback system. These callback events\nallow LangGraph\nstream/\nastream and\nastream_events to surface the chat model's output in real-time.\nExample:\ndef node(state):\n...\n# The code below uses the invoke method, but LangChain will\n# automatically switch to streaming mode\n# when it detects that the overall\n# application is being streamed.\nai_message = model.invoke(state[\"messages\"])\n...\nfor chunk in compiled_graph.stream(..., mode=\"messages\"):\n...\n## Async Programming\nLangChain offers both synchronous (sync) and asynchronous (async) versions of many of its methods. The async methods are typically prefixed with an \"a\" (e.g.,\nainvoke,\nastream). When writing async code, it's crucial to consistently use these asynchronous methods to ensure non-blocking behavior and optimal performance.\nIf streaming data fails to appear in real-time, please ensure that you are using the correct async methods for your workflow.\nPlease review the async programming in LangChain guide for more information on writing async code with LangChain.\n## Related Resources\nPlease see the following how-to guides for specific examples of streaming in LangChain:\n- LangGraph conceptual guide on streaming\n- LangGraph streaming how-to guides\n- How to stream runnables: This how-to guide goes over common streaming patterns with LangChain components (e.g., chat models) and with LCEL.\n- How to stream chat models\n- How to stream tool calls\nFor writing custom data to the stream, please see the following resources:\n- If using LangGraph, see how to stream custom data.\n- If using LCEL, see how to dispatch custom callback events.\n\n\n"}, {"path": "/workspace/my-app/docs/sources/www.nngroup.com-articles-prompt-suggestions.md", "title": "www.nngroup.com-articles-prompt-suggestions", "text": "---\ntitle: www.nngroup.com-articles-prompt-suggestions\nslug: www.nngroup.com-articles-prompt-suggestions\ntags:\n- source\nwhy_for_designers: TODO\nbot_application: TODO\ncollaboration_prompts:\n- What retrieval strategy applies here?\nsources:\n- url: https://www.nngroup.com/articles/prompt-suggestions/\n  title: www.nngroup.com-articles-prompt-suggestions\n  author: ''\n  license: internal-copy\n  retrieved_at: '2025-08-17'\n  policy: copy\nfigures:\n- path: ../assets/www.nngroup.com/www.nngroup.com-articles-prompt-suggestions/83d11e4e618b.webp\n  caption: Figure\n  credit_name: www.nngroup.com\n  credit_url: https://media.nngroup.com/media/people/photos/Kate-Headshot-2022.jpg.256x256_q75_autocrop_crop-smart_upscale.jpg\n  license: internal-copy\n- path: ../assets/www.nngroup.com/www.nngroup.com-articles-prompt-suggestions/29f27e555423.webp\n  caption: Figure\n  credit_name: www.nngroup.com\n  credit_url: https://media.nngroup.com/media/people/photos/Tim-portrait-2022.jpg.256x256_q75_autocrop_crop-smart_upscale.jpg\n  license: internal-copy\n- path: ../assets/www.nngroup.com/www.nngroup.com-articles-prompt-suggestions/14347dce96d1.webp\n  caption: 'Traditional search suggestions: When users started typing within a Google\n    search field, the system showed traditional search suggestions.'\n  credit_name: www.nngroup.com\n  credit_url: https://media.nngroup.com/media/editor/2025/04/23/333.png\n  license: internal-copy\n- path: ../assets/www.nngroup.com/www.nngroup.com-articles-prompt-suggestions/d007f418caff.webp\n  caption: \"Enriched search suggestions:\\_Amazon showed enriched search suggestions,\\\n    \\ based on previous searches.\"\n  credit_name: www.nngroup.com\n  credit_url: https://media.nngroup.com/media/editor/2025/04/23/222.png\n  license: internal-copy\n- path: ../assets/www.nngroup.com/www.nngroup.com-articles-prompt-suggestions/a150123ff0f9.webp\n  caption: \"Prompt suggestions:\\_ChatGPT displayed prompt suggestions before users\\\n    \\ started engaging with the system.\"\n  credit_name: www.nngroup.com\n  credit_url: https://media.nngroup.com/media/editor/2025/04/23/3-prompt-suggestion.png\n  license: internal-copy\n- path: ../assets/www.nngroup.com/www.nngroup.com-articles-prompt-suggestions/3d977cae090f.webp\n  caption: 'Manus: The use-case prompt suggestions below the chat field illustrated\n    how people might use the pseudo agentic AI tool. When clicked, the use cases included\n    complete replays of the conversations, so new users could see demonstrations of\n    how they should use the product.'\n  credit_name: www.nngroup.com\n  credit_url: https://media.nngroup.com/media/editor/2025/04/15/usecasepromptsuggestion1.png\n  license: internal-copy\n- path: ../assets/www.nngroup.com/www.nngroup.com-articles-prompt-suggestions/8459169d7e76.webp\n  caption: \"ChatGPT: As the user typed \\u201Chow much should I feed,\\u201D the system\\\n    \\ attempted to complete the question for her. These are prompt-autocomplete suggestions.\"\n  credit_name: www.nngroup.com\n  credit_url: https://media.nngroup.com/media/editor/2025/04/15/gpt1.png\n  license: internal-copy\n- path: ../assets/www.nngroup.com/www.nngroup.com-articles-prompt-suggestions/19ad55ea06af.webp\n  caption: 'ChatGPT: In this conversation, the system offered multiple followup questions\n    relevant to the current conversation (no-yeast or thin-crust dough recipes for\n    pizza.)'\n  credit_name: www.nngroup.com\n  credit_url: https://media.nngroup.com/media/editor/2025/04/15/followup-question-suggestion-chatgpt.png\n  license: internal-copy\n- path: ../assets/www.nngroup.com/www.nngroup.com-articles-prompt-suggestions/54d3249df98d.webp\n  caption: Figure\n  credit_name: www.nngroup.com\n  credit_url: https://media.nngroup.com/media/videos/thumbnails/CARE_Structure_for_Crafting_AI_Prompts_Thumbnail.jpg.650x364_q75_autocrop_crop-smart_upscale.jpg\n  license: internal-copy\n- path: ../assets/www.nngroup.com/www.nngroup.com-articles-prompt-suggestions/70d44a616567.webp\n  caption: Figure\n  credit_name: www.nngroup.com\n  credit_url: https://media.nngroup.com/media/videos/thumbnails/Synthetic_AI_Users_Thumbnail.jpg.650x364_q75_autocrop_crop-smart_upscale.jpg\n  license: internal-copy\n- path: ../assets/www.nngroup.com/www.nngroup.com-articles-prompt-suggestions/abbd1f9a4631.webp\n  caption: Figure\n  credit_name: www.nngroup.com\n  credit_url: https://media.nngroup.com/media/videos/thumbnails/Your_AI_UX_Intern_Thumbnail.jpg.650x364_q75_autocrop_crop-smart_upscale.jpg\n  license: internal-copy\nupdated_at: '2025-08-17'\ncompleted: false\n---\n\n# www.nngroup.com-articles-prompt-suggestions\n\n> Synthesis: TODO\n\nPrompt suggestions are a common design element in AI-chat features, but users often ignore them, especially when they\u2019re not in the right place or don\u2019t feel useful. Done well, though, prompt suggestions can guide, inspire, and help users complete tasks efficiently.\n## What Are Prompt Suggestions?\nPrompt suggestions are typically used in generative-AI (genAI) systems that accept open-text prompts from users. When people first begin using an AI system, they\u2019re often faced with the \u201cblank page\u201d problem and must think about how they could use the product.\nPrompt suggestionsare system-generated hints that guide users in forming queries or commands for AI tools. These can be full questions, phrases, or even single keywords that aim to showcase system capabilities, reduce user effort, or encourage exploration.\n### Prompt Suggestions vs. Search Suggestions\nPrompt suggestions play a similar role to search suggestions.\nTraditional search suggestionsare recommended queries that appear below the search box as users type in their search query. They change as users type each letter of their query.\nEnriched search suggestionsare expanded recommendations for site content presented to the user during on-site search and shown under the search box, before or while the user types a query. Before the user starts typing, for example, the system may suggest popular, trending, or frequently searched items.\nUnlike search suggestions, prompt suggestions are not just about predicting the end of a sentence. Instead, they often aim to inspire interaction and guide discovery of the AI tool\u2019s capabilities. They set expectations for what the system can do, and how they should interact with it.\nEspecially for new genAI users (which is still a large segment of the population currently),\n** this feature is critical for supporting ** **discoverability** ** and ** **learnability** **.**\n## The Purpose of Prompt Suggestions\nWhen designed thoughtfully, prompt suggestions can significantly improve the user experience of AI tools. Well-designed prompt suggestions can:\n**Reduce** **cognitive load**: By offering ideas or phrasing, prompt suggestions lower the effort required to get started. **Reduce** **interaction cost**: If the prompt suggestion is right, users can skip typing. **Encourage engagement**: Good suggestions lead users to explore topics and tasks they might have missed (or considered impossible to perform with the AI tool). **Improve task efficiency**: They help users formulate better prompts, especially when they\u2019re unsure how to begin or what to ask.\nOne participant in a recent genAI study that we conducted told us:\n\u201cI do really like [the prompt suggestions] because they [included] a lot of questions [...] I didn't even realize I had. [...] I definitely like that it triggered a thought process that I forgot I even had in my head.\u201d\n## Three Types of AI-Prompt Suggestions\nThere are currently three different approaches to prompt suggestions, each with a slightly different purpose:\n**Use-case prompt suggestions:**support system learnability and creativity **Prompt-autocomplete suggestions:**increase efficiency **Followup-question suggestions:**increase engagement\nThis article provides a\n**high-level overview of all three types of prompt suggestions. **Separate articles will discuss design considerations and best practices for each.\n### Use-Case Prompt Suggestions\n**Use-case prompt suggestions** are examples of good AI prompts. They are displayed within most AI tools, usually (but not only) to novice users. They are intended to help users understand what they can use the AI tool for and how to interact with it. When designed well, they set accurate expectations for the system and guide users toward effective prompting.\n### Prompt Autocomplete\n**A prompt autocomplete **is a continuation of a user prompt displayed below the chat box. Like search autocomplete, prompt autocomplete aims to increase efficiency for users \u2014 that is, help them complete their input quickly and accurately, following AI-usage best practices and avoiding mistakes, like typos.\n### Followup Questions\n**Followup questions **are prompt suggestions related to a preestablished context (such as an ongoing conversation). They are usually displayed below the answer to the user\u2019s previous prompt.\nFollowup questions aim to create engagement and make it easier for the user to continue the existing conversation. Based on previous inputs and outputs, the system suggests alternative followup actions or information that users may want or need.\nCurrently, these tend to be the most relevant and useful for users, because they\u2019re tailored to a specific need or interest.\n\n![Figure](../assets/www.nngroup.com/www.nngroup.com-articles-prompt-suggestions/83d11e4e618b.webp)\n<figcaption>Figure 1. Credit: [www.nngroup.com](https://media.nngroup.com/media/people/photos/Kate-Headshot-2022.jpg.256x256_q75_autocrop_crop-smart_upscale.jpg), License: internal-copy</figcaption>\n\n![Figure](../assets/www.nngroup.com/www.nngroup.com-articles-prompt-suggestions/29f27e555423.webp)\n<figcaption>Figure 2. Credit: [www.nngroup.com](https://media.nngroup.com/media/people/photos/Tim-portrait-2022.jpg.256x256_q75_autocrop_crop-smart_upscale.jpg), License: internal-copy</figcaption>\n\n![Traditional search suggestions: When users started typing within a Google search field, the system showed traditional search suggestions.](../assets/www.nngroup.com/www.nngroup.com-articles-prompt-suggestions/14347dce96d1.webp)\n<figcaption>Figure 3. Credit: [www.nngroup.com](https://media.nngroup.com/media/editor/2025/04/23/333.png), License: internal-copy</figcaption>\n\n![Enriched search suggestions:\u00a0Amazon showed enriched search suggestions, based on previous searches.](../assets/www.nngroup.com/www.nngroup.com-articles-prompt-suggestions/d007f418caff.webp)\n<figcaption>Figure 4. Credit: [www.nngroup.com](https://media.nngroup.com/media/editor/2025/04/23/222.png), License: internal-copy</figcaption>\n\n![Prompt suggestions:\u00a0ChatGPT displayed prompt suggestions before users started engaging with the system.](../assets/www.nngroup.com/www.nngroup.com-articles-prompt-suggestions/a150123ff0f9.webp)\n<figcaption>Figure 5. Credit: [www.nngroup.com](https://media.nngroup.com/media/editor/2025/04/23/3-prompt-suggestion.png), License: internal-copy</figcaption>\n\n![Manus: The use-case prompt suggestions below the chat field illustrated how people might use the pseudo agentic AI tool. When clicked, the use cases included complete replays of the conversations, so new users could see demonstrations of how they should use the product.](../assets/www.nngroup.com/www.nngroup.com-articles-prompt-suggestions/3d977cae090f.webp)\n<figcaption>Figure 6. Credit: [www.nngroup.com](https://media.nngroup.com/media/editor/2025/04/15/usecasepromptsuggestion1.png), License: internal-copy</figcaption>\n\n![ChatGPT: As the user typed \u201chow much should I feed,\u201d the system attempted to complete the question for her. These are prompt-autocomplete suggestions.](../assets/www.nngroup.com/www.nngroup.com-articles-prompt-suggestions/8459169d7e76.webp)\n<figcaption>Figure 7. Credit: [www.nngroup.com](https://media.nngroup.com/media/editor/2025/04/15/gpt1.png), License: internal-copy</figcaption>\n\n![ChatGPT: In this conversation, the system offered multiple followup questions relevant to the current conversation (no-yeast or thin-crust dough recipes for pizza.)](../assets/www.nngroup.com/www.nngroup.com-articles-prompt-suggestions/19ad55ea06af.webp)\n<figcaption>Figure 8. Credit: [www.nngroup.com](https://media.nngroup.com/media/editor/2025/04/15/followup-question-suggestion-chatgpt.png), License: internal-copy</figcaption>\n\n![Figure](../assets/www.nngroup.com/www.nngroup.com-articles-prompt-suggestions/54d3249df98d.webp)\n<figcaption>Figure 9. Credit: [www.nngroup.com](https://media.nngroup.com/media/videos/thumbnails/CARE_Structure_for_Crafting_AI_Prompts_Thumbnail.jpg.650x364_q75_autocrop_crop-smart_upscale.jpg), License: internal-copy</figcaption>\n\n![Figure](../assets/www.nngroup.com/www.nngroup.com-articles-prompt-suggestions/70d44a616567.webp)\n<figcaption>Figure 10. Credit: [www.nngroup.com](https://media.nngroup.com/media/videos/thumbnails/Synthetic_AI_Users_Thumbnail.jpg.650x364_q75_autocrop_crop-smart_upscale.jpg), License: internal-copy</figcaption>\n\n![Figure](../assets/www.nngroup.com/www.nngroup.com-articles-prompt-suggestions/abbd1f9a4631.webp)\n<figcaption>Figure 11. Credit: [www.nngroup.com](https://media.nngroup.com/media/videos/thumbnails/Your_AI_UX_Intern_Thumbnail.jpg.650x364_q75_autocrop_crop-smart_upscale.jpg), License: internal-copy</figcaption>\n"}, {"path": "/workspace/my-app/docs/sources/www.langchain.com-langgraph.md", "title": "www.langchain.com-langgraph", "text": "---\ntitle: www.langchain.com-langgraph\nslug: www.langchain.com-langgraph\ntags:\n- source\nwhy_for_designers: TODO\nbot_application: TODO\ncollaboration_prompts:\n- What retrieval strategy applies here?\nsources:\n- url: https://www.langchain.com/langgraph\n  title: www.langchain.com-langgraph\n  author: ''\n  license: internal-copy\n  retrieved_at: '2025-08-17'\n  policy: copy\nfigures:\n- path: ../assets/www.langchain.com/www.langchain.com-langgraph/886d194dcad1.webp\n  caption: Figure\n  credit_name: www.langchain.com\n  credit_url: https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66db8c2317fe5b9ad2b84ea0_lcacademylogo.png\n  license: internal-copy\nupdated_at: '2025-08-17'\ncompleted: false\n---\n\n# www.langchain.com-langgraph\n\n> Synthesis: TODO\n\n## Controllable cognitive architecture for any task\nLangGraph's flexible framework supports diverse control flows \u00e2 single agent, multi-agent, hierarchical, sequential \u00e2 and robustly handles realistic, complex scenarios.\nEnsure reliability with easy-to-add moderation and quality loops that prevent agents from veering off course.\nUse LangGraph Platform to templatize your cognitive architecture so that tools, prompts, and models are easily configurable with LangGraph Platform Assistants.\n\n![Figure](../assets/www.langchain.com/www.langchain.com-langgraph/886d194dcad1.webp)\n<figcaption>Figure 1. Credit: [www.langchain.com](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66db8c2317fe5b9ad2b84ea0_lcacademylogo.png), License: internal-copy</figcaption>\n"}, {"path": "/workspace/my-app/docs/sources/www.anthropic.com-engineering-building-effective-agents.md", "title": "www.anthropic.com-engineering-building-effective-agents", "text": "---\ntitle: www.anthropic.com-engineering-building-effective-agents\nslug: www.anthropic.com-engineering-building-effective-agents\ntags:\n- source\nwhy_for_designers: TODO\nbot_application: TODO\ncollaboration_prompts:\n- What retrieval strategy applies here?\nsources:\n- url: https://www.anthropic.com/engineering/building-effective-agents\n  title: www.anthropic.com-engineering-building-effective-agents\n  author: ''\n  license: internal-copy\n  retrieved_at: '2025-08-17'\n  policy: quote\nfigures:\n- path: ../assets/www.anthropic.com/www.anthropic.com-engineering-building-effective-agents/e7b97fc35541.webp\n  caption: The augmented LLM\n  credit_name: www.anthropic.com\n  credit_url: https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fd3083d3f40bb2b6f477901cc9a240738d3dd1371-2401x1000.png&w=3840&q=75\n  license: internal-copy\n- path: ../assets/www.anthropic.com/www.anthropic.com-engineering-building-effective-agents/190994fc386e.webp\n  caption: The prompt chaining workflow\n  credit_name: www.anthropic.com\n  credit_url: https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F7418719e3dab222dccb379b8879e1dc08ad34c78-2401x1000.png&w=3840&q=75\n  license: internal-copy\n- path: ../assets/www.anthropic.com/www.anthropic.com-engineering-building-effective-agents/a74d404aaa37.webp\n  caption: The routing workflow\n  credit_name: www.anthropic.com\n  credit_url: https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F5c0c0e9fe4def0b584c04d37849941da55e5e71c-2401x1000.png&w=3840&q=75\n  license: internal-copy\n- path: ../assets/www.anthropic.com/www.anthropic.com-engineering-building-effective-agents/b7e384eb5411.webp\n  caption: The parallelization workflow\n  credit_name: www.anthropic.com\n  credit_url: https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F406bb032ca007fd1624f261af717d70e6ca86286-2401x1000.png&w=3840&q=75\n  license: internal-copy\n- path: ../assets/www.anthropic.com/www.anthropic.com-engineering-building-effective-agents/1d85998e6a68.webp\n  caption: The orchestrator-workers workflow\n  credit_name: www.anthropic.com\n  credit_url: https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F8985fc683fae4780fb34eab1365ab78c7e51bc8e-2401x1000.png&w=3840&q=75\n  license: internal-copy\n- path: ../assets/www.anthropic.com/www.anthropic.com-engineering-building-effective-agents/9c9873b4e0a9.webp\n  caption: The evaluator-optimizer workflow\n  credit_name: www.anthropic.com\n  credit_url: https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F14f51e6406ccb29e695da48b17017e899a6119c7-2401x1000.png&w=3840&q=75\n  license: internal-copy\n- path: ../assets/www.anthropic.com/www.anthropic.com-engineering-building-effective-agents/aed8cefcfb11.webp\n  caption: Autonomous agent\n  credit_name: www.anthropic.com\n  credit_url: https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F58d9f10c985c4eb5d53798dea315f7bb5ab6249e-2401x1000.png&w=3840&q=75\n  license: internal-copy\n- path: ../assets/www.anthropic.com/www.anthropic.com-engineering-building-effective-agents/2f82b99cf57a.webp\n  caption: High-level flow of a coding agent\n  credit_name: www.anthropic.com\n  credit_url: https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F4b9a1f4eb63d5962a6e1746ac26bbc857cf3474f-2400x1666.png&w=3840&q=75\n  license: internal-copy\nupdated_at: '2025-08-17'\ncompleted: false\n---\n\n# www.anthropic.com-engineering-building-effective-agents\n\n> Synthesis: TODO\n\nOver the past year, we've worked with dozens of teams building large language model (LLM) agents across industries. Consistently, the most successful implementations weren't using complex frameworks or specialized libraries. Instead, they were building with simple, composable patterns.\nIn this post, we share what we\u2019ve learned from working with our customers and building agents ourselves, and give practical advice for developers on building effective agents.\n## What are agents?\n\"Agent\" can be defined in several ways. Some customers define agents as fully autonomous systems that operate independently over extended periods, using various tools to accomplish complex tasks. Others use the term to describe more prescriptive implementations that follow predefined workflows. At Anthropic, we categorize all these variations as\n**agentic systems**, but draw an important architectural distinction between **workflows **and ** agents**: **Workflows**are systems where LLMs and tools are orchestrated through predefined code paths. **Agents**, on the other hand, are systems where LLMs dynamically direct their own processes and tool usage, maintaining control over how they accomplish tasks.\nBelow, we will explore both types of agentic systems in detail. In Appendix 1 (\u201cAgents in Practice\u201d), we describe two domains where customers have found particular value in using these kinds of systems.\n## When (and when not) to use agents\nWhen building applications with LLMs, we recommend finding the simplest solution possible, and only increasing complexity when needed. This might mean not building agentic systems at all. Agentic systems often trade latency and cost for better task performance, and you should consider when this tradeoff makes sense.\nWhen more complexity is warranted, workflows offer predictability and consistency for well-defined tasks, whereas agents are the better option when flexibility and model-driven decision-making are needed at scale. For many applications, however, optimizing single LLM calls with retrieval and in-context examples is usually enough.\n## When and how to use frameworks\nThere are many frameworks that make agentic systems easier to implement, including:\n- LangGraph from LangChain;\n- Amazon Bedrock's AI Agent framework;\n- Rivet, a drag and drop GUI LLM workflow builder; and\n- Vellum, another GUI tool for building and testing complex workflows.\nThese frameworks make it easy to get started by simplifying standard low-level tasks like calling LLMs, defining and parsing tools, and chaining calls together. However, they often create extra layers of abstraction that can obscure the underlying prompts and responses, making them harder to debug. They can also make it tempting to add complexity when a simpler setup would suffice.\nWe suggest that developers start by using LLM APIs directly: many patterns can be implemented in a few lines of code. If you do use a framework, ensure you understand the underlying code. Incorrect assumptions about what's under the hood are a common source of customer error.\nSee our cookbook for some sample implementations.\n## Building blocks, workflows, and agents\nIn this section, we\u2019ll explore the common patterns for agentic systems we\u2019ve seen in production. We'll start with our foundational building block\u2014the augmented LLM\u2014and progressively increase complexity, from simple compositional workflows to autonomous agents.\n### Building block: The augmented LLM\nThe basic building block of agentic systems is an LLM enhanced with augmentations such as retrieval, tools, and memory. Our current models can actively use these capabilities\u2014generating their own search queries, selecting appropriate tools, and determining what information to retain.\nWe recommend focusing on two key aspects of the implementation: tailoring these capabilities to your specific use case and ensuring they provide an easy, well-documented interface for your LLM. While there are many ways to implement these augmentations, one approach is through our recently released Model Context Protocol, which allows developers to integrate with a growing ecosystem of third-party tools with a simple client implementation.\nFor the remainder of this post, we'll assume each LLM call has access to these augmented capabilities.\n### Workflow: Prompt chaining\nPrompt chaining decomposes a task into a sequence of steps, where each LLM call processes the output of the previous one. You can add programmatic checks (see \"gate\u201d in the diagram below) on any intermediate steps to ensure that the process is still on track.\n**When to use this workflow:** This workflow is ideal for situations where the task can be easily and cleanly decomposed into fixed subtasks. The main goal is to trade off latency for higher accuracy, by making each LLM call an easier task. **Examples where prompt chaining is useful:**\n- Generating Marketing copy, then translating it into a different language.\n- Writing an outline of a document, checking that the outline meets certain criteria, then writing the document based on the outline.\n### Workflow: Routing\nRouting classifies an input and directs it to a specialized followup task. This workflow allows for separation of concerns, and building more specialized prompts. Without this workflow, optimizing for one kind of input can hurt performance on other inputs.\n**When to use this workflow:** Routing works well for complex tasks where there are distinct categories that are better handled separately, and where classification can be handled accurately, either by an LLM or a more traditional classification model/algorithm. **Examples where routing is useful:**\n- Directing different types of customer service queries (general questions, refund requests, technical support) into different downstream processes, prompts, and tools.\n- Routing easy/common questions to smaller models like Claude 3.5 Haiku and hard/unusual questions to more capable models like Claude 3.5 Sonnet to optimize cost and speed.\n### Workflow: Parallelization\nLLMs can sometimes work simultaneously on a task and have their outputs aggregated programmatically. This workflow, parallelization, manifests in two key variations:\n**Sectioning**: Breaking a task into independent subtasks run in parallel. **Voting:**Running the same task multiple times to get diverse outputs. **When to use this workflow:** Parallelization is effective when the divided subtasks can be parallelized for speed, or when multiple perspectives or attempts are needed for higher confidence results. For complex tasks with multiple considerations, LLMs generally perform better when each consideration is handled by a separate LLM call, allowing focused attention on each specific aspect. **Examples where parallelization is useful:** **Sectioning**:\n- Implementing guardrails where one model instance processes user queries while another screens them for inappropriate content or requests. This tends to perform better than having the same LLM call handle both guardrails and the core response.\n- Automating evals for evaluating LLM performance, where each LLM call evaluates a different aspect of the model\u2019s performance on a given prompt.\n**Voting**:\n- Reviewing a piece of code for vulnerabilities, where several different prompts review and flag the code if they find a problem.\n- Evaluating whether a given piece of content is inappropriate, with multiple prompts evaluating different aspects or requiring different vote thresholds to balance false positives and negatives.\n### Workflow: Orchestrator-workers\nIn the orchestrator-workers workflow, a central LLM dynamically breaks down tasks, delegates them to worker LLMs, and synthesizes their results.\n**When to use this workflow:** This workflow is well-suited for complex tasks where you can\u2019t predict the subtasks needed (in coding, for example, the number of files that need to be changed and the nature of the change in each file likely depend on the task). Whereas it\u2019s topographically similar, the key difference from parallelization is its flexibility\u2014subtasks aren't pre-defined, but determined by the orchestrator based on the specific input. **Example where orchestrator-workers is useful:**\n- Coding products that make complex changes to multiple files each time.\n- Search tasks that involve gathering and analyzing information from multiple sources for possible relevant information.\n### Workflow: Evaluator-optimizer\nIn the evaluator-optimizer workflow, one LLM call generates a response while another provides evaluation and feedback in a loop.\n**When to use this workflow:** This workflow is particularly effective when we have clear evaluation criteria, and when iterative refinement provides measurable value. The two signs of good fit are, first, that LLM responses can be demonstrably improved when a human articulates their feedback; and second, that the LLM can provide such feedback. This is analogous to the iterative writing process a human writer might go through when producing a polished document. **Examples where evaluator-optimizer is useful:**\n- Literary translation where there are nuances that the translator LLM might not capture initially, but where an evaluator LLM can provide useful critiques.\n- Complex search tasks that require multiple rounds of searching and analysis to gather comprehensive information, where the evaluator decides whether further searches are warranted.\n### Agents\nAgents are emerging in production as LLMs mature in key capabilities\u2014understanding complex inputs, engaging in reasoning and planning, using tools reliably, and recovering from errors. Agents begin their work with either a command from, or interactive discussion with, the human user. Once the task is clear, agents plan and operate independently, potentially returning to the human for further information or judgement. During execution, it's crucial for the agents to gain \u201cground truth\u201d from the environment at each step (such as tool call results or code execution) to assess its progress. Agents can then pause for human feedback at checkpoints or when encountering blockers. The task often terminates upon completion, but it\u2019s also common to include stopping conditions (such as a maximum number of iterations) to maintain control.\nAgents can handle sophisticated tasks, but their implementation is often straightforward. They are typically just LLMs using tools based on environmental feedback in a loop. It is therefore crucial to design toolsets and their documentation clearly and thoughtfully. We expand on best practices for tool development in Appendix 2 (\"Prompt Engineering your Tools\").\n**When to use agents:** Agents can be used for open-ended problems where it\u2019s difficult or impossible to predict the required number of steps, and where you can\u2019t hardcode a fixed path. The LLM will potentially operate for many turns, and you must have some level of trust in its decision-making. Agents' autonomy makes them ideal for scaling tasks in trusted environments.\nThe autonomous nature of agents means higher costs, and the potential for compounding errors. We recommend extensive testing in sandboxed environments, along with the appropriate guardrails.\n**Examples where agents are useful:**\nThe following examples are from our own implementations:\n- A coding Agent to resolve SWE-bench tasks, which involve edits to many files based on a task description;\n- Our \u201ccomputer use\u201d reference implementation, where Claude uses a computer to accomplish tasks.\n## Combining and customizing these patterns\nThese building blocks aren't prescriptive. They're common patterns that developers can shape and combine to fit different use cases. The key to success, as with any LLM features, is measuring performance and iterating on implementations. To repeat: you should consider adding complexity\n*only* when it demonstrably improves outcomes.\n## Summary\nSuccess in the LLM space isn't about building the most sophisticated system. It's about building the\n*right* system for your needs. Start with simple prompts, optimize them with comprehensive evaluation, and add multi-step agentic systems only when simpler solutions fall short.\nWhen implementing agents, we try to follow three core principles:\n- Maintain\n**simplicity**in your agent's design.\n- Prioritize\n**transparency**by explicitly showing the agent\u2019s planning steps.\n- Carefully craft your agent-computer interface (ACI) through thorough tool\n**documentation and testing**.\nFrameworks can help you get started quickly, but don't hesitate to reduce abstraction layers and build with basic components as you move to production. By following these principles, you can create agents that are not only powerful but also reliable, maintainable, and trusted by their users.\n### Acknowledgements\nWritten by Erik Schluntz and Barry Zhang. This work draws upon our experiences building agents at Anthropic and the valuable insights shared by our customers, for which we're deeply grateful.\n## Appendix 1: Agents in practice\nOur work with customers has revealed two particularly promising applications for AI agents that demonstrate the practical value of the patterns discussed above. Both applications illustrate how agents add the most value for tasks that require both conversation and action, have clear success criteria, enable feedback loops, and integrate meaningful human oversight.\n### A. Customer support\nCustomer support combines familiar chatbot interfaces with enhanced capabilities through tool integration. This is a natural fit for more open-ended agents because:\n- Support interactions naturally follow a conversation flow while requiring access to external information and actions;\n- Tools can be integrated to pull customer data, order history, and knowledge base articles;\n- Actions such as issuing refunds or updating tickets can be handled programmatically; and\n- Success can be clearly measured through user-defined resolutions.\nSeveral companies have demonstrated the viability of this approach through usage-based pricing models that charge only for successful resolutions, showing confidence in their agents' effectiveness.\n### B. Coding agents\nThe software development space has shown remarkable potential for LLM features, with capabilities evolving from code completion to autonomous problem-solving. Agents are particularly effective because:\n- Code solutions are verifiable through automated tests;\n- Agents can iterate on solutions using test results as feedback;\n- The problem space is well-defined and structured; and\n- Output quality can be measured objectively.\nIn our own implementation, agents can now solve real GitHub issues in the SWE-bench Verified benchmark based on the pull request description alone. However, whereas automated testing helps verify functionality, human review remains crucial for ensuring solutions align with broader system requirements.\n## Appendix 2: Prompt engineering your tools\nNo matter which agentic system you're building, tools will likely be an important part of your agent. Tools enable Claude to interact with external services and APIs by specifying their exact structure and definition in our API. When Claude responds, it will include a tool use block in the API response if it plans to invoke a tool. Tool definitions and specifications should be given just as much prompt engineering attention as your overall prompts. In this brief appendix, we describe how to prompt engineer your tools.\nThere are often several ways to specify the same action. For instance, you can specify a file edit by writing a diff, or by rewriting the entire file. For structured output, you can return code inside markdown or inside JSON. In software engineering, differences like these are cosmetic and can be converted losslessly from one to the other. However, some formats are much more difficult for an LLM to write than others. Writing a diff requires knowing how many lines are changing in the chunk header before the new code is written. Writing code inside JSON (compared to markdown) requires extra escaping of newlines and quotes.\nOur suggestions for deciding on tool formats are the following:\n- Give the model enough tokens to \"think\" before it writes itself into a corner.\n- Keep the format close to what the model has seen naturally occurring in text on the internet.\n- Make sure there's no formatting \"overhead\" such as having to keep an accurate count of thousands of lines of code, or string-escaping any code it writes.\nOne rule of thumb is to think about how much effort goes into human-computer interfaces (HCI), and plan to invest just as much effort in creating good\n*agent*-computer interfaces (ACI). Here are some thoughts on how to do so:\n- Put yourself in the model's shoes. Is it obvious how to use this tool, based on the description and parameters, or would you need to think carefully about it? If so, then it\u2019s probably also true for the model. A good tool definition often includes example usage, edge cases, input format requirements, and clear boundaries from other tools.\n- How can you change parameter names or descriptions to make things more obvious? Think of this as writing a great docstring for a junior developer on your team. This is especially important when using many similar tools.\n- Test how the model uses your tools: Run many example inputs in our workbench to see what mistakes the model makes, and iterate.\n- Poka-yoke your tools. Change the arguments so that it is harder to make mistakes.\nWhile building our agent for SWE-bench, we actually spent more time optimizing our tools than the overall prompt. For example, we found that the model would make mistakes with tools using relative filepaths after the agent had moved out of the root directory. To fix this, we changed the tool to always require absolute filepaths\u2014and we found that the model used this method flawlessly.\n\n![The augmented LLM](../assets/www.anthropic.com/www.anthropic.com-engineering-building-effective-agents/e7b97fc35541.webp)\n<figcaption>Figure 1. Credit: [www.anthropic.com](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fd3083d3f40bb2b6f477901cc9a240738d3dd1371-2401x1000.png&w=3840&q=75), License: internal-copy</figcaption>\n\n![The prompt chaining workflow](../assets/www.anthropic.com/www.anthropic.com-engineering-building-effective-agents/190994fc386e.webp)\n<figcaption>Figure 2. Credit: [www.anthropic.com](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F7418719e3dab222dccb379b8879e1dc08ad34c78-2401x1000.png&w=3840&q=75), License: internal-copy</figcaption>\n\n![The routing workflow](../assets/www.anthropic.com/www.anthropic.com-engineering-building-effective-agents/a74d404aaa37.webp)\n<figcaption>Figure 3. Credit: [www.anthropic.com](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F5c0c0e9fe4def0b584c04d37849941da55e5e71c-2401x1000.png&w=3840&q=75), License: internal-copy</figcaption>\n\n![The parallelization workflow](../assets/www.anthropic.com/www.anthropic.com-engineering-building-effective-agents/b7e384eb5411.webp)\n<figcaption>Figure 4. Credit: [www.anthropic.com](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F406bb032ca007fd1624f261af717d70e6ca86286-2401x1000.png&w=3840&q=75), License: internal-copy</figcaption>\n\n![The orchestrator-workers workflow](../assets/www.anthropic.com/www.anthropic.com-engineering-building-effective-agents/1d85998e6a68.webp)\n<figcaption>Figure 5. Credit: [www.anthropic.com](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F8985fc683fae4780fb34eab1365ab78c7e51bc8e-2401x1000.png&w=3840&q=75), License: internal-copy</figcaption>\n\n![The evaluator-optimizer workflow](../assets/www.anthropic.com/www.anthropic.com-engineering-building-effective-agents/9c9873b4e0a9.webp)\n<figcaption>Figure 6. Credit: [www.anthropic.com](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F14f51e6406ccb29e695da48b17017e899a6119c7-2401x1000.png&w=3840&q=75), License: internal-copy</figcaption>\n\n![Autonomous agent](../assets/www.anthropic.com/www.anthropic.com-engineering-building-effective-agents/aed8cefcfb11.webp)\n<figcaption>Figure 7. Credit: [www.anthropic.com](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F58d9f10c985c4eb5d53798dea315f7bb5ab6249e-2401x1000.png&w=3840&q=75), License: internal-copy</figcaption>\n\n![High-level flow of a coding agent](../assets/www.anthropic.com/www.anthropic.com-engineering-building-effective-agents/2f82b99cf57a.webp)\n<figcaption>Figure 8. Credit: [www.anthropic.com](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F4b9a1f4eb63d5962a6e1746ac26bbc857cf3474f-2400x1666.png&w=3840&q=75), License: internal-copy</figcaption>\n"}, {"path": "/workspace/my-app/docs/sources/docs.crewai.com-en-concepts-knowledge.md", "title": "docs.crewai.com-en-concepts-knowledge", "text": "---\ntitle: docs.crewai.com-en-concepts-knowledge\nslug: docs.crewai.com-en-concepts-knowledge\ntags:\n- source\nwhy_for_designers: TODO\nbot_application: TODO\ncollaboration_prompts:\n- What retrieval strategy applies here?\nsources:\n- url: https://docs.crewai.com/en/concepts/knowledge\n  title: docs.crewai.com-en-concepts-knowledge\n  author: ''\n  license: internal-copy\n  retrieved_at: '2025-08-17'\n  policy: copy\nfigures:\n- path: ../assets/docs.crewai.com/docs.crewai.com-en-concepts-knowledge/71bc45159c09.webp\n  caption: light logo\n  credit_name: docs.crewai.com\n  credit_url: https://mintlify.s3.us-west-1.amazonaws.com/crewai/images/crew_only_logo.png\n  license: internal-copy\n- path: ../assets/docs.crewai.com/docs.crewai.com-en-concepts-knowledge/71bc45159c09.webp\n  caption: dark logo\n  credit_name: docs.crewai.com\n  credit_url: https://mintlify.s3.us-west-1.amazonaws.com/crewai/images/crew_only_logo.png\n  license: internal-copy\nupdated_at: '2025-08-17'\ncompleted: false\n---\n\n# docs.crewai.com-en-concepts-knowledge\n\n> Synthesis: TODO\n\nWhat is knowledge in CrewAI and how to use it.\nknowledge directory at the root of your project.\nAlso, use relative paths from the\nknowledge directory when creating the source.\nfrom crewai import Agent, Task, Crew, Process, LLM\nfrom crewai.knowledge.source.string_knowledge_source import StringKnowledgeSource\n# Create a knowledge source\ncontent = \"Users name is John. He is 30 years old and lives in San Francisco.\"\nstring_source = StringKnowledgeSource(content=content)\n# Create an LLM with a temperature of 0 to ensure deterministic outputs\nllm = LLM(model=\"gpt-4o-mini\", temperature=0)\n# Create an agent with the knowledge store\nagent = Agent(\nrole=\"About User\",\ngoal=\"You know everything about the user.\",\nbackstory=\"You are a master at understanding people and their preferences.\",\nverbose=True,\nallow_delegation=False,\nllm=llm,\n)\ntask = Task(\ndescription=\"Answer the following questions about the user: {question}\",\nexpected_output=\"An answer to the question.\",\nagent=agent,\n)\ncrew = Crew(\nagents=[agent],\ntasks=[task],\nverbose=True,\nprocess=Process.sequential,\nknowledge_sources=[string_source], # Enable knowledge by adding the sources here\n)\nresult = crew.kickoff(inputs={\"question\": \"What city does John live in and how old is he?\"})\ndocling for the following example to work:\nuv add docling\nfrom crewai import LLM, Agent, Crew, Process, Task\nfrom crewai.knowledge.source.crew_docling_source import CrewDoclingSource\n# Create a knowledge source from web content\ncontent_source = CrewDoclingSource(\nfile_paths=[\n\"https://lilianweng.github.io/posts/2024-11-28-reward-hacking\",\n\"https://lilianweng.github.io/posts/2024-07-07-hallucination\",\n],\n)\n# Create an LLM with a temperature of 0 to ensure deterministic outputs\nllm = LLM(model=\"gpt-4o-mini\", temperature=0)\n# Create an agent with the knowledge store\nagent = Agent(\nrole=\"About papers\",\ngoal=\"You know everything about the papers.\",\nbackstory=\"You are a master at understanding papers and their content.\",\nverbose=True,\nallow_delegation=False,\nllm=llm,\n)\ntask = Task(\ndescription=\"Answer the following questions about the papers: {question}\",\nexpected_output=\"An answer to the question.\",\nagent=agent,\n)\ncrew = Crew(\nagents=[agent],\ntasks=[task],\nverbose=True,\nprocess=Process.sequential,\nknowledge_sources=[content_source],\n)\nresult = crew.kickoff(\ninputs={\"question\": \"What is the reward hacking paper about? Be sure to provide sources.\"}\n)\nfrom crewai.knowledge.source.text_file_knowledge_source import TextFileKnowledgeSource\ntext_source = TextFileKnowledgeSource(\nfile_paths=[\"document.txt\", \"another.txt\"]\n)\nfrom crewai.knowledge.source.pdf_knowledge_source import PDFKnowledgeSource\npdf_source = PDFKnowledgeSource(\nfile_paths=[\"document.pdf\", \"another.pdf\"]\n)\nfrom crewai.knowledge.source.csv_knowledge_source import CSVKnowledgeSource\ncsv_source = CSVKnowledgeSource(\nfile_paths=[\"data.csv\"]\n)\nfrom crewai.knowledge.source.excel_knowledge_source import ExcelKnowledgeSource\nexcel_source = ExcelKnowledgeSource(\nfile_paths=[\"spreadsheet.xlsx\"]\n)\nfrom crewai.knowledge.source.json_knowledge_source import JSONKnowledgeSource\njson_source = JSONKnowledgeSource(\nfile_paths=[\"data.json\"]\n)\nfrom crewai import Agent, Task, Crew\nfrom crewai.knowledge.source.string_knowledge_source import StringKnowledgeSource\n# Agent with its own knowledge - NO crew knowledge needed\nspecialist_knowledge = StringKnowledgeSource(\ncontent=\"Specialized technical information for this agent only\"\n)\nspecialist_agent = Agent(\nrole=\"Technical Specialist\",\ngoal=\"Provide technical expertise\",\nbackstory=\"Expert in specialized technical domains\",\nknowledge_sources=[specialist_knowledge] # Agent-specific knowledge\n)\ntask = Task(\ndescription=\"Answer technical questions\",\nagent=specialist_agent,\nexpected_output=\"Technical answer\"\n)\n# No crew-level knowledge required\ncrew = Crew(\nagents=[specialist_agent],\ntasks=[task]\n)\nresult = crew.kickoff() # Agent knowledge works independently\ncrew.kickoff()\ncrew.kickoff(), here\u2019s the exact sequence:\n# During kickoff\nfor agent in self.agents:\nagent.crew = self # Agent gets reference to crew\nagent.set_knowledge(crew_embedder=self.embedder) # Agent knowledge initialized\nagent.create_agent_executor()\n# Agent knowledge storage\nagent_collection_name = agent.role # e.g., \"Technical Specialist\"\n# Crew knowledge storage\ncrew_collection_name = \"crew\"\n# Both stored in same ChromaDB instance but different collections\n# Path: ~/.local/share/CrewAI/{project}/knowledge/\n# \u251c\u2500\u2500 crew/ # Crew knowledge collection\n# \u251c\u2500\u2500 Technical Specialist/ # Agent knowledge collection\n# \u2514\u2500\u2500 Another Agent Role/ # Another agent's collection\nfrom crewai import Agent, Task, Crew\nfrom crewai.knowledge.source.string_knowledge_source import StringKnowledgeSource\n# Agent-specific knowledge\nagent_knowledge = StringKnowledgeSource(\ncontent=\"Agent-specific information that only this agent needs\"\n)\nagent = Agent(\nrole=\"Specialist\",\ngoal=\"Use specialized knowledge\",\nbackstory=\"Expert with specific knowledge\",\nknowledge_sources=[agent_knowledge],\nembedder={ # Agent can have its own embedder\n\"provider\": \"openai\",\n\"config\": {\"model\": \"text-embedding-3-small\"}\n}\n)\ntask = Task(\ndescription=\"Answer using your specialized knowledge\",\nagent=agent,\nexpected_output=\"Answer based on agent knowledge\"\n)\n# No crew knowledge needed\ncrew = Crew(agents=[agent], tasks=[task])\nresult = crew.kickoff() # Works perfectly\n# Crew-wide knowledge (shared by all agents)\ncrew_knowledge = StringKnowledgeSource(\ncontent=\"Company policies and general information for all agents\"\n)\n# Agent-specific knowledge\nspecialist_knowledge = StringKnowledgeSource(\ncontent=\"Technical specifications only the specialist needs\"\n)\nspecialist = Agent(\nrole=\"Technical Specialist\",\ngoal=\"Provide technical expertise\",\nbackstory=\"Technical expert\",\nknowledge_sources=[specialist_knowledge] # Agent-specific\n)\ngeneralist = Agent(\nrole=\"General Assistant\",\ngoal=\"Provide general assistance\",\nbackstory=\"General helper\"\n# No agent-specific knowledge\n)\ncrew = Crew(\nagents=[specialist, generalist],\ntasks=[...],\nknowledge_sources=[crew_knowledge] # Crew-wide knowledge\n)\n# Result:\n# - specialist gets: crew_knowledge + specialist_knowledge\n# - generalist gets: crew_knowledge only\n# Different knowledge for different agents\nsales_knowledge = StringKnowledgeSource(content=\"Sales procedures and pricing\")\ntech_knowledge = StringKnowledgeSource(content=\"Technical documentation\")\nsupport_knowledge = StringKnowledgeSource(content=\"Support procedures\")\nsales_agent = Agent(\nrole=\"Sales Representative\",\nknowledge_sources=[sales_knowledge],\nembedder={\"provider\": \"openai\", \"config\": {\"model\": \"text-embedding-3-small\"}}\n)\ntech_agent = Agent(\nrole=\"Technical Expert\",\nknowledge_sources=[tech_knowledge],\nembedder={\"provider\": \"ollama\", \"config\": {\"model\": \"mxbai-embed-large\"}}\n)\nsupport_agent = Agent(\nrole=\"Support Specialist\",\nknowledge_sources=[support_knowledge]\n# Will use crew embedder as fallback\n)\ncrew = Crew(\nagents=[sales_agent, tech_agent, support_agent],\ntasks=[...],\nembedder={ # Fallback embedder for agents without their own\n\"provider\": \"google\",\n\"config\": {\"model\": \"text-embedding-004\"}\n}\n)\n# Each agent gets only their specific knowledge\n# Each can use different embedding providers\nfrom crewai.knowledge.knowledge_config import KnowledgeConfig\nknowledge_config = KnowledgeConfig(results_limit=10, score_threshold=0.5)\nagent = Agent(\n...\nknowledge_config=knowledge_config\n)\nresults_limit: is the number of relevant documents to return. Default is 3.\nscore_threshold: is the minimum score for a document to be considered relevant. Default is 0.35.\n~/Library/Application Support/CrewAI/{project_name}/\n\u2514\u2500\u2500 knowledge/ # Knowledge ChromaDB files\n\u251c\u2500\u2500 chroma.sqlite3 # ChromaDB metadata\n\u251c\u2500\u2500 {collection_id}/ # Vector embeddings\n\u2514\u2500\u2500 knowledge_{collection}/ # Named collections\n~/.local/share/CrewAI/{project_name}/\n\u2514\u2500\u2500 knowledge/\n\u251c\u2500\u2500 chroma.sqlite3\n\u251c\u2500\u2500 {collection_id}/\n\u2514\u2500\u2500 knowledge_{collection}/\nC:\\Users\\{username}\\AppData\\Local\\CrewAI\\{project_name}\\\n\u2514\u2500\u2500 knowledge\\\n\u251c\u2500\u2500 chroma.sqlite3\n\u251c\u2500\u2500 {collection_id}\\\n\u2514\u2500\u2500 knowledge_{collection}\\\nfrom crewai.utilities.paths import db_storage_path\nimport os\n# Get the knowledge storage path\nknowledge_path = os.path.join(db_storage_path(), \"knowledge\")\nprint(f\"Knowledge storage location: {knowledge_path}\")\n# List knowledge collections and files\nif os.path.exists(knowledge_path):\nprint(\"\\nKnowledge storage contents:\")\nfor item in os.listdir(knowledge_path):\nitem_path = os.path.join(knowledge_path, item)\nif os.path.isdir(item_path):\nprint(f\"\ud83d\udcc1 Collection: {item}/\")\n# Show collection contents\ntry:\nfor subitem in os.listdir(item_path):\nprint(f\" \u2514\u2500\u2500 {subitem}\")\nexcept PermissionError:\nprint(f\" \u2514\u2500\u2500 (permission denied)\")\nelse:\nprint(f\"\ud83d\udcc4 {item}\")\nelse:\nprint(\"No knowledge storage found yet.\")\nimport os\nfrom crewai import Crew\n# Set custom storage location for all CrewAI data\nos.environ[\"CREWAI_STORAGE_DIR\"] = \"./my_project_storage\"\n# All knowledge will now be stored in ./my_project_storage/knowledge/\ncrew = Crew(\nagents=[...],\ntasks=[...],\nknowledge_sources=[...]\n)\nfrom crewai.knowledge.storage.knowledge_storage import KnowledgeStorage\nfrom crewai.knowledge.source.string_knowledge_source import StringKnowledgeSource\n# Create custom storage with specific embedder\ncustom_storage = KnowledgeStorage(\nembedder={\n\"provider\": \"ollama\",\n\"config\": {\"model\": \"mxbai-embed-large\"}\n},\ncollection_name=\"my_custom_knowledge\"\n)\n# Use with knowledge sources\nknowledge_source = StringKnowledgeSource(\ncontent=\"Your knowledge content here\"\n)\nknowledge_source.storage = custom_storage\nimport os\nfrom pathlib import Path\n# Store knowledge in project directory\nproject_root = Path(__file__).parent\nknowledge_dir = project_root / \"knowledge_storage\"\nos.environ[\"CREWAI_STORAGE_DIR\"] = str(knowledge_dir)\n# Now all knowledge will be stored in your project directory\ntext-embedding-3-small) for knowledge storage, even when using different LLM providers. You can easily customize this to match your setup.\nfrom crewai import Agent, Crew, LLM\nfrom crewai.knowledge.source.string_knowledge_source import StringKnowledgeSource\n# When using Claude as your LLM...\nagent = Agent(\nrole=\"Researcher\",\ngoal=\"Research topics\",\nbackstory=\"Expert researcher\",\nllm=LLM(provider=\"anthropic\", model=\"claude-3-sonnet\") # Using Claude\n)\n# CrewAI will still use OpenAI embeddings by default for knowledge\n# This ensures consistency but may not match your LLM provider preference\nknowledge_source = StringKnowledgeSource(content=\"Research data...\")\ncrew = Crew(\nagents=[agent],\ntasks=[...],\nknowledge_sources=[knowledge_source]\n# Default: Uses OpenAI embeddings even with Claude LLM\n)\n# Option 1: Use Voyage AI (recommended by Anthropic for Claude users)\ncrew = Crew(\nagents=[agent],\ntasks=[...],\nknowledge_sources=[knowledge_source],\nembedder={\n\"provider\": \"voyageai\", # Recommended for Claude users\n\"config\": {\n\"api_key\": \"your-voyage-api-key\",\n\"model\": \"voyage-3\" # or \"voyage-3-large\" for best quality\n}\n}\n)\n# Option 2: Use local embeddings (no external API calls)\ncrew = Crew(\nagents=[agent],\ntasks=[...],\nknowledge_sources=[knowledge_source],\nembedder={\n\"provider\": \"ollama\",\n\"config\": {\n\"model\": \"mxbai-embed-large\",\n\"url\": \"http://localhost:11434/api/embeddings\"\n}\n}\n)\n# Option 3: Agent-level embedding customization\nagent = Agent(\nrole=\"Researcher\",\ngoal=\"Research topics\",\nbackstory=\"Expert researcher\",\nknowledge_sources=[knowledge_source],\nembedder={\n\"provider\": \"google\",\n\"config\": {\n\"model\": \"models/text-embedding-004\",\n\"api_key\": \"your-google-key\"\n}\n}\n)\nagent = Agent(\nrole=\"Researcher\",\ngoal=\"Research topics\",\nbackstory=\"Expert researcher\",\nknowledge_sources=[knowledge_source],\nembedder={\n\"provider\": \"azure\",\n\"config\": {\n\"api_key\": \"your-azure-api-key\",\n\"model\": \"text-embedding-ada-002\", # change to the model you are using and is deployed in Azure\n\"api_base\": \"https://your-azure-endpoint.openai.azure.com/\",\n\"api_version\": \"2024-02-01\"\n}\n}\n)\n_get_knowledge_search_query method is triggered\n# Original task prompt\ntask_prompt = \"Answer the following questions about the user's favorite movies: What movie did John watch last week? Format your answer in JSON.\"\n# Behind the scenes, this might be rewritten as:\nrewritten_query = \"What movies did John watch last week?\"\nfrom crewai.utilities.events import (\nKnowledgeRetrievalStartedEvent,\nKnowledgeRetrievalCompletedEvent,\n)\nfrom crewai.utilities.events.base_event_listener import BaseEventListener\nclass KnowledgeMonitorListener(BaseEventListener):\ndef setup_listeners(self, crewai_event_bus):\n@crewai_event_bus.on(KnowledgeRetrievalStartedEvent)\ndef on_knowledge_retrieval_started(source, event):\nprint(f\"Agent '{event.agent.role}' started retrieving knowledge\")\n@crewai_event_bus.on(KnowledgeRetrievalCompletedEvent)\ndef on_knowledge_retrieval_completed(source, event):\nprint(f\"Agent '{event.agent.role}' completed knowledge retrieval\")\nprint(f\"Query: {event.query}\")\nprint(f\"Retrieved {len(event.retrieved_knowledge)} knowledge chunks\")\n# Create an instance of your listener\nknowledge_monitor = KnowledgeMonitorListener()\nBaseKnowledgeSource class. Let\u2019s create a practical example that fetches and processes space news articles.\nfrom crewai import Agent, Task, Crew, Process, LLM\nfrom crewai.knowledge.source.base_knowledge_source import BaseKnowledgeSource\nimport requests\nfrom datetime import datetime\nfrom typing import Dict, Any\nfrom pydantic import BaseModel, Field\nclass SpaceNewsKnowledgeSource(BaseKnowledgeSource):\n\"\"\"Knowledge source that fetches data from Space News API.\"\"\"\napi_endpoint: str = Field(description=\"API endpoint URL\")\nlimit: int = Field(default=10, description=\"Number of articles to fetch\")\ndef load_content(self) -> Dict[Any, str]:\n\"\"\"Fetch and format space news articles.\"\"\"\ntry:\nresponse = requests.get(\nf\"{self.api_endpoint}?limit={self.limit}\"\n)\nresponse.raise_for_status()\ndata = response.json()\narticles = data.get('results', [])\nformatted_data = self.validate_content(articles)\nreturn {self.api_endpoint: formatted_data}\nexcept Exception as e:\nraise ValueError(f\"Failed to fetch space news: {str(e)}\")\ndef validate_content(self, articles: list) -> str:\n\"\"\"Format articles into readable text.\"\"\"\nformatted = \"Space News Articles:\\n\\n\"\nfor article in articles:\nformatted += f\"\"\"\nTitle: {article['title']}\nPublished: {article['published_at']}\nSummary: {article['summary']}\nNews Site: {article['news_site']}\nURL: {article['url']}\n-------------------\"\"\"\nreturn formatted\ndef add(self) -> None:\n\"\"\"Process and store the articles.\"\"\"\ncontent = self.load_content()\nfor _, text in content.items():\nchunks = self._chunk_text(text)\nself.chunks.extend(chunks)\nself._save_documents()\n# Create knowledge source\nrecent_news = SpaceNewsKnowledgeSource(\napi_endpoint=\"https://api.spaceflightnewsapi.net/v4/articles\",\nlimit=10,\n)\n# Create specialized agent\nspace_analyst = Agent(\nrole=\"Space News Analyst\",\ngoal=\"Answer questions about space news accurately and comprehensively\",\nbackstory=\"\"\"You are a space industry analyst with expertise in space exploration,\nsatellite technology, and space industry trends. You excel at answering questions\nabout space news and providing detailed, accurate information.\"\"\",\nknowledge_sources=[recent_news],\nllm=LLM(model=\"gpt-4\", temperature=0.0)\n)\n# Create task that handles user questions\nanalysis_task = Task(\ndescription=\"Answer this question about space news: {user_question}\",\nexpected_output=\"A detailed answer based on the recent space news articles\",\nagent=space_analyst\n)\n# Create and run the crew\ncrew = Crew(\nagents=[space_analyst],\ntasks=[analysis_task],\nverbose=True,\nprocess=Process.sequential\n)\n# Example usage\nresult = crew.kickoff(\ninputs={\"user_question\": \"What are the latest developments in space exploration?\"}\n)\nfrom crewai import Agent, Crew, Task\nfrom crewai.knowledge.source.string_knowledge_source import StringKnowledgeSource\nknowledge_source = StringKnowledgeSource(content=\"Test knowledge\")\nagent = Agent(\nrole=\"Test Agent\",\ngoal=\"Test knowledge\",\nbackstory=\"Testing\",\nknowledge_sources=[knowledge_source]\n)\ncrew = Crew(agents=[agent], tasks=[Task(...)])\n# Before kickoff - knowledge not initialized\nprint(f\"Before kickoff - Agent knowledge: {getattr(agent, 'knowledge', None)}\")\ncrew.kickoff()\n# After kickoff - knowledge initialized\nprint(f\"After kickoff - Agent knowledge: {agent.knowledge}\")\nprint(f\"Agent knowledge collection: {agent.knowledge.storage.collection_name}\")\nprint(f\"Number of sources: {len(agent.knowledge.sources)}\")\nimport os\nfrom crewai.utilities.paths import db_storage_path\n# Check storage structure\nstorage_path = db_storage_path()\nknowledge_path = os.path.join(storage_path, \"knowledge\")\nif os.path.exists(knowledge_path):\nprint(\"Knowledge collections found:\")\nfor collection in os.listdir(knowledge_path):\ncollection_path = os.path.join(knowledge_path, collection)\nif os.path.isdir(collection_path):\nprint(f\" - {collection}/\")\n# Show collection contents\nfor item in os.listdir(collection_path):\nprint(f\" \u2514\u2500\u2500 {item}\")\n# Test agent knowledge retrieval\nif hasattr(agent, 'knowledge') and agent.knowledge:\ntest_query = [\"test query\"]\nresults = agent.knowledge.query(test_query)\nprint(f\"Agent knowledge results: {len(results)} documents found\")\n# Test crew knowledge retrieval (if exists)\nif hasattr(crew, 'knowledge') and crew.knowledge:\ncrew_results = crew.query_knowledge(test_query)\nprint(f\"Crew knowledge results: {len(crew_results)} documents found\")\nimport chromadb\nfrom crewai.utilities.paths import db_storage_path\nimport os\n# Connect to CrewAI's knowledge ChromaDB\nknowledge_path = os.path.join(db_storage_path(), \"knowledge\")\nif os.path.exists(knowledge_path):\nclient = chromadb.PersistentClient(path=knowledge_path)\ncollections = client.list_collections()\nprint(\"Knowledge Collections:\")\nfor collection in collections:\nprint(f\" - {collection.name}: {collection.count()} documents\")\n# Sample a few documents to verify content\nif collection.count() > 0:\nsample = collection.peek(limit=2)\nprint(f\" Sample content: {sample['documents'][0][:100]}...\")\nelse:\nprint(\"No knowledge storage found\")\nfrom crewai.knowledge.source.string_knowledge_source import StringKnowledgeSource\n# Create a test knowledge source\ntest_source = StringKnowledgeSource(\ncontent=\"Test knowledge content for debugging\",\nchunk_size=100, # Small chunks for testing\nchunk_overlap=20\n)\n# Check chunking behavior\nprint(f\"Original content length: {len(test_source.content)}\")\nprint(f\"Chunk size: {test_source.chunk_size}\")\nprint(f\"Chunk overlap: {test_source.chunk_overlap}\")\n# Process and inspect chunks\ntest_source.add()\nprint(f\"Number of chunks created: {len(test_source.chunks)}\")\nfor i, chunk in enumerate(test_source.chunks[:3]): # Show first 3 chunks\nprint(f\"Chunk {i+1}: {chunk[:50]}...\")\n# Ensure files are in the correct location\nfrom crewai.utilities.constants import KNOWLEDGE_DIRECTORY\nimport os\nknowledge_dir = KNOWLEDGE_DIRECTORY # Usually \"knowledge\"\nfile_path = os.path.join(knowledge_dir, \"your_file.pdf\")\nif not os.path.exists(file_path):\nprint(f\"File not found: {file_path}\")\nprint(f\"Current working directory: {os.getcwd()}\")\nprint(f\"Expected knowledge directory: {os.path.abspath(knowledge_dir)}\")\n# This happens when switching embedding providers\n# Reset knowledge storage to clear old embeddings\ncrew.reset_memories(command_type='knowledge')\n# Or use consistent embedding providers\ncrew = Crew(\nagents=[...],\ntasks=[...],\nknowledge_sources=[...],\nembedder={\"provider\": \"openai\", \"config\": {\"model\": \"text-embedding-3-small\"}}\n)\n# Fix storage permissions\nchmod -R 755 ~/.local/share/CrewAI/\n# Verify storage location consistency\nimport os\nfrom crewai.utilities.paths import db_storage_path\nprint(\"CREWAI_STORAGE_DIR:\", os.getenv(\"CREWAI_STORAGE_DIR\"))\nprint(\"Computed storage path:\", db_storage_path())\nprint(\"Knowledge path:\", os.path.join(db_storage_path(), \"knowledge\"))\n# Reset only agent-specific knowledge\ncrew.reset_memories(command_type='agent_knowledge')\n# Reset both crew and agent knowledge\ncrew.reset_memories(command_type='knowledge')\n# CLI commands\n# crewai reset-memories --agent-knowledge # Agent knowledge only\n# crewai reset-memories --knowledge # All knowledge\ncrewai reset-memories command with the\n--knowledge option.\ncrewai reset-memories --knowledge\nContent Organization\nPerformance Tips\nOne Time Knowledge\nKnowledge Management\nProduction Best Practices\nCREWAI_STORAGE_DIR to a known location in production\n\n![light logo](../assets/docs.crewai.com/docs.crewai.com-en-concepts-knowledge/71bc45159c09.webp)\n<figcaption>Figure 1. Credit: [docs.crewai.com](https://mintlify.s3.us-west-1.amazonaws.com/crewai/images/crew_only_logo.png), License: internal-copy</figcaption>\n\n![dark logo](../assets/docs.crewai.com/docs.crewai.com-en-concepts-knowledge/71bc45159c09.webp)\n<figcaption>Figure 2. Credit: [docs.crewai.com](https://mintlify.s3.us-west-1.amazonaws.com/crewai/images/crew_only_logo.png), License: internal-copy</figcaption>\n"}, {"path": "/workspace/my-app/docs/sources/index.md", "title": "index", "text": "---\ntitle: Sources\nslug: sources\nupdated_at: \"2025-08-16\"\n---\n\n# Sources\n\nIngested source pages with attribution and image credits.\n\nUse the ingestion pipeline to add more URLs in `scripts/ingest/allowlist.txt`, then run:\n\n```bash\npython3 scripts/ingest/ingest.py\n```\n\n## All sources\n\n{{ list_sources() }}"}, {"path": "/workspace/my-app/docs/sources/www.nngroup.com-articles-ai-chatbots-discourage-error-checking.md", "title": "www.nngroup.com-articles-ai-chatbots-discourage-error-checking", "text": "---\ntitle: www.nngroup.com-articles-ai-chatbots-discourage-error-checking\nslug: www.nngroup.com-articles-ai-chatbots-discourage-error-checking\ntags:\n- source\nwhy_for_designers: TODO\nbot_application: TODO\ncollaboration_prompts:\n- What retrieval strategy applies here?\nsources:\n- url: https://www.nngroup.com/articles/ai-chatbots-discourage-error-checking/\n  title: www.nngroup.com-articles-ai-chatbots-discourage-error-checking\n  author: ''\n  license: internal-copy\n  retrieved_at: '2025-08-17'\n  policy: copy\nfigures:\n- path: ../assets/www.nngroup.com/www.nngroup.com-articles-ai-chatbots-discourage-error-checking/2dd31cc60888.webp\n  caption: Figure\n  credit_name: www.nngroup.com\n  credit_url: https://media.nngroup.com/media/people/photos/pavel.jpg.256x256_q75_autocrop_crop-smart_upscale.jpg\n  license: internal-copy\n- path: ../assets/www.nngroup.com/www.nngroup.com-articles-ai-chatbots-discourage-error-checking/e738ac78664e.webp\n  caption: \"\\u274CA warning from the IEA.org chatbot will scroll off the screen soon\\\n    \\ after the user starts conversing with the bot.\"\n  credit_name: www.nngroup.com\n  credit_url: https://media.nngroup.com/media/editor/2025/05/06/ieaorg-chatbot-error.jpg\n  license: internal-copy\n- path: ../assets/www.nngroup.com/www.nngroup.com-articles-ai-chatbots-discourage-error-checking/fa220043106b.webp\n  caption: \"These types of followup questions might encourage users to think critically\\\n    \\ and question the validity of the LLM\\u2019s response.\"\n  credit_name: www.nngroup.com\n  credit_url: https://media.nngroup.com/media/editor/2025/05/06/copilot-alt-prompts.jpg\n  license: internal-copy\n- path: ../assets/www.nngroup.com/www.nngroup.com-articles-ai-chatbots-discourage-error-checking/b90b2832a514.webp\n  caption: ChatGPT allows users to ask clarifying questions related to a selected\n    portion of the generated answer by clicking the quote icon but does not actively\n    prompt users to engage with the response.\n  credit_name: www.nngroup.com\n  credit_url: https://media.nngroup.com/media/editor/2025/05/06/chatgpt-highlight-response.jpg\n  license: internal-copy\n- path: ../assets/www.nngroup.com/www.nngroup.com-articles-ai-chatbots-discourage-error-checking/54d3249df98d.webp\n  caption: Figure\n  credit_name: www.nngroup.com\n  credit_url: https://media.nngroup.com/media/videos/thumbnails/CARE_Structure_for_Crafting_AI_Prompts_Thumbnail.jpg.650x364_q75_autocrop_crop-smart_upscale.jpg\n  license: internal-copy\n- path: ../assets/www.nngroup.com/www.nngroup.com-articles-ai-chatbots-discourage-error-checking/70d44a616567.webp\n  caption: Figure\n  credit_name: www.nngroup.com\n  credit_url: https://media.nngroup.com/media/videos/thumbnails/Synthetic_AI_Users_Thumbnail.jpg.650x364_q75_autocrop_crop-smart_upscale.jpg\n  license: internal-copy\n- path: ../assets/www.nngroup.com/www.nngroup.com-articles-ai-chatbots-discourage-error-checking/abbd1f9a4631.webp\n  caption: Figure\n  credit_name: www.nngroup.com\n  credit_url: https://media.nngroup.com/media/videos/thumbnails/Your_AI_UX_Intern_Thumbnail.jpg.650x364_q75_autocrop_crop-smart_upscale.jpg\n  license: internal-copy\nupdated_at: '2025-08-17'\ncompleted: false\n---\n\n# www.nngroup.com-articles-ai-chatbots-discourage-error-checking\n\n> Synthesis: TODO\n\nLarge language models (LLMs) are being widely introduced into professional workflows through both new standalone software and integrations with existing tools. One of the most important ways that these tools increase users\u2019 productivity is through text generation. But\n**generated content is ** **prone to hallucinations** **,** with the AI extrapolating outside of its training data to return outputs that are truth-like *, *but incorrect.\nDesigners of generative AI (genAI) products must help users identify and correct these errors. Most genAI tools currently fail in this responsibility.\n## Users Struggle to Verify AI Outputs Successfully\nWhen humans produce text, they alternate between writing and editing. GenAI tools make people more productive by accelerating the pace at which they can create content \u2014 the AI will near-instantaneously generate some the text based on the prompt, and the user needs to spend time only on editing.\nSince LLMs are trained on a corpus of grammatically and syntactically correct content, their outputs may not need much copyediting. But for professional writing, editing also includes fact-checking and ensuring that any argument follows from the facts.\nThis work is not an afterthought \u2014 it is a difficult and time-consuming process. Users of genAI tools have been struggling to error-check their outputs successfully. Lawyers have been caught citing cases that don\u2019t exist. Scientists refer to hallucinated papers and journals. Doctors end up trusting erroneous AI diagnoses over their own expertise.\n**It is irresponsible to blame the users for misinformation generated by LLMs. **People are efficient (not lazy). Users adopt genAI tools precisely because they come with the promise of greater efficiency. They will not go out of their way to check the work if the effort of doing so does not seem proportional to the AI\u2019s likelihood of being wrong.\n## Reducing Errors Increases Interaction Cost\nIt\u2019s true that an exceptionally careful user can reduce the number of mistakes the LLM will produce. But doing so requires additional knowledge and user effort.\nTo verify the answers provided by a chatbot, the user will need to follow the same steps as they would when reviewing any other text, and then some.\n**Review the entire output:**Writers build up a mental model of their work in their heads as they go and maintain a sense of its structure. Editing an LLM output requires the user to understand the structure of the answer and build up that mental model from scratch \u2014 even between prompts on the same topic, as LLM outputs are nondeterministic. **Identify items that need verification:**The text will present and connect different key ideas. It is those ideas \u2014 facts and claims made on their basis \u2014 that the editor needs to pay attention to. **Validate the claims being made:**In situations where the LLM has provided a citation, the editor needs to track down the text (assuming that it exists) and find the content being cited within it. The editor also needs to determine whether the source is *authoritative*. If the LLM has not cited a source, the editor must perform research from scratch to confirm the claim as factual (or ask the LLM for supporting evidence). **Verify that the arguments follow from the claims:**The editor must separate facts and the conclusions being drawn from those facts. This often requires a significant degree of subject-matter expertise. **Articulate corrections:**If the editor is not willing to rewrite the output themselves, they must update the prompt to tell the LLM what needs changing. **Verify the response again:**Due to the limited context window and nondeterministic output of LLMs, there is no guarantee that any given error was fixed or new errors were not introduced.\nSome of these steps may be shortened with careful prompt engineering. For example, asking the LLM to generate one small and digestible part of a text at a time allows users greater control over what is written and how it is incorporated into the larger whole of the work. But these strategies are workarounds that require user effort and an in-depth understanding of how AIs hallucinate. And working through a document one part at a time is possible only for expert users who already know what those parts ought to be.\n## LLM Outputs Signal Authoritativeness\nHumans do not check every single novel piece of information for accuracy; the effort to do so would be unjustifiable. The extent to which we take a claim at its value is based on how closely it fits with what we already know and the trustworthiness of the source.\nOutputs of genAI tools mimic certain attributes that we associate with authoritative sources. Their tone is unerringly confident, regardless of the accuracy of the response. They are grammatically correct and meticulously formatted. Through a phenomenon called the halo effect, users\u2019 positive perception of one attribute of the LLM\u2019s responses causes them to have a positive predisposition about all its other attributes \u2014 including accuracy.\nWhile we ought to consider AI-generated content an early draft, due to these signals we often treat it as finished work. Users frequently have such confidence in an LLM output that they do not even read through it end to end. For example, in our recent round of intern hiring, we were disappointed to see cover letters ending in \u201cLet me know if you need anything else!\u201d\nAdditionally, nothing about the chat interface (a plain text box) acknowledges that the LLM\u2019s outputs might not be entirely accurate outside of a small label:\n*XYZ bot can make mistakes, please check responses.* Instead, chatbots typically encourage users to move on from the response by prompting them with a new question.\n## Users Are Not Building Expertise to Spot Errors\nImproving writing productivity is not the only application of genAI. These tools are often used as virtual experts, providing advice or performing tasks outside of the user\u2019s field of expertise. In practices such as vibe coding, where an AI generates code based on plain-language prompts, the user does not need to know anything about software development to create a usable program.\nA side effect of this mode of usage is that users lack a sufficient understanding of the subject matter to meaningfully validate the LLM\u2019s outputs. A vibe coder has no way of evaluating the quality of the code for security vulnerabilities, and a guidebook author might find out they gave bad advice only when customers start getting poisoned.\nBut far from all AI-generated answers are hallucinations. As genAI performance progresses to a point where the majority of LLM responses are correct, even users who started out by verifying every statement may start to feel that their effort is not paying off. As users increasingly take AI outputs at face value, they might lose the expertise necessary to evaluate the accuracy of those outputs. As a Microsoft paper by Hank Lee identifies, operators who come to rely on genAI for routine decisions end up being unprepared for situations that require their intervention.\n## A Finished Product Is Harder to Evaluate\nWhile users can choose to collaborate with an LLM section by section to create a document or a working program, they can also just ask it to produce the entire text by itself. While this is a much faster way of getting an output, it creates additional problems when it comes to verifying the output\u2019s quality and troubleshooting its constituent parts.\nSafety-systems researcher David D. Woods observed that the task of analyzing a whole is made far more difficult without having worked on the parts. The human counterpart has no context for any of the AI\u2019s decisions because they have no access to the system\u2019s \u201cthought process.\u201d To effectively correct any technical issues in the solution, the user has to effectively solve the entire problem on their own in order to reconstruct the mental model necessary for identifying flaws.\n## Designing Checkable AI Tools\nThe particulars of LLM technology may mean that, at least for the time being, hallucinations are here to stay. Responsibly engineered models should structure their outputs in a way that helps users identify these errors, by exposing the reasoning that leads to a conclusion or clearly communicating the degree of confidence for generated assertions. But the designers of genAI tools also have an opportunity to make error checking easier and more salient during interactions between users and LLMs.\n### Followup Questions that Encourage Critical Thinking\nToday\u2019s genAI chatbots often prompt followup questions; this feature could be used to embed fact-checking best practices into the tool. Rather than urging users to move on, these prompts could\n**empower them to investigate the response** with the same amount of skepticism that an editor should have for any submission. Prompts that ask for more details about sources or for the degree of certainty about the LLM\u2019s conclusion would encourage critical thinking and prevent users from taking the LLM\u2019s answer at face value.\n### Highlight Referenced Text in the Source\nSome existing chatbots can already provide citations for assertions in their responses . These citations are usually links to relevant webpages. However, users still need to do the work of verifying the accuracy of the citation: they have to click on the link, find the passage being referenced, and ensure that the context supports the meaning of the excerpt. Providing a deep link to the referenced passage or showing it in a preview of the source would greatly reduce the interaction cost of checking the LLM\u2019s assertions.\n### Make It Easy to Ask Clarifying Questions\nAnother opportunity to help the user engage more critically with the answer is to make the content more interactive. Allowing users to\n**ask clarifying questions about details of the answer** simply by clicking on selected parts of the response would make it easier for users to dig deeper into the statements being made by the LLM and leverage the AI\u2019s own knowledge to spot and resolve any hallucinations.\n### References\nAminu Abdullahi. 2025. Developers Beware: Slopsquatting & Vibe Coding Can Increase Risk of AI-Powered Attacks. (April 2025). Retrieved April 20, 2025 from https://www.techrepublic.com/article/news-slopsquatting-vibe-coding-ai-cybersecurity-risk/\nConstance Grady. 2024. The AI grift that can literally poison you. (April 2024). Retrieved April 20, 2025 from https://www.vox.com/24141648/ai-ebook-grift-mushroom-foraging-mycological-society\nDavid Woods. 1985. Cognitive technologies: The design of joint human-machine cognitive systems.\n*AI Magazine* 6, 4 (1985).\nKlaudia Ja\u017awi\u0144ska, Aisvarya Chandrasekar. 2025. AI search has a citation problem. (March 2025). Retrieved April 10, 2025 from https://www.cjr.org/tow_center/we-compared-eight-ai-search-engines-theyre-all-bad-at-citing-news.php\nHao-Ping (Hank) Lee , Advait Sarkar, Lev Tankelevitch, Ian Drosos, Sean Rintel, Richard Banks, and Nicholas Wilson. The impact of generative ai on critical thinking: self-reported reductions in cognitive effort and confidence effects from a survey of knowledge workers.\n*CHI Conference on Human Factors in Computing Systems (CHI \u201925).* ACM, New York, NY, USA.\n\n![Figure](../assets/www.nngroup.com/www.nngroup.com-articles-ai-chatbots-discourage-error-checking/2dd31cc60888.webp)\n<figcaption>Figure 1. Credit: [www.nngroup.com](https://media.nngroup.com/media/people/photos/pavel.jpg.256x256_q75_autocrop_crop-smart_upscale.jpg), License: internal-copy</figcaption>\n\n![\u274cA warning from the IEA.org chatbot will scroll off the screen soon after the user starts conversing with the bot.](../assets/www.nngroup.com/www.nngroup.com-articles-ai-chatbots-discourage-error-checking/e738ac78664e.webp)\n<figcaption>Figure 2. Credit: [www.nngroup.com](https://media.nngroup.com/media/editor/2025/05/06/ieaorg-chatbot-error.jpg), License: internal-copy</figcaption>\n\n![These types of followup questions might encourage users to think critically and question the validity of the LLM\u2019s response.](../assets/www.nngroup.com/www.nngroup.com-articles-ai-chatbots-discourage-error-checking/fa220043106b.webp)\n<figcaption>Figure 3. Credit: [www.nngroup.com](https://media.nngroup.com/media/editor/2025/05/06/copilot-alt-prompts.jpg), License: internal-copy</figcaption>\n\n![ChatGPT allows users to ask clarifying questions related to a selected portion of the generated answer by clicking the quote icon but does not actively prompt users to engage with the response.](../assets/www.nngroup.com/www.nngroup.com-articles-ai-chatbots-discourage-error-checking/b90b2832a514.webp)\n<figcaption>Figure 4. Credit: [www.nngroup.com](https://media.nngroup.com/media/editor/2025/05/06/chatgpt-highlight-response.jpg), License: internal-copy</figcaption>\n\n![Figure](../assets/www.nngroup.com/www.nngroup.com-articles-ai-chatbots-discourage-error-checking/54d3249df98d.webp)\n<figcaption>Figure 5. Credit: [www.nngroup.com](https://media.nngroup.com/media/videos/thumbnails/CARE_Structure_for_Crafting_AI_Prompts_Thumbnail.jpg.650x364_q75_autocrop_crop-smart_upscale.jpg), License: internal-copy</figcaption>\n\n![Figure](../assets/www.nngroup.com/www.nngroup.com-articles-ai-chatbots-discourage-error-checking/70d44a616567.webp)\n<figcaption>Figure 6. Credit: [www.nngroup.com](https://media.nngroup.com/media/videos/thumbnails/Synthetic_AI_Users_Thumbnail.jpg.650x364_q75_autocrop_crop-smart_upscale.jpg), License: internal-copy</figcaption>\n\n![Figure](../assets/www.nngroup.com/www.nngroup.com-articles-ai-chatbots-discourage-error-checking/abbd1f9a4631.webp)\n<figcaption>Figure 7. Credit: [www.nngroup.com](https://media.nngroup.com/media/videos/thumbnails/Your_AI_UX_Intern_Thumbnail.jpg.650x364_q75_autocrop_crop-smart_upscale.jpg), License: internal-copy</figcaption>\n"}, {"path": "/workspace/my-app/docs/sources/github.com-anthropics-anthropic-cookbook-tree-main-patterns-agents.md", "title": "github.com-anthropics-anthropic-cookbook-tree-main-patterns-agents", "text": "---\ntitle: github.com-anthropics-anthropic-cookbook-tree-main-patterns-agents\nslug: github.com-anthropics-anthropic-cookbook-tree-main-patterns-agents\ntags:\n- source\nwhy_for_designers: TODO\nbot_application: TODO\ncollaboration_prompts:\n- What retrieval strategy applies here?\nsources:\n- url: https://github.com/anthropics/anthropic-cookbook/tree/main/patterns/agents\n  title: github.com-anthropics-anthropic-cookbook-tree-main-patterns-agents\n  author: ''\n  license: internal-copy\n  retrieved_at: '2025-08-17'\n  policy: copy\nfigures: []\nupdated_at: '2025-08-17'\ncompleted: false\n---\n\n# github.com-anthropics-anthropic-cookbook-tree-main-patterns-agents\n\n> Synthesis: TODO\n\nWe read every piece of feedback, and take your input very seriously.\nTo see all available qualifiers, see our documentation.\nThere was an error while loading. Please reload this page.\n\n\n"}, {"path": "/workspace/my-app/docs/sources/python.langchain.com-docs-concepts-multimodality.md", "title": "python.langchain.com-docs-concepts-multimodality", "text": "---\ntitle: python.langchain.com-docs-concepts-multimodality\nslug: python.langchain.com-docs-concepts-multimodality\ntags:\n- source\nwhy_for_designers: TODO\nbot_application: TODO\ncollaboration_prompts:\n- What retrieval strategy applies here?\nsources:\n- url: https://python.langchain.com/docs/concepts/multimodality/\n  title: python.langchain.com-docs-concepts-multimodality\n  author: ''\n  license: internal-copy\n  retrieved_at: '2025-08-17'\n  policy: copy\nfigures: []\nupdated_at: '2025-08-17'\ncompleted: false\n---\n\n# python.langchain.com-docs-concepts-multimodality\n\n> Synthesis: TODO\n\n# Multimodality\n## Overview\n**Multimodality** refers to the ability to work with data that comes in different forms, such as text, audio, images, and video. Multimodality can appear in various components, allowing models and systems to handle and process a mix of these data types seamlessly. **Chat Models**: These could, in theory, accept and generate multimodal inputs and outputs, handling a variety of data types like text, images, audio, and video. **Embedding Models**: Embedding Models can represent multimodal content, embedding various forms of data\u2014such as text, images, and audio\u2014into vector spaces. **Vector Stores**: Vector stores could search over embeddings that represent multimodal data, enabling retrieval across different types of information.\n## Multimodality in chat models\nLangChain supports multimodal data as input to chat models:\n- Following provider-specific formats\n- Adhering to a cross-provider standard (see how-to guides for detail)\n### How to use multimodal models\n- Use the chat model integration table to identify which models support multimodality.\n- Reference the relevant how-to guides for specific examples of how to use multimodal models.\n### What kind of multimodality is supported?\n#### Inputs\nSome models can accept multimodal inputs, such as images, audio, video, or files. The types of multimodal inputs supported depend on the model provider. For instance, OpenAI, Anthropic, and Google Gemini support documents like PDFs as inputs.\nThe gist of passing multimodal inputs to a chat model is to use content blocks that specify a type and corresponding data. For example, to pass an image to a chat model as URL:\nfrom langchain_core.messages import HumanMessage\nmessage = HumanMessage(\ncontent=[\n{\"type\": \"text\", \"text\": \"Describe the weather in this image:\"},\n{\n\"type\": \"image\",\n\"source_type\": \"url\",\n\"url\": \"https://...\",\n},\n],\n)\nresponse = model.invoke([message])\nWe can also pass the image as in-line data:\nfrom langchain_core.messages import HumanMessage\nmessage = HumanMessage(\ncontent=[\n{\"type\": \"text\", \"text\": \"Describe the weather in this image:\"},\n{\n\"type\": \"image\",\n\"source_type\": \"base64\",\n\"data\": \"<base64 string>\",\n\"mime_type\": \"image/jpeg\",\n},\n],\n)\nresponse = model.invoke([message])\nTo pass a PDF file as in-line data (or URL, as supported by providers such as\nAnthropic), just change\n\"type\" to\n\"file\" and\n\"mime_type\" to\n\"application/pdf\".\nSee the how-to guides for more detail.\nMost chat models that support multimodal\n**image** inputs also accept those values in\nOpenAI's Chat Completions format:\nfrom langchain_core.messages import HumanMessage\nmessage = HumanMessage(\ncontent=[\n{\"type\": \"text\", \"text\": \"Describe the weather in this image:\"},\n{\"type\": \"image_url\", \"image_url\": {\"url\": image_url}},\n],\n)\nresponse = model.invoke([message])\nOtherwise, chat models will typically accept the native, provider-specific content block format. See chat model integrations for detail on specific providers.\n#### Outputs\nSome chat models support multimodal outputs, such as images and audio. Multimodal outputs will appear as part of the AIMessage response object. See for example:\n- Generating audio outputs with OpenAI;\n- Generating image outputs with Google Gemini.\n#### Tools\nCurrently, no chat model is designed to work\n**directly** with multimodal data in a tool call request or ToolMessage result.\nHowever, a chat model can easily interact with multimodal data by invoking tools with references (e.g., a URL) to the multimodal data, rather than the data itself. For example, any model capable of tool calling can be equipped with tools to download and process images, audio, or video.\n## Multimodality in embedding models\n**Embeddings** are vector representations of data used for tasks like similarity search and retrieval.\nThe current embedding interface used in LangChain is optimized entirely for text-based data, and will\n**not** work with multimodal data.\nAs use cases involving multimodal search and retrieval tasks become more common, we expect to expand the embedding interface to accommodate other data types like images, audio, and video.\n## Multimodality in vector stores\nVector stores are databases for storing and retrieving embeddings, which are typically used in search and retrieval tasks. Similar to embeddings, vector stores are currently optimized for text-based data.\nAs use cases involving multimodal search and retrieval tasks become more common, we expect to expand the vector store interface to accommodate other data types like images, audio, and video.\n\n\n"}, {"path": "/workspace/my-app/docs/sources/python.langchain.com-docs-concepts.md", "title": "python.langchain.com-docs-concepts", "text": "---\ntitle: python.langchain.com-docs-concepts\nslug: python.langchain.com-docs-concepts\ntags:\n- source\nwhy_for_designers: TODO\nbot_application: TODO\ncollaboration_prompts:\n- What retrieval strategy applies here?\nsources:\n- url: https://python.langchain.com/docs/concepts/\n  title: python.langchain.com-docs-concepts\n  author: ''\n  license: internal-copy\n  retrieved_at: '2025-08-17'\n  policy: copy\nfigures: []\nupdated_at: '2025-08-17'\ncompleted: false\n---\n\n# python.langchain.com-docs-concepts\n\n> Synthesis: TODO\n\n# Conceptual guide\nThis guide provides explanations of the key concepts behind the LangChain framework and AI applications more broadly.\nWe recommend that you go through at least one of the Tutorials before diving into the conceptual guide. This will provide practical context that will make it easier to understand the concepts discussed here.\nThe conceptual guide does not cover step-by-step instructions or specific implementation examples \u2014 those are found in the How-to guides and Tutorials. For detailed reference material, please see the API reference.\n## High level\n**Why LangChain?**: Overview of the value that LangChain provides. **Architecture**: How packages are organized in the LangChain ecosystem.\n## Concepts\n**Chat models**: LLMs exposed via a chat API that process sequences of messages as input and output a message. **Messages**: The unit of communication in chat models, used to represent model input and output. **Chat history**: A conversation represented as a sequence of messages, alternating between user messages and model responses. **Tools**: A function with an associated schema defining the function's name, description, and the arguments it accepts. **Tool calling**: A type of chat model API that accepts tool schemas, along with messages, as input and returns invocations of those tools as part of the output message. **Structured output**: A technique to make a chat model respond in a structured format, such as JSON that matches a given schema. **Memory**: Information about a conversation that is persisted so that it can be used in future conversations. **Multimodality**: The ability to work with data that comes in different forms, such as text, audio, images, and video. **Runnable interface**: The base abstraction that many LangChain components and the LangChain Expression Language are built on. **Streaming**: LangChain streaming APIs for surfacing results as they are generated. **LangChain Expression Language (LCEL)**: A syntax for orchestrating LangChain components. Most useful for simpler applications. **Document loaders**: Load a source as a list of documents. **Retrieval**: Information retrieval systems can retrieve structured or unstructured data from a datasource in response to a query. **Text splitters**: Split long text into smaller chunks that can be individually indexed to enable granular retrieval. **Embedding models**: Models that represent data such as text or images in a vector space. **Vector stores**: Storage of and efficient search over vectors and associated metadata. **Retriever**: A component that returns relevant documents from a knowledge base in response to a query. **Retrieval Augmented Generation (RAG)**: A technique that enhances language models by combining them with external knowledge bases. **Agents**: Use a language model to choose a sequence of actions to take. Agents can interact with external resources via tool. **Prompt templates**: Component for factoring out the static parts of a model \"prompt\" (usually a sequence of messages). Useful for serializing, versioning, and reusing these static parts. **Output parsers**: Responsible for taking the output of a model and transforming it into a more suitable format for downstream tasks. Output parsers were primarily useful prior to the general availability of tool calling and structured outputs. **Few-shot prompting**: A technique for improving model performance by providing a few examples of the task to perform in the prompt. **Example selectors**: Used to select the most relevant examples from a dataset based on a given input. Example selectors are used in few-shot prompting to select examples for a prompt. **Async programming**: The basics that one should know to use LangChain in an asynchronous context. **Callbacks**: Callbacks enable the execution of custom auxiliary code in built-in components. Callbacks are used to stream outputs from LLMs in LangChain, trace the intermediate steps of an application, and more. **Tracing**: The process of recording the steps that an application takes to go from input to output. Tracing is essential for debugging and diagnosing issues in complex applications. **Evaluation**: The process of assessing the performance and effectiveness of AI applications. This involves testing the model's responses against a set of predefined criteria or benchmarks to ensure it meets the desired quality standards and fulfills the intended purpose. This process is vital for building reliable applications. **Testing**: The process of verifying that a component of an integration or application works as expected. Testing is essential for ensuring that the application behaves correctly and that changes to the codebase do not introduce new bugs.\n## Glossary\n**AIMessageChunk**: A partial response from an AI message. Used when streaming responses from a chat model. **AIMessage**: Represents a complete response from an AI model. **astream_events**: Stream granular information from LCEL chains. **BaseTool**: The base class for all tools in LangChain. **batch**: Use to execute a runnable with batch inputs. **bind_tools**: Allows models to interact with tools. **Caching**: Storing results to avoid redundant calls to a chat model. **Chat models**: Chat models that handle multiple data modalities. **Configurable runnables**: Creating configurable Runnables. **Context window**: The maximum size of input a chat model can process. **Conversation patterns**: Common patterns in chat interactions. **Document**: LangChain's representation of a document. **Embedding models**: Models that generate vector embeddings for various data types. **HumanMessage**: Represents a message from a human user. **InjectedState**: A state injected into a tool function. **InjectedStore**: A store that can be injected into a tool for data persistence. **InjectedToolArg**: Mechanism to inject arguments into tool functions. **input and output types**: Types used for input and output in Runnables. **Integration packages**: Third-party packages that integrate with LangChain. **Integration tests**: Tests that verify the correctness of the interaction between components, usually run with access to the underlying API that powers an integration. **invoke**: A standard method to invoke a Runnable. **JSON mode**: Returning responses in JSON format. **langchain-community**: Community-driven components for LangChain. **langchain-core**: Core langchain package. Includes base interfaces and in-memory implementations. **langchain**: A package for higher level components (e.g., some pre-built chains). **langgraph**: Powerful orchestration layer for LangChain. Use to build complex pipelines and workflows. **langserve**: Used to deploy LangChain Runnables as REST endpoints. Uses FastAPI. Works primarily for LangChain Runnables, does not currently integrate with LangGraph. **LLMs (legacy)**: Older language models that take a string as input and return a string as output. **Managing chat history**: Techniques to maintain and manage the chat history. **OpenAI format**: OpenAI's message format for chat models. **Propagation of RunnableConfig**: Propagating configuration through Runnables. Read if working with python 3.9, 3.10 and async. **rate-limiting**: Client side rate limiting for chat models. **RemoveMessage**: An abstraction used to remove a message from chat history, used primarily in LangGraph. **role**: Represents the role (e.g., user, assistant) of a chat message. **RunnableConfig**: Use to pass run time information to Runnables (e.g.,\nrun_name,\nrun_id,\ntags,\nmetadata,\nmax_concurrency,\nrecursion_limit,\nconfigurable).\n**Standard parameters for chat models**: Parameters such as API key,\ntemperature, and\nmax_tokens.\n**Standard tests**: A defined set of unit and integration tests that all integrations must pass. **stream**: Use to stream output from a Runnable or a graph. **Tokenization**: The process of converting data into tokens and vice versa. **Tokens**: The basic unit that a language model reads, processes, and generates under the hood. **Tool artifacts**: Add artifacts to the output of a tool that will not be sent to the model, but will be available for downstream processing. **Tool binding**: Binding tools to models. **@tool**: Decorator for creating tools in LangChain. **Toolkits**: A collection of tools that can be used together. **ToolMessage**: Represents a message that contains the results of a tool execution. **Unit tests**: Tests that verify the correctness of individual components, run in isolation without access to the Internet. **Vector stores**: Datastores specialized for storing and efficiently searching vector embeddings. **with_structured_output**: A helper method for chat models that natively support tool calling to get structured output matching a given schema specified via Pydantic, JSON schema or a function. **with_types**: Method to overwrite the input and output types of a runnable. Useful when working with complex LCEL chains and deploying with LangServe.\n\n\n"}]