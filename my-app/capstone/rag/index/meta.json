[{"path": "/workspace/my-app/docs/sources/python.langchain.com-docs-concepts-streaming.md", "title": "python.langchain.com-docs-concepts-streaming", "text": "---\ntitle: python.langchain.com-docs-concepts-streaming\nslug: python.langchain.com-docs-concepts-streaming\ntags:\n- source\nwhy_for_designers: TODO\nbot_application: TODO\ncollaboration_prompts:\n- What retrieval strategy applies here?\nsources:\n- url: https://python.langchain.com/docs/concepts/streaming/\n  title: python.langchain.com-docs-concepts-streaming\n  author: ''\n  license: internal-copy\n  retrieved_at: '2025-08-17'\n  policy: copy\nfigures: []\nupdated_at: '2025-08-17'\ncompleted: false\n---\n\n# python.langchain.com-docs-concepts-streaming\n\n> Synthesis: TODO\n\n# Streaming\n**Streaming** is crucial for enhancing the responsiveness of applications built on LLMs. By displaying output progressively, even before a complete response is ready, streaming significantly improves user experience (UX), particularly when dealing with the latency of LLMs.\n## Overview\nGenerating full responses from LLMs often incurs a delay of several seconds, which becomes more noticeable in complex applications with multiple model calls. Fortunately, LLMs generate responses iteratively, allowing for intermediate results to be displayed as they are produced. By streaming these intermediate outputs, LangChain enables smoother UX in LLM-powered apps and offers built-in support for streaming at the core of its design.\nIn this guide, we'll discuss streaming in LLM applications and explore how LangChain's streaming APIs facilitate real-time output from various components in your application.\n## What to stream in LLM applications\nIn applications involving LLMs, several types of data can be streamed to improve user experience by reducing perceived latency and increasing transparency. These include:\n### 1. Streaming LLM outputs\nThe most common and critical data to stream is the output generated by the LLM itself. LLMs often take time to generate full responses, and by streaming the output in real-time, users can see partial results as they are produced. This provides immediate feedback and helps reduce the wait time for users.\n### 2. Streaming pipeline or workflow progress\nBeyond just streaming LLM output, it\u2019s useful to stream progress through more complex workflows or pipelines, giving users a sense of how the application is progressing overall. This could include:\n-\n**In LangGraph Workflows:**With LangGraph, workflows are composed of nodes and edges that represent various steps. Streaming here involves tracking changes to the **graph state**as individual **nodes**request updates. This allows for more granular monitoring of which node in the workflow is currently active, giving real-time updates about the status of the workflow as it progresses through different stages.\n-\n**In LCEL Pipelines:**Streaming updates from an LCEL pipeline involves capturing progress from individual **sub-runnables**. For example, as different steps or components of the pipeline execute, you can stream which sub-runnable is currently running, providing real-time insight into the overall pipeline's progress.\nStreaming pipeline or workflow progress is essential in providing users with a clear picture of where the application is in the execution process.\n### 3. Streaming custom data\nIn some cases, you may need to stream\n**custom data** that goes beyond the information provided by the pipeline or workflow structure. This custom information is injected within a specific step in the workflow, whether that step is a tool or a LangGraph node. For example, you could stream updates about what a tool is doing in real-time or the progress through a LangGraph node. This granular data, which is emitted directly from within the step, provides more detailed insights into the execution of the workflow and is especially useful in complex processes where more visibility is needed.\n## Streaming APIs\nLangChain has two main APIs for streaming output in real-time. These APIs are supported by any component that implements the Runnable Interface, including LLMs, compiled LangGraph graphs, and any Runnable generated with LCEL.\n- sync stream and async astream: Use to stream outputs from individual Runnables (e.g., a chat model) as they are generated or stream any workflow created with LangGraph.\n- The async only astream_events: Use this API to get access to custom events and intermediate outputs from LLM applications built entirely with LCEL. Note that this API is available, but not needed when working with LangGraph.\nIn addition, there is a\n**legacy** async astream_log API. This API is not recommended for new projects it is more complex and less feature-rich than the other streaming APIs.\nstream() and\nastream()\nThe\nstream() method returns an iterator that yields chunks of output synchronously as they are produced. You can use a\nfor loop to process each chunk in real-time. For example, when using an LLM, this allows the output to be streamed incrementally as it is generated, reducing the wait time for users.\nThe type of chunk yielded by the\nstream() and\nastream() methods depends on the component being streamed. For example, when streaming from an LLM each component will be an AIMessageChunk; however, for other components, the chunk may be different.\nThe\nstream() method returns an iterator that yields these chunks as they are produced. For example,\nfor chunk in component.stream(some_input):\n# IMPORTANT: Keep the processing of each chunk as efficient as possible.\n# While you're processing the current chunk, the upstream component is\n# waiting to produce the next one. For example, if working with LangGraph,\n# graph execution is paused while the current chunk is being processed.\n# In extreme cases, this could even result in timeouts (e.g., when llm outputs are\n# streamed from an API that has a timeout).\nprint(chunk)\nThe asynchronous version,\nastream(), works similarly but is designed for non-blocking workflows. You can use it in asynchronous code to achieve the same real-time streaming behavior.\n#### Usage with chat models\nWhen using\nstream() or\nastream() with chat models, the output is streamed as AIMessageChunks as it is generated by the LLM. This allows you to present or process the LLM's output incrementally as it's being produced, which is particularly useful in interactive applications or interfaces.\n#### Usage with LangGraph\nLangGraph compiled graphs are Runnables and support the standard streaming APIs.\nWhen using the\n*stream* and *astream* methods with LangGraph, you can choose **one or more** streaming mode which allow you to control the type of output that is streamed. The available streaming modes are: **\"values\"**: Emit all values of the state for each step. **\"updates\"**: Emit only the node name(s) and updates that were returned by the node(s) after each step. **\"debug\"**: Emit debug events for each step. **\"messages\"**: Emit LLM messages token-by-token. **\"custom\"**: Emit custom output written using LangGraph's StreamWriter.\nFor more information, please see:\n- LangGraph streaming conceptual guide for more information on how to stream when working with LangGraph.\n- LangGraph streaming how-to guides for specific examples of streaming in LangGraph.\n#### Usage with LCEL\nIf you compose multiple Runnables using LangChain\u2019s Expression Language (LCEL), the\nstream() and\nastream() methods will, by convention, stream the output of the last step in the chain. This allows the final processed result to be streamed incrementally.\n**LCEL** tries to optimize streaming latency in pipelines so that the streaming results from the last step are available as soon as possible.\nastream_events\nUse the\nastream_events API to access custom data and intermediate outputs from LLM applications built entirely with LCEL.\nWhile this API is available for use with LangGraph as well, it is usually not necessary when working with LangGraph, as the\nstream and\nastream methods provide comprehensive streaming capabilities for LangGraph graphs.\nFor chains constructed using\n**LCEL**, the\n.stream() method only streams the output of the final step from the chain. This might be sufficient for some applications, but as you build more complex chains of several LLM calls together, you may want to use the intermediate values of the chain alongside the final output. For example, you may want to return sources alongside the final generation when building a chat-over-documents app.\nThere are ways to do this using callbacks, or by constructing your chain in such a way that it passes intermediate\nvalues to the end with something like chained\n.assign() calls, but LangChain also includes an\n.astream_events() method that combines the flexibility of callbacks with the ergonomics of\n.stream(). When called, it returns an iterator\nwhich yields various types of events that you can filter and process according\nto the needs of your project.\nHere's one small example that prints just events containing streamed chat model output:\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_anthropic import ChatAnthropic\nmodel = ChatAnthropic(model=\"claude-3-7-sonnet-20250219\")\nprompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\nparser = StrOutputParser()\nchain = prompt | model | parser\nasync for event in chain.astream_events({\"topic\": \"parrot\"}):\nkind = event[\"event\"]\nif kind == \"on_chat_model_stream\":\nprint(event, end=\"|\", flush=True)\n**API Reference:**StrOutputParser | ChatPromptTemplate\nYou can roughly think of it as an iterator over callback events (though the format differs) - and you can use it on almost all LangChain components!\nSee this guide for more detailed information on how to use\n.astream_events(), including a table listing available events.\n## Writing custom data to the stream\nTo write custom data to the stream, you will need to choose one of the following methods based on the component you are working with:\n- LangGraph's StreamWriter can be used to write custom data that will surface through\n**stream**and **astream**APIs when working with LangGraph. **Important**this is a LangGraph feature, so it is not available when working with pure LCEL. See how to streaming custom data for more information.\n- dispatch_events / adispatch_events can be used to write custom data that will be surfaced through the\n**astream_events**API. See how to dispatch custom callback events for more information.\n## \"Auto-Streaming\" Chat Models\nLangChain simplifies streaming from chat models by automatically enabling streaming mode in certain cases, even when you\u2019re not explicitly calling the streaming methods. This is particularly useful when you use the non-streaming\ninvoke method but still want to stream the entire application, including intermediate results from the chat model.\n### How It Works\nWhen you call the\ninvoke (or\nainvoke) method on a chat model, LangChain will automatically switch to streaming mode if it detects that you are trying to stream the overall application.\nUnder the hood, it'll have\ninvoke (or\nainvoke) use the\nstream (or\nastream) method to generate its output. The result of the invocation will be the same as far as the code that was using\ninvoke is concerned; however, while the chat model is being streamed, LangChain will take care of invoking\non_llm_new_token events in LangChain's callback system. These callback events\nallow LangGraph\nstream/\nastream and\nastream_events to surface the chat model's output in real-time.\nExample:\ndef node(state):\n...\n# The code below uses the invoke method, but LangChain will\n# automatically switch to streaming mode\n# when it detects that the overall\n# application is being streamed.\nai_message = model.invoke(state[\"messages\"])\n...\nfor chunk in compiled_graph.stream(..., mode=\"messages\"):\n...\n## Async Programming\nLangChain offers both synchronous (sync) and asynchronous (async) versions of many of its methods. The async methods are typically prefixed with an \"a\" (e.g.,\nainvoke,\nastream). When writing async code, it's crucial to consistently use these asynchronous methods to ensure non-blocking behavior and optimal performance.\nIf streaming data fails to appear in real-time, please ensure that you are using the correct async methods for your workflow.\nPlease review the async programming in LangChain guide for more information on writing async code with LangChain.\n## Related Resources\nPlease see the following how-to guides for specific examples of streaming in LangChain:\n- LangGraph conceptual guide on streaming\n- LangGraph streaming how-to guides\n- How to stream runnables: This how-to guide goes over common streaming patterns with LangChain components (e.g., chat models) and with LCEL.\n- How to stream chat models\n- How to stream tool calls\nFor writing custom data to the stream, please see the following resources:\n- If using LangGraph, see how to stream custom data.\n- If using LCEL, see how to dispatch custom callback events.\n\n\n"}, {"path": "/workspace/my-app/docs/sources/www.anthropic.com-engineering-building-effective-agents.md", "title": "www.anthropic.com-engineering-building-effective-agents", "text": "---\ntitle: www.anthropic.com-engineering-building-effective-agents\nslug: www.anthropic.com-engineering-building-effective-agents\ntags:\n- source\nwhy_for_designers: TODO\nbot_application: TODO\ncollaboration_prompts:\n- What retrieval strategy applies here?\nsources:\n- url: https://www.anthropic.com/engineering/building-effective-agents\n  title: www.anthropic.com-engineering-building-effective-agents\n  author: ''\n  license: internal-copy\n  retrieved_at: '2025-08-17'\n  policy: quote\nfigures:\n- path: ../assets/www.anthropic.com/www.anthropic.com-engineering-building-effective-agents/e7b97fc35541.webp\n  caption: The augmented LLM\n  credit_name: www.anthropic.com\n  credit_url: https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fd3083d3f40bb2b6f477901cc9a240738d3dd1371-2401x1000.png&w=3840&q=75\n  license: internal-copy\n- path: ../assets/www.anthropic.com/www.anthropic.com-engineering-building-effective-agents/190994fc386e.webp\n  caption: The prompt chaining workflow\n  credit_name: www.anthropic.com\n  credit_url: https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F7418719e3dab222dccb379b8879e1dc08ad34c78-2401x1000.png&w=3840&q=75\n  license: internal-copy\n- path: ../assets/www.anthropic.com/www.anthropic.com-engineering-building-effective-agents/a74d404aaa37.webp\n  caption: The routing workflow\n  credit_name: www.anthropic.com\n  credit_url: https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F5c0c0e9fe4def0b584c04d37849941da55e5e71c-2401x1000.png&w=3840&q=75\n  license: internal-copy\n- path: ../assets/www.anthropic.com/www.anthropic.com-engineering-building-effective-agents/b7e384eb5411.webp\n  caption: The parallelization workflow\n  credit_name: www.anthropic.com\n  credit_url: https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F406bb032ca007fd1624f261af717d70e6ca86286-2401x1000.png&w=3840&q=75\n  license: internal-copy\n- path: ../assets/www.anthropic.com/www.anthropic.com-engineering-building-effective-agents/1d85998e6a68.webp\n  caption: The orchestrator-workers workflow\n  credit_name: www.anthropic.com\n  credit_url: https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F8985fc683fae4780fb34eab1365ab78c7e51bc8e-2401x1000.png&w=3840&q=75\n  license: internal-copy\n- path: ../assets/www.anthropic.com/www.anthropic.com-engineering-building-effective-agents/9c9873b4e0a9.webp\n  caption: The evaluator-optimizer workflow\n  credit_name: www.anthropic.com\n  credit_url: https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F14f51e6406ccb29e695da48b17017e899a6119c7-2401x1000.png&w=3840&q=75\n  license: internal-copy\n- path: ../assets/www.anthropic.com/www.anthropic.com-engineering-building-effective-agents/aed8cefcfb11.webp\n  caption: Autonomous agent\n  credit_name: www.anthropic.com\n  credit_url: https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F58d9f10c985c4eb5d53798dea315f7bb5ab6249e-2401x1000.png&w=3840&q=75\n  license: internal-copy\n- path: ../assets/www.anthropic.com/www.anthropic.com-engineering-building-effective-agents/2f82b99cf57a.webp\n  caption: High-level flow of a coding agent\n  credit_name: www.anthropic.com\n  credit_url: https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F4b9a1f4eb63d5962a6e1746ac26bbc857cf3474f-2400x1666.png&w=3840&q=75\n  license: internal-copy\nupdated_at: '2025-08-17'\ncompleted: false\n---\n\n# www.anthropic.com-engineering-building-effective-agents\n\n> Synthesis: TODO\n\nOver the past year, we've worked with dozens of teams building large language model (LLM) agents across industries. Consistently, the most successful implementations weren't using complex frameworks or specialized libraries. Instead, they were building with simple, composable patterns.\nIn this post, we share what we\u2019ve learned from working with our customers and building agents ourselves, and give practical advice for developers on building effective agents.\n## What are agents?\n\"Agent\" can be defined in several ways. Some customers define agents as fully autonomous systems that operate independently over extended periods, using various tools to accomplish complex tasks. Others use the term to describe more prescriptive implementations that follow predefined workflows. At Anthropic, we categorize all these variations as\n**agentic systems**, but draw an important architectural distinction between **workflows **and ** agents**: **Workflows**are systems where LLMs and tools are orchestrated through predefined code paths. **Agents**, on the other hand, are systems where LLMs dynamically direct their own processes and tool usage, maintaining control over how they accomplish tasks.\nBelow, we will explore both types of agentic systems in detail. In Appendix 1 (\u201cAgents in Practice\u201d), we describe two domains where customers have found particular value in using these kinds of systems.\n## When (and when not) to use agents\nWhen building applications with LLMs, we recommend finding the simplest solution possible, and only increasing complexity when needed. This might mean not building agentic systems at all. Agentic systems often trade latency and cost for better task performance, and you should consider when this tradeoff makes sense.\nWhen more complexity is warranted, workflows offer predictability and consistency for well-defined tasks, whereas agents are the better option when flexibility and model-driven decision-making are needed at scale. For many applications, however, optimizing single LLM calls with retrieval and in-context examples is usually enough.\n## When and how to use frameworks\nThere are many frameworks that make agentic systems easier to implement, including:\n- LangGraph from LangChain;\n- Amazon Bedrock's AI Agent framework;\n- Rivet, a drag and drop GUI LLM workflow builder; and\n- Vellum, another GUI tool for building and testing complex workflows.\nThese frameworks make it easy to get started by simplifying standard low-level tasks like calling LLMs, defining and parsing tools, and chaining calls together. However, they often create extra layers of abstraction that can obscure the underlying prompts and responses, making them harder to debug. They can also make it tempting to add complexity when a simpler setup would suffice.\nWe suggest that developers start by using LLM APIs directly: many patterns can be implemented in a few lines of code. If you do use a framework, ensure you understand the underlying code. Incorrect assumptions about what's under the hood are a common source of customer error.\nSee our cookbook for some sample implementations.\n## Building blocks, workflows, and agents\nIn this section, we\u2019ll explore the common patterns for agentic systems we\u2019ve seen in production. We'll start with our foundational building block\u2014the augmented LLM\u2014and progressively increase complexity, from simple compositional workflows to autonomous agents.\n### Building block: The augmented LLM\nThe basic building block of agentic systems is an LLM enhanced with augmentations such as retrieval, tools, and memory. Our current models can actively use these capabilities\u2014generating their own search queries, selecting appropriate tools, and determining what information to retain.\nWe recommend focusing on two key aspects of the implementation: tailoring these capabilities to your specific use case and ensuring they provide an easy, well-documented interface for your LLM. While there are many ways to implement these augmentations, one approach is through our recently released Model Context Protocol, which allows developers to integrate with a growing ecosystem of third-party tools with a simple client implementation.\nFor the remainder of this post, we'll assume each LLM call has access to these augmented capabilities.\n### Workflow: Prompt chaining\nPrompt chaining decomposes a task into a sequence of steps, where each LLM call processes the output of the previous one. You can add programmatic checks (see \"gate\u201d in the diagram below) on any intermediate steps to ensure that the process is still on track.\n**When to use this workflow:** This workflow is ideal for situations where the task can be easily and cleanly decomposed into fixed subtasks. The main goal is to trade off latency for higher accuracy, by making each LLM call an easier task. **Examples where prompt chaining is useful:**\n- Generating Marketing copy, then translating it into a different language.\n- Writing an outline of a document, checking that the outline meets certain criteria, then writing the document based on the outline.\n### Workflow: Routing\nRouting classifies an input and directs it to a specialized followup task. This workflow allows for separation of concerns, and building more specialized prompts. Without this workflow, optimizing for one kind of input can hurt performance on other inputs.\n**When to use this workflow:** Routing works well for complex tasks where there are distinct categories that are better handled separately, and where classification can be handled accurately, either by an LLM or a more traditional classification model/algorithm. **Examples where routing is useful:**\n- Directing different types of customer service queries (general questions, refund requests, technical support) into different downstream processes, prompts, and tools.\n- Routing easy/common questions to smaller models like Claude 3.5 Haiku and hard/unusual questions to more capable models like Claude 3.5 Sonnet to optimize cost and speed.\n### Workflow: Parallelization\nLLMs can sometimes work simultaneously on a task and have their outputs aggregated programmatically. This workflow, parallelization, manifests in two key variations:\n**Sectioning**: Breaking a task into independent subtasks run in parallel. **Voting:**Running the same task multiple times to get diverse outputs. **When to use this workflow:** Parallelization is effective when the divided subtasks can be parallelized for speed, or when multiple perspectives or attempts are needed for higher confidence results. For complex tasks with multiple considerations, LLMs generally perform better when each consideration is handled by a separate LLM call, allowing focused attention on each specific aspect. **Examples where parallelization is useful:** **Sectioning**:\n- Implementing guardrails where one model instance processes user queries while another screens them for inappropriate content or requests. This tends to perform better than having the same LLM call handle both guardrails and the core response.\n- Automating evals for evaluating LLM performance, where each LLM call evaluates a different aspect of the model\u2019s performance on a given prompt.\n**Voting**:\n- Reviewing a piece of code for vulnerabilities, where several different prompts review and flag the code if they find a problem.\n- Evaluating whether a given piece of content is inappropriate, with multiple prompts evaluating different aspects or requiring different vote thresholds to balance false positives and negatives.\n### Workflow: Orchestrator-workers\nIn the orchestrator-workers workflow, a central LLM dynamically breaks down tasks, delegates them to worker LLMs, and synthesizes their results.\n**When to use this workflow:** This workflow is well-suited for complex tasks where you can\u2019t predict the subtasks needed (in coding, for example, the number of files that need to be changed and the nature of the change in each file likely depend on the task). Whereas it\u2019s topographically similar, the key difference from parallelization is its flexibility\u2014subtasks aren't pre-defined, but determined by the orchestrator based on the specific input. **Example where orchestrator-workers is useful:**\n- Coding products that make complex changes to multiple files each time.\n- Search tasks that involve gathering and analyzing information from multiple sources for possible relevant information.\n### Workflow: Evaluator-optimizer\nIn the evaluator-optimizer workflow, one LLM call generates a response while another provides evaluation and feedback in a loop.\n**When to use this workflow:** This workflow is particularly effective when we have clear evaluation criteria, and when iterative refinement provides measurable value. The two signs of good fit are, first, that LLM responses can be demonstrably improved when a human articulates their feedback; and second, that the LLM can provide such feedback. This is analogous to the iterative writing process a human writer might go through when producing a polished document. **Examples where evaluator-optimizer is useful:**\n- Literary translation where there are nuances that the translator LLM might not capture initially, but where an evaluator LLM can provide useful critiques.\n- Complex search tasks that require multiple rounds of searching and analysis to gather comprehensive information, where the evaluator decides whether further searches are warranted.\n### Agents\nAgents are emerging in production as LLMs mature in key capabilities\u2014understanding complex inputs, engaging in reasoning and planning, using tools reliably, and recovering from errors. Agents begin their work with either a command from, or interactive discussion with, the human user. Once the task is clear, agents plan and operate independently, potentially returning to the human for further information or judgement. During execution, it's crucial for the agents to gain \u201cground truth\u201d from the environment at each step (such as tool call results or code execution) to assess its progress. Agents can then pause for human feedback at checkpoints or when encountering blockers. The task often terminates upon completion, but it\u2019s also common to include stopping conditions (such as a maximum number of iterations) to maintain control.\nAgents can handle sophisticated tasks, but their implementation is often straightforward. They are typically just LLMs using tools based on environmental feedback in a loop. It is therefore crucial to design toolsets and their documentation clearly and thoughtfully. We expand on best practices for tool development in Appendix 2 (\"Prompt Engineering your Tools\").\n**When to use agents:** Agents can be used for open-ended problems where it\u2019s difficult or impossible to predict the required number of steps, and where you can\u2019t hardcode a fixed path. The LLM will potentially operate for many turns, and you must have some level of trust in its decision-making. Agents' autonomy makes them ideal for scaling tasks in trusted environments.\nThe autonomous nature of agents means higher costs, and the potential for compounding errors. We recommend extensive testing in sandboxed environments, along with the appropriate guardrails.\n**Examples where agents are useful:**\nThe following examples are from our own implementations:\n- A coding Agent to resolve SWE-bench tasks, which involve edits to many files based on a task description;\n- Our \u201ccomputer use\u201d reference implementation, where Claude uses a computer to accomplish tasks.\n## Combining and customizing these patterns\nThese building blocks aren't prescriptive. They're common patterns that developers can shape and combine to fit different use cases. The key to success, as with any LLM features, is measuring performance and iterating on implementations. To repeat: you should consider adding complexity\n*only* when it demonstrably improves outcomes.\n## Summary\nSuccess in the LLM space isn't about building the most sophisticated system. It's about building the\n*right* system for your needs. Start with simple prompts, optimize them with comprehensive evaluation, and add multi-step agentic systems only when simpler solutions fall short.\nWhen implementing agents, we try to follow three core principles:\n- Maintain\n**simplicity**in your agent's design.\n- Prioritize\n**transparency**by explicitly showing the agent\u2019s planning steps.\n- Carefully craft your agent-computer interface (ACI) through thorough tool\n**documentation and testing**.\nFrameworks can help you get started quickly, but don't hesitate to reduce abstraction layers and build with basic components as you move to production. By following these principles, you can create agents that are not only powerful but also reliable, maintainable, and trusted by their users.\n### Acknowledgements\nWritten by Erik Schluntz and Barry Zhang. This work draws upon our experiences building agents at Anthropic and the valuable insights shared by our customers, for which we're deeply grateful.\n## Appendix 1: Agents in practice\nOur work with customers has revealed two particularly promising applications for AI agents that demonstrate the practical value of the patterns discussed above. Both applications illustrate how agents add the most value for tasks that require both conversation and action, have clear success criteria, enable feedback loops, and integrate meaningful human oversight.\n### A. Customer support\nCustomer support combines familiar chatbot interfaces with enhanced capabilities through tool integration. This is a natural fit for more open-ended agents because:\n- Support interactions naturally follow a conversation flow while requiring access to external information and actions;\n- Tools can be integrated to pull customer data, order history, and knowledge base articles;\n- Actions such as issuing refunds or updating tickets can be handled programmatically; and\n- Success can be clearly measured through user-defined resolutions.\nSeveral companies have demonstrated the viability of this approach through usage-based pricing models that charge only for successful resolutions, showing confidence in their agents' effectiveness.\n### B. Coding agents\nThe software development space has shown remarkable potential for LLM features, with capabilities evolving from code completion to autonomous problem-solving. Agents are particularly effective because:\n- Code solutions are verifiable through automated tests;\n- Agents can iterate on solutions using test results as feedback;\n- The problem space is well-defined and structured; and\n- Output quality can be measured objectively.\nIn our own implementation, agents can now solve real GitHub issues in the SWE-bench Verified benchmark based on the pull request description alone. However, whereas automated testing helps verify functionality, human review remains crucial for ensuring solutions align with broader system requirements.\n## Appendix 2: Prompt engineering your tools\nNo matter which agentic system you're building, tools will likely be an important part of your agent. Tools enable Claude to interact with external services and APIs by specifying their exact structure and definition in our API. When Claude responds, it will include a tool use block in the API response if it plans to invoke a tool. Tool definitions and specifications should be given just as much prompt engineering attention as your overall prompts. In this brief appendix, we describe how to prompt engineer your tools.\nThere are often several ways to specify the same action. For instance, you can specify a file edit by writing a diff, or by rewriting the entire file. For structured output, you can return code inside markdown or inside JSON. In software engineering, differences like these are cosmetic and can be converted losslessly from one to the other. However, some formats are much more difficult for an LLM to write than others. Writing a diff requires knowing how many lines are changing in the chunk header before the new code is written. Writing code inside JSON (compared to markdown) requires extra escaping of newlines and quotes.\nOur suggestions for deciding on tool formats are the following:\n- Give the model enough tokens to \"think\" before it writes itself into a corner.\n- Keep the format close to what the model has seen naturally occurring in text on the internet.\n- Make sure there's no formatting \"overhead\" such as having to keep an accurate count of thousands of lines of code, or string-escaping any code it writes.\nOne rule of thumb is to think about how much effort goes into human-computer interfaces (HCI), and plan to invest just as much effort in creating good\n*agent*-computer interfaces (ACI). Here are some thoughts on how to do so:\n- Put yourself in the model's shoes. Is it obvious how to use this tool, based on the description and parameters, or would you need to think carefully about it? If so, then it\u2019s probably also true for the model. A good tool definition often includes example usage, edge cases, input format requirements, and clear boundaries from other tools.\n- How can you change parameter names or descriptions to make things more obvious? Think of this as writing a great docstring for a junior developer on your team. This is especially important when using many similar tools.\n- Test how the model uses your tools: Run many example inputs in our workbench to see what mistakes the model makes, and iterate.\n- Poka-yoke your tools. Change the arguments so that it is harder to make mistakes.\nWhile building our agent for SWE-bench, we actually spent more time optimizing our tools than the overall prompt. For example, we found that the model would make mistakes with tools using relative filepaths after the agent had moved out of the root directory. To fix this, we changed the tool to always require absolute filepaths\u2014and we found that the model used this method flawlessly.\n\n![The augmented LLM](../assets/www.anthropic.com/www.anthropic.com-engineering-building-effective-agents/e7b97fc35541.webp)\n<figcaption>Figure 1. Credit: [www.anthropic.com](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fd3083d3f40bb2b6f477901cc9a240738d3dd1371-2401x1000.png&w=3840&q=75), License: internal-copy</figcaption>\n\n![The prompt chaining workflow](../assets/www.anthropic.com/www.anthropic.com-engineering-building-effective-agents/190994fc386e.webp)\n<figcaption>Figure 2. Credit: [www.anthropic.com](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F7418719e3dab222dccb379b8879e1dc08ad34c78-2401x1000.png&w=3840&q=75), License: internal-copy</figcaption>\n\n![The routing workflow](../assets/www.anthropic.com/www.anthropic.com-engineering-building-effective-agents/a74d404aaa37.webp)\n<figcaption>Figure 3. Credit: [www.anthropic.com](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F5c0c0e9fe4def0b584c04d37849941da55e5e71c-2401x1000.png&w=3840&q=75), License: internal-copy</figcaption>\n\n![The parallelization workflow](../assets/www.anthropic.com/www.anthropic.com-engineering-building-effective-agents/b7e384eb5411.webp)\n<figcaption>Figure 4. Credit: [www.anthropic.com](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F406bb032ca007fd1624f261af717d70e6ca86286-2401x1000.png&w=3840&q=75), License: internal-copy</figcaption>\n\n![The orchestrator-workers workflow](../assets/www.anthropic.com/www.anthropic.com-engineering-building-effective-agents/1d85998e6a68.webp)\n<figcaption>Figure 5. Credit: [www.anthropic.com](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F8985fc683fae4780fb34eab1365ab78c7e51bc8e-2401x1000.png&w=3840&q=75), License: internal-copy</figcaption>\n\n![The evaluator-optimizer workflow](../assets/www.anthropic.com/www.anthropic.com-engineering-building-effective-agents/9c9873b4e0a9.webp)\n<figcaption>Figure 6. Credit: [www.anthropic.com](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F14f51e6406ccb29e695da48b17017e899a6119c7-2401x1000.png&w=3840&q=75), License: internal-copy</figcaption>\n\n![Autonomous agent](../assets/www.anthropic.com/www.anthropic.com-engineering-building-effective-agents/aed8cefcfb11.webp)\n<figcaption>Figure 7. Credit: [www.anthropic.com](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F58d9f10c985c4eb5d53798dea315f7bb5ab6249e-2401x1000.png&w=3840&q=75), License: internal-copy</figcaption>\n\n![High-level flow of a coding agent](../assets/www.anthropic.com/www.anthropic.com-engineering-building-effective-agents/2f82b99cf57a.webp)\n<figcaption>Figure 8. Credit: [www.anthropic.com](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F4b9a1f4eb63d5962a6e1746ac26bbc857cf3474f-2400x1666.png&w=3840&q=75), License: internal-copy</figcaption>\n"}, {"path": "/workspace/my-app/docs/sources/docs.crewai.com-en-concepts-knowledge.md", "title": "docs.crewai.com-en-concepts-knowledge", "text": "---\ntitle: docs.crewai.com-en-concepts-knowledge\nslug: docs.crewai.com-en-concepts-knowledge\ntags:\n- source\nwhy_for_designers: TODO\nbot_application: TODO\ncollaboration_prompts:\n- What retrieval strategy applies here?\nsources:\n- url: https://docs.crewai.com/en/concepts/knowledge\n  title: docs.crewai.com-en-concepts-knowledge\n  author: ''\n  license: internal-copy\n  retrieved_at: '2025-08-17'\n  policy: copy\nfigures:\n- path: ../assets/docs.crewai.com/docs.crewai.com-en-concepts-knowledge/71bc45159c09.webp\n  caption: light logo\n  credit_name: docs.crewai.com\n  credit_url: https://mintlify.s3.us-west-1.amazonaws.com/crewai/images/crew_only_logo.png\n  license: internal-copy\n- path: ../assets/docs.crewai.com/docs.crewai.com-en-concepts-knowledge/71bc45159c09.webp\n  caption: dark logo\n  credit_name: docs.crewai.com\n  credit_url: https://mintlify.s3.us-west-1.amazonaws.com/crewai/images/crew_only_logo.png\n  license: internal-copy\nupdated_at: '2025-08-17'\ncompleted: false\n---\n\n# docs.crewai.com-en-concepts-knowledge\n\n> Synthesis: TODO\n\nWhat is knowledge in CrewAI and how to use it.\nknowledge directory at the root of your project.\nAlso, use relative paths from the\nknowledge directory when creating the source.\nfrom crewai import Agent, Task, Crew, Process, LLM\nfrom crewai.knowledge.source.string_knowledge_source import StringKnowledgeSource\n# Create a knowledge source\ncontent = \"Users name is John. He is 30 years old and lives in San Francisco.\"\nstring_source = StringKnowledgeSource(content=content)\n# Create an LLM with a temperature of 0 to ensure deterministic outputs\nllm = LLM(model=\"gpt-4o-mini\", temperature=0)\n# Create an agent with the knowledge store\nagent = Agent(\nrole=\"About User\",\ngoal=\"You know everything about the user.\",\nbackstory=\"You are a master at understanding people and their preferences.\",\nverbose=True,\nallow_delegation=False,\nllm=llm,\n)\ntask = Task(\ndescription=\"Answer the following questions about the user: {question}\",\nexpected_output=\"An answer to the question.\",\nagent=agent,\n)\ncrew = Crew(\nagents=[agent],\ntasks=[task],\nverbose=True,\nprocess=Process.sequential,\nknowledge_sources=[string_source], # Enable knowledge by adding the sources here\n)\nresult = crew.kickoff(inputs={\"question\": \"What city does John live in and how old is he?\"})\ndocling for the following example to work:\nuv add docling\nfrom crewai import LLM, Agent, Crew, Process, Task\nfrom crewai.knowledge.source.crew_docling_source import CrewDoclingSource\n# Create a knowledge source from web content\ncontent_source = CrewDoclingSource(\nfile_paths=[\n\"https://lilianweng.github.io/posts/2024-11-28-reward-hacking\",\n\"https://lilianweng.github.io/posts/2024-07-07-hallucination\",\n],\n)\n# Create an LLM with a temperature of 0 to ensure deterministic outputs\nllm = LLM(model=\"gpt-4o-mini\", temperature=0)\n# Create an agent with the knowledge store\nagent = Agent(\nrole=\"About papers\",\ngoal=\"You know everything about the papers.\",\nbackstory=\"You are a master at understanding papers and their content.\",\nverbose=True,\nallow_delegation=False,\nllm=llm,\n)\ntask = Task(\ndescription=\"Answer the following questions about the papers: {question}\",\nexpected_output=\"An answer to the question.\",\nagent=agent,\n)\ncrew = Crew(\nagents=[agent],\ntasks=[task],\nverbose=True,\nprocess=Process.sequential,\nknowledge_sources=[content_source],\n)\nresult = crew.kickoff(\ninputs={\"question\": \"What is the reward hacking paper about? Be sure to provide sources.\"}\n)\nfrom crewai.knowledge.source.text_file_knowledge_source import TextFileKnowledgeSource\ntext_source = TextFileKnowledgeSource(\nfile_paths=[\"document.txt\", \"another.txt\"]\n)\nfrom crewai.knowledge.source.pdf_knowledge_source import PDFKnowledgeSource\npdf_source = PDFKnowledgeSource(\nfile_paths=[\"document.pdf\", \"another.pdf\"]\n)\nfrom crewai.knowledge.source.csv_knowledge_source import CSVKnowledgeSource\ncsv_source = CSVKnowledgeSource(\nfile_paths=[\"data.csv\"]\n)\nfrom crewai.knowledge.source.excel_knowledge_source import ExcelKnowledgeSource\nexcel_source = ExcelKnowledgeSource(\nfile_paths=[\"spreadsheet.xlsx\"]\n)\nfrom crewai.knowledge.source.json_knowledge_source import JSONKnowledgeSource\njson_source = JSONKnowledgeSource(\nfile_paths=[\"data.json\"]\n)\nfrom crewai import Agent, Task, Crew\nfrom crewai.knowledge.source.string_knowledge_source import StringKnowledgeSource\n# Agent with its own knowledge - NO crew knowledge needed\nspecialist_knowledge = StringKnowledgeSource(\ncontent=\"Specialized technical information for this agent only\"\n)\nspecialist_agent = Agent(\nrole=\"Technical Specialist\",\ngoal=\"Provide technical expertise\",\nbackstory=\"Expert in specialized technical domains\",\nknowledge_sources=[specialist_knowledge] # Agent-specific knowledge\n)\ntask = Task(\ndescription=\"Answer technical questions\",\nagent=specialist_agent,\nexpected_output=\"Technical answer\"\n)\n# No crew-level knowledge required\ncrew = Crew(\nagents=[specialist_agent],\ntasks=[task]\n)\nresult = crew.kickoff() # Agent knowledge works independently\ncrew.kickoff()\ncrew.kickoff(), here\u2019s the exact sequence:\n# During kickoff\nfor agent in self.agents:\nagent.crew = self # Agent gets reference to crew\nagent.set_knowledge(crew_embedder=self.embedder) # Agent knowledge initialized\nagent.create_agent_executor()\n# Agent knowledge storage\nagent_collection_name = agent.role # e.g., \"Technical Specialist\"\n# Crew knowledge storage\ncrew_collection_name = \"crew\"\n# Both stored in same ChromaDB instance but different collections\n# Path: ~/.local/share/CrewAI/{project}/knowledge/\n# \u251c\u2500\u2500 crew/ # Crew knowledge collection\n# \u251c\u2500\u2500 Technical Specialist/ # Agent knowledge collection\n# \u2514\u2500\u2500 Another Agent Role/ # Another agent's collection\nfrom crewai import Agent, Task, Crew\nfrom crewai.knowledge.source.string_knowledge_source import StringKnowledgeSource\n# Agent-specific knowledge\nagent_knowledge = StringKnowledgeSource(\ncontent=\"Agent-specific information that only this agent needs\"\n)\nagent = Agent(\nrole=\"Specialist\",\ngoal=\"Use specialized knowledge\",\nbackstory=\"Expert with specific knowledge\",\nknowledge_sources=[agent_knowledge],\nembedder={ # Agent can have its own embedder\n\"provider\": \"openai\",\n\"config\": {\"model\": \"text-embedding-3-small\"}\n}\n)\ntask = Task(\ndescription=\"Answer using your specialized knowledge\",\nagent=agent,\nexpected_output=\"Answer based on agent knowledge\"\n)\n# No crew knowledge needed\ncrew = Crew(agents=[agent], tasks=[task])\nresult = crew.kickoff() # Works perfectly\n# Crew-wide knowledge (shared by all agents)\ncrew_knowledge = StringKnowledgeSource(\ncontent=\"Company policies and general information for all agents\"\n)\n# Agent-specific knowledge\nspecialist_knowledge = StringKnowledgeSource(\ncontent=\"Technical specifications only the specialist needs\"\n)\nspecialist = Agent(\nrole=\"Technical Specialist\",\ngoal=\"Provide technical expertise\",\nbackstory=\"Technical expert\",\nknowledge_sources=[specialist_knowledge] # Agent-specific\n)\ngeneralist = Agent(\nrole=\"General Assistant\",\ngoal=\"Provide general assistance\",\nbackstory=\"General helper\"\n# No agent-specific knowledge\n)\ncrew = Crew(\nagents=[specialist, generalist],\ntasks=[...],\nknowledge_sources=[crew_knowledge] # Crew-wide knowledge\n)\n# Result:\n# - specialist gets: crew_knowledge + specialist_knowledge\n# - generalist gets: crew_knowledge only\n# Different knowledge for different agents\nsales_knowledge = StringKnowledgeSource(content=\"Sales procedures and pricing\")\ntech_knowledge = StringKnowledgeSource(content=\"Technical documentation\")\nsupport_knowledge = StringKnowledgeSource(content=\"Support procedures\")\nsales_agent = Agent(\nrole=\"Sales Representative\",\nknowledge_sources=[sales_knowledge],\nembedder={\"provider\": \"openai\", \"config\": {\"model\": \"text-embedding-3-small\"}}\n)\ntech_agent = Agent(\nrole=\"Technical Expert\",\nknowledge_sources=[tech_knowledge],\nembedder={\"provider\": \"ollama\", \"config\": {\"model\": \"mxbai-embed-large\"}}\n)\nsupport_agent = Agent(\nrole=\"Support Specialist\",\nknowledge_sources=[support_knowledge]\n# Will use crew embedder as fallback\n)\ncrew = Crew(\nagents=[sales_agent, tech_agent, support_agent],\ntasks=[...],\nembedder={ # Fallback embedder for agents without their own\n\"provider\": \"google\",\n\"config\": {\"model\": \"text-embedding-004\"}\n}\n)\n# Each agent gets only their specific knowledge\n# Each can use different embedding providers\nfrom crewai.knowledge.knowledge_config import KnowledgeConfig\nknowledge_config = KnowledgeConfig(results_limit=10, score_threshold=0.5)\nagent = Agent(\n...\nknowledge_config=knowledge_config\n)\nresults_limit: is the number of relevant documents to return. Default is 3.\nscore_threshold: is the minimum score for a document to be considered relevant. Default is 0.35.\n~/Library/Application Support/CrewAI/{project_name}/\n\u2514\u2500\u2500 knowledge/ # Knowledge ChromaDB files\n\u251c\u2500\u2500 chroma.sqlite3 # ChromaDB metadata\n\u251c\u2500\u2500 {collection_id}/ # Vector embeddings\n\u2514\u2500\u2500 knowledge_{collection}/ # Named collections\n~/.local/share/CrewAI/{project_name}/\n\u2514\u2500\u2500 knowledge/\n\u251c\u2500\u2500 chroma.sqlite3\n\u251c\u2500\u2500 {collection_id}/\n\u2514\u2500\u2500 knowledge_{collection}/\nC:\\Users\\{username}\\AppData\\Local\\CrewAI\\{project_name}\\\n\u2514\u2500\u2500 knowledge\\\n\u251c\u2500\u2500 chroma.sqlite3\n\u251c\u2500\u2500 {collection_id}\\\n\u2514\u2500\u2500 knowledge_{collection}\\\nfrom crewai.utilities.paths import db_storage_path\nimport os\n# Get the knowledge storage path\nknowledge_path = os.path.join(db_storage_path(), \"knowledge\")\nprint(f\"Knowledge storage location: {knowledge_path}\")\n# List knowledge collections and files\nif os.path.exists(knowledge_path):\nprint(\"\\nKnowledge storage contents:\")\nfor item in os.listdir(knowledge_path):\nitem_path = os.path.join(knowledge_path, item)\nif os.path.isdir(item_path):\nprint(f\"\ud83d\udcc1 Collection: {item}/\")\n# Show collection contents\ntry:\nfor subitem in os.listdir(item_path):\nprint(f\" \u2514\u2500\u2500 {subitem}\")\nexcept PermissionError:\nprint(f\" \u2514\u2500\u2500 (permission denied)\")\nelse:\nprint(f\"\ud83d\udcc4 {item}\")\nelse:\nprint(\"No knowledge storage found yet.\")\nimport os\nfrom crewai import Crew\n# Set custom storage location for all CrewAI data\nos.environ[\"CREWAI_STORAGE_DIR\"] = \"./my_project_storage\"\n# All knowledge will now be stored in ./my_project_storage/knowledge/\ncrew = Crew(\nagents=[...],\ntasks=[...],\nknowledge_sources=[...]\n)\nfrom crewai.knowledge.storage.knowledge_storage import KnowledgeStorage\nfrom crewai.knowledge.source.string_knowledge_source import StringKnowledgeSource\n# Create custom storage with specific embedder\ncustom_storage = KnowledgeStorage(\nembedder={\n\"provider\": \"ollama\",\n\"config\": {\"model\": \"mxbai-embed-large\"}\n},\ncollection_name=\"my_custom_knowledge\"\n)\n# Use with knowledge sources\nknowledge_source = StringKnowledgeSource(\ncontent=\"Your knowledge content here\"\n)\nknowledge_source.storage = custom_storage\nimport os\nfrom pathlib import Path\n# Store knowledge in project directory\nproject_root = Path(__file__).parent\nknowledge_dir = project_root / \"knowledge_storage\"\nos.environ[\"CREWAI_STORAGE_DIR\"] = str(knowledge_dir)\n# Now all knowledge will be stored in your project directory\ntext-embedding-3-small) for knowledge storage, even when using different LLM providers. You can easily customize this to match your setup.\nfrom crewai import Agent, Crew, LLM\nfrom crewai.knowledge.source.string_knowledge_source import StringKnowledgeSource\n# When using Claude as your LLM...\nagent = Agent(\nrole=\"Researcher\",\ngoal=\"Research topics\",\nbackstory=\"Expert researcher\",\nllm=LLM(provider=\"anthropic\", model=\"claude-3-sonnet\") # Using Claude\n)\n# CrewAI will still use OpenAI embeddings by default for knowledge\n# This ensures consistency but may not match your LLM provider preference\nknowledge_source = StringKnowledgeSource(content=\"Research data...\")\ncrew = Crew(\nagents=[agent],\ntasks=[...],\nknowledge_sources=[knowledge_source]\n# Default: Uses OpenAI embeddings even with Claude LLM\n)\n# Option 1: Use Voyage AI (recommended by Anthropic for Claude users)\ncrew = Crew(\nagents=[agent],\ntasks=[...],\nknowledge_sources=[knowledge_source],\nembedder={\n\"provider\": \"voyageai\", # Recommended for Claude users\n\"config\": {\n\"api_key\": \"your-voyage-api-key\",\n\"model\": \"voyage-3\" # or \"voyage-3-large\" for best quality\n}\n}\n)\n# Option 2: Use local embeddings (no external API calls)\ncrew = Crew(\nagents=[agent],\ntasks=[...],\nknowledge_sources=[knowledge_source],\nembedder={\n\"provider\": \"ollama\",\n\"config\": {\n\"model\": \"mxbai-embed-large\",\n\"url\": \"http://localhost:11434/api/embeddings\"\n}\n}\n)\n# Option 3: Agent-level embedding customization\nagent = Agent(\nrole=\"Researcher\",\ngoal=\"Research topics\",\nbackstory=\"Expert researcher\",\nknowledge_sources=[knowledge_source],\nembedder={\n\"provider\": \"google\",\n\"config\": {\n\"model\": \"models/text-embedding-004\",\n\"api_key\": \"your-google-key\"\n}\n}\n)\nagent = Agent(\nrole=\"Researcher\",\ngoal=\"Research topics\",\nbackstory=\"Expert researcher\",\nknowledge_sources=[knowledge_source],\nembedder={\n\"provider\": \"azure\",\n\"config\": {\n\"api_key\": \"your-azure-api-key\",\n\"model\": \"text-embedding-ada-002\", # change to the model you are using and is deployed in Azure\n\"api_base\": \"https://your-azure-endpoint.openai.azure.com/\",\n\"api_version\": \"2024-02-01\"\n}\n}\n)\n_get_knowledge_search_query method is triggered\n# Original task prompt\ntask_prompt = \"Answer the following questions about the user's favorite movies: What movie did John watch last week? Format your answer in JSON.\"\n# Behind the scenes, this might be rewritten as:\nrewritten_query = \"What movies did John watch last week?\"\nfrom crewai.utilities.events import (\nKnowledgeRetrievalStartedEvent,\nKnowledgeRetrievalCompletedEvent,\n)\nfrom crewai.utilities.events.base_event_listener import BaseEventListener\nclass KnowledgeMonitorListener(BaseEventListener):\ndef setup_listeners(self, crewai_event_bus):\n@crewai_event_bus.on(KnowledgeRetrievalStartedEvent)\ndef on_knowledge_retrieval_started(source, event):\nprint(f\"Agent '{event.agent.role}' started retrieving knowledge\")\n@crewai_event_bus.on(KnowledgeRetrievalCompletedEvent)\ndef on_knowledge_retrieval_completed(source, event):\nprint(f\"Agent '{event.agent.role}' completed knowledge retrieval\")\nprint(f\"Query: {event.query}\")\nprint(f\"Retrieved {len(event.retrieved_knowledge)} knowledge chunks\")\n# Create an instance of your listener\nknowledge_monitor = KnowledgeMonitorListener()\nBaseKnowledgeSource class. Let\u2019s create a practical example that fetches and processes space news articles.\nfrom crewai import Agent, Task, Crew, Process, LLM\nfrom crewai.knowledge.source.base_knowledge_source import BaseKnowledgeSource\nimport requests\nfrom datetime import datetime\nfrom typing import Dict, Any\nfrom pydantic import BaseModel, Field\nclass SpaceNewsKnowledgeSource(BaseKnowledgeSource):\n\"\"\"Knowledge source that fetches data from Space News API.\"\"\"\napi_endpoint: str = Field(description=\"API endpoint URL\")\nlimit: int = Field(default=10, description=\"Number of articles to fetch\")\ndef load_content(self) -> Dict[Any, str]:\n\"\"\"Fetch and format space news articles.\"\"\"\ntry:\nresponse = requests.get(\nf\"{self.api_endpoint}?limit={self.limit}\"\n)\nresponse.raise_for_status()\ndata = response.json()\narticles = data.get('results', [])\nformatted_data = self.validate_content(articles)\nreturn {self.api_endpoint: formatted_data}\nexcept Exception as e:\nraise ValueError(f\"Failed to fetch space news: {str(e)}\")\ndef validate_content(self, articles: list) -> str:\n\"\"\"Format articles into readable text.\"\"\"\nformatted = \"Space News Articles:\\n\\n\"\nfor article in articles:\nformatted += f\"\"\"\nTitle: {article['title']}\nPublished: {article['published_at']}\nSummary: {article['summary']}\nNews Site: {article['news_site']}\nURL: {article['url']}\n-------------------\"\"\"\nreturn formatted\ndef add(self) -> None:\n\"\"\"Process and store the articles.\"\"\"\ncontent = self.load_content()\nfor _, text in content.items():\nchunks = self._chunk_text(text)\nself.chunks.extend(chunks)\nself._save_documents()\n# Create knowledge source\nrecent_news = SpaceNewsKnowledgeSource(\napi_endpoint=\"https://api.spaceflightnewsapi.net/v4/articles\",\nlimit=10,\n)\n# Create specialized agent\nspace_analyst = Agent(\nrole=\"Space News Analyst\",\ngoal=\"Answer questions about space news accurately and comprehensively\",\nbackstory=\"\"\"You are a space industry analyst with expertise in space exploration,\nsatellite technology, and space industry trends. You excel at answering questions\nabout space news and providing detailed, accurate information.\"\"\",\nknowledge_sources=[recent_news],\nllm=LLM(model=\"gpt-4\", temperature=0.0)\n)\n# Create task that handles user questions\nanalysis_task = Task(\ndescription=\"Answer this question about space news: {user_question}\",\nexpected_output=\"A detailed answer based on the recent space news articles\",\nagent=space_analyst\n)\n# Create and run the crew\ncrew = Crew(\nagents=[space_analyst],\ntasks=[analysis_task],\nverbose=True,\nprocess=Process.sequential\n)\n# Example usage\nresult = crew.kickoff(\ninputs={\"user_question\": \"What are the latest developments in space exploration?\"}\n)\nfrom crewai import Agent, Crew, Task\nfrom crewai.knowledge.source.string_knowledge_source import StringKnowledgeSource\nknowledge_source = StringKnowledgeSource(content=\"Test knowledge\")\nagent = Agent(\nrole=\"Test Agent\",\ngoal=\"Test knowledge\",\nbackstory=\"Testing\",\nknowledge_sources=[knowledge_source]\n)\ncrew = Crew(agents=[agent], tasks=[Task(...)])\n# Before kickoff - knowledge not initialized\nprint(f\"Before kickoff - Agent knowledge: {getattr(agent, 'knowledge', None)}\")\ncrew.kickoff()\n# After kickoff - knowledge initialized\nprint(f\"After kickoff - Agent knowledge: {agent.knowledge}\")\nprint(f\"Agent knowledge collection: {agent.knowledge.storage.collection_name}\")\nprint(f\"Number of sources: {len(agent.knowledge.sources)}\")\nimport os\nfrom crewai.utilities.paths import db_storage_path\n# Check storage structure\nstorage_path = db_storage_path()\nknowledge_path = os.path.join(storage_path, \"knowledge\")\nif os.path.exists(knowledge_path):\nprint(\"Knowledge collections found:\")\nfor collection in os.listdir(knowledge_path):\ncollection_path = os.path.join(knowledge_path, collection)\nif os.path.isdir(collection_path):\nprint(f\" - {collection}/\")\n# Show collection contents\nfor item in os.listdir(collection_path):\nprint(f\" \u2514\u2500\u2500 {item}\")\n# Test agent knowledge retrieval\nif hasattr(agent, 'knowledge') and agent.knowledge:\ntest_query = [\"test query\"]\nresults = agent.knowledge.query(test_query)\nprint(f\"Agent knowledge results: {len(results)} documents found\")\n# Test crew knowledge retrieval (if exists)\nif hasattr(crew, 'knowledge') and crew.knowledge:\ncrew_results = crew.query_knowledge(test_query)\nprint(f\"Crew knowledge results: {len(crew_results)} documents found\")\nimport chromadb\nfrom crewai.utilities.paths import db_storage_path\nimport os\n# Connect to CrewAI's knowledge ChromaDB\nknowledge_path = os.path.join(db_storage_path(), \"knowledge\")\nif os.path.exists(knowledge_path):\nclient = chromadb.PersistentClient(path=knowledge_path)\ncollections = client.list_collections()\nprint(\"Knowledge Collections:\")\nfor collection in collections:\nprint(f\" - {collection.name}: {collection.count()} documents\")\n# Sample a few documents to verify content\nif collection.count() > 0:\nsample = collection.peek(limit=2)\nprint(f\" Sample content: {sample['documents'][0][:100]}...\")\nelse:\nprint(\"No knowledge storage found\")\nfrom crewai.knowledge.source.string_knowledge_source import StringKnowledgeSource\n# Create a test knowledge source\ntest_source = StringKnowledgeSource(\ncontent=\"Test knowledge content for debugging\",\nchunk_size=100, # Small chunks for testing\nchunk_overlap=20\n)\n# Check chunking behavior\nprint(f\"Original content length: {len(test_source.content)}\")\nprint(f\"Chunk size: {test_source.chunk_size}\")\nprint(f\"Chunk overlap: {test_source.chunk_overlap}\")\n# Process and inspect chunks\ntest_source.add()\nprint(f\"Number of chunks created: {len(test_source.chunks)}\")\nfor i, chunk in enumerate(test_source.chunks[:3]): # Show first 3 chunks\nprint(f\"Chunk {i+1}: {chunk[:50]}...\")\n# Ensure files are in the correct location\nfrom crewai.utilities.constants import KNOWLEDGE_DIRECTORY\nimport os\nknowledge_dir = KNOWLEDGE_DIRECTORY # Usually \"knowledge\"\nfile_path = os.path.join(knowledge_dir, \"your_file.pdf\")\nif not os.path.exists(file_path):\nprint(f\"File not found: {file_path}\")\nprint(f\"Current working directory: {os.getcwd()}\")\nprint(f\"Expected knowledge directory: {os.path.abspath(knowledge_dir)}\")\n# This happens when switching embedding providers\n# Reset knowledge storage to clear old embeddings\ncrew.reset_memories(command_type='knowledge')\n# Or use consistent embedding providers\ncrew = Crew(\nagents=[...],\ntasks=[...],\nknowledge_sources=[...],\nembedder={\"provider\": \"openai\", \"config\": {\"model\": \"text-embedding-3-small\"}}\n)\n# Fix storage permissions\nchmod -R 755 ~/.local/share/CrewAI/\n# Verify storage location consistency\nimport os\nfrom crewai.utilities.paths import db_storage_path\nprint(\"CREWAI_STORAGE_DIR:\", os.getenv(\"CREWAI_STORAGE_DIR\"))\nprint(\"Computed storage path:\", db_storage_path())\nprint(\"Knowledge path:\", os.path.join(db_storage_path(), \"knowledge\"))\n# Reset only agent-specific knowledge\ncrew.reset_memories(command_type='agent_knowledge')\n# Reset both crew and agent knowledge\ncrew.reset_memories(command_type='knowledge')\n# CLI commands\n# crewai reset-memories --agent-knowledge # Agent knowledge only\n# crewai reset-memories --knowledge # All knowledge\ncrewai reset-memories command with the\n--knowledge option.\ncrewai reset-memories --knowledge\nContent Organization\nPerformance Tips\nOne Time Knowledge\nKnowledge Management\nProduction Best Practices\nCREWAI_STORAGE_DIR to a known location in production\n\n![light logo](../assets/docs.crewai.com/docs.crewai.com-en-concepts-knowledge/71bc45159c09.webp)\n<figcaption>Figure 1. Credit: [docs.crewai.com](https://mintlify.s3.us-west-1.amazonaws.com/crewai/images/crew_only_logo.png), License: internal-copy</figcaption>\n\n![dark logo](../assets/docs.crewai.com/docs.crewai.com-en-concepts-knowledge/71bc45159c09.webp)\n<figcaption>Figure 2. Credit: [docs.crewai.com](https://mintlify.s3.us-west-1.amazonaws.com/crewai/images/crew_only_logo.png), License: internal-copy</figcaption>\n"}, {"path": "/workspace/my-app/docs/sources/index.md", "title": "index", "text": "---\ntitle: Sources\nslug: sources\nupdated_at: \"2025-08-16\"\n---\n\n# Sources\n\nIngested source pages with attribution and image credits.\n\nUse the ingestion pipeline to add more URLs in `scripts/ingest/allowlist.txt`, then run:\n\n```bash\npython3 scripts/ingest/ingest.py\n```\n\n## All sources\n\n{{ list_sources() }}"}, {"path": "/workspace/my-app/docs/sources/github.com-anthropics-anthropic-cookbook-tree-main-patterns-agents.md", "title": "github.com-anthropics-anthropic-cookbook-tree-main-patterns-agents", "text": "---\ntitle: github.com-anthropics-anthropic-cookbook-tree-main-patterns-agents\nslug: github.com-anthropics-anthropic-cookbook-tree-main-patterns-agents\ntags:\n- source\nwhy_for_designers: TODO\nbot_application: TODO\ncollaboration_prompts:\n- What retrieval strategy applies here?\nsources:\n- url: https://github.com/anthropics/anthropic-cookbook/tree/main/patterns/agents\n  title: github.com-anthropics-anthropic-cookbook-tree-main-patterns-agents\n  author: ''\n  license: internal-copy\n  retrieved_at: '2025-08-17'\n  policy: copy\nfigures: []\nupdated_at: '2025-08-17'\ncompleted: false\n---\n\n# github.com-anthropics-anthropic-cookbook-tree-main-patterns-agents\n\n> Synthesis: TODO\n\nWe read every piece of feedback, and take your input very seriously.\nTo see all available qualifiers, see our documentation.\nThere was an error while loading. Please reload this page.\n\n\n"}, {"path": "/workspace/my-app/docs/sources/python.langchain.com-docs-concepts.md", "title": "python.langchain.com-docs-concepts", "text": "---\ntitle: python.langchain.com-docs-concepts\nslug: python.langchain.com-docs-concepts\ntags:\n- source\nwhy_for_designers: TODO\nbot_application: TODO\ncollaboration_prompts:\n- What retrieval strategy applies here?\nsources:\n- url: https://python.langchain.com/docs/concepts/\n  title: python.langchain.com-docs-concepts\n  author: ''\n  license: internal-copy\n  retrieved_at: '2025-08-17'\n  policy: copy\nfigures: []\nupdated_at: '2025-08-17'\ncompleted: false\n---\n\n# python.langchain.com-docs-concepts\n\n> Synthesis: TODO\n\n# Conceptual guide\nThis guide provides explanations of the key concepts behind the LangChain framework and AI applications more broadly.\nWe recommend that you go through at least one of the Tutorials before diving into the conceptual guide. This will provide practical context that will make it easier to understand the concepts discussed here.\nThe conceptual guide does not cover step-by-step instructions or specific implementation examples \u2014 those are found in the How-to guides and Tutorials. For detailed reference material, please see the API reference.\n## High level\n**Why LangChain?**: Overview of the value that LangChain provides. **Architecture**: How packages are organized in the LangChain ecosystem.\n## Concepts\n**Chat models**: LLMs exposed via a chat API that process sequences of messages as input and output a message. **Messages**: The unit of communication in chat models, used to represent model input and output. **Chat history**: A conversation represented as a sequence of messages, alternating between user messages and model responses. **Tools**: A function with an associated schema defining the function's name, description, and the arguments it accepts. **Tool calling**: A type of chat model API that accepts tool schemas, along with messages, as input and returns invocations of those tools as part of the output message. **Structured output**: A technique to make a chat model respond in a structured format, such as JSON that matches a given schema. **Memory**: Information about a conversation that is persisted so that it can be used in future conversations. **Multimodality**: The ability to work with data that comes in different forms, such as text, audio, images, and video. **Runnable interface**: The base abstraction that many LangChain components and the LangChain Expression Language are built on. **Streaming**: LangChain streaming APIs for surfacing results as they are generated. **LangChain Expression Language (LCEL)**: A syntax for orchestrating LangChain components. Most useful for simpler applications. **Document loaders**: Load a source as a list of documents. **Retrieval**: Information retrieval systems can retrieve structured or unstructured data from a datasource in response to a query. **Text splitters**: Split long text into smaller chunks that can be individually indexed to enable granular retrieval. **Embedding models**: Models that represent data such as text or images in a vector space. **Vector stores**: Storage of and efficient search over vectors and associated metadata. **Retriever**: A component that returns relevant documents from a knowledge base in response to a query. **Retrieval Augmented Generation (RAG)**: A technique that enhances language models by combining them with external knowledge bases. **Agents**: Use a language model to choose a sequence of actions to take. Agents can interact with external resources via tool. **Prompt templates**: Component for factoring out the static parts of a model \"prompt\" (usually a sequence of messages). Useful for serializing, versioning, and reusing these static parts. **Output parsers**: Responsible for taking the output of a model and transforming it into a more suitable format for downstream tasks. Output parsers were primarily useful prior to the general availability of tool calling and structured outputs. **Few-shot prompting**: A technique for improving model performance by providing a few examples of the task to perform in the prompt. **Example selectors**: Used to select the most relevant examples from a dataset based on a given input. Example selectors are used in few-shot prompting to select examples for a prompt. **Async programming**: The basics that one should know to use LangChain in an asynchronous context. **Callbacks**: Callbacks enable the execution of custom auxiliary code in built-in components. Callbacks are used to stream outputs from LLMs in LangChain, trace the intermediate steps of an application, and more. **Tracing**: The process of recording the steps that an application takes to go from input to output. Tracing is essential for debugging and diagnosing issues in complex applications. **Evaluation**: The process of assessing the performance and effectiveness of AI applications. This involves testing the model's responses against a set of predefined criteria or benchmarks to ensure it meets the desired quality standards and fulfills the intended purpose. This process is vital for building reliable applications. **Testing**: The process of verifying that a component of an integration or application works as expected. Testing is essential for ensuring that the application behaves correctly and that changes to the codebase do not introduce new bugs.\n## Glossary\n**AIMessageChunk**: A partial response from an AI message. Used when streaming responses from a chat model. **AIMessage**: Represents a complete response from an AI model. **astream_events**: Stream granular information from LCEL chains. **BaseTool**: The base class for all tools in LangChain. **batch**: Use to execute a runnable with batch inputs. **bind_tools**: Allows models to interact with tools. **Caching**: Storing results to avoid redundant calls to a chat model. **Chat models**: Chat models that handle multiple data modalities. **Configurable runnables**: Creating configurable Runnables. **Context window**: The maximum size of input a chat model can process. **Conversation patterns**: Common patterns in chat interactions. **Document**: LangChain's representation of a document. **Embedding models**: Models that generate vector embeddings for various data types. **HumanMessage**: Represents a message from a human user. **InjectedState**: A state injected into a tool function. **InjectedStore**: A store that can be injected into a tool for data persistence. **InjectedToolArg**: Mechanism to inject arguments into tool functions. **input and output types**: Types used for input and output in Runnables. **Integration packages**: Third-party packages that integrate with LangChain. **Integration tests**: Tests that verify the correctness of the interaction between components, usually run with access to the underlying API that powers an integration. **invoke**: A standard method to invoke a Runnable. **JSON mode**: Returning responses in JSON format. **langchain-community**: Community-driven components for LangChain. **langchain-core**: Core langchain package. Includes base interfaces and in-memory implementations. **langchain**: A package for higher level components (e.g., some pre-built chains). **langgraph**: Powerful orchestration layer for LangChain. Use to build complex pipelines and workflows. **langserve**: Used to deploy LangChain Runnables as REST endpoints. Uses FastAPI. Works primarily for LangChain Runnables, does not currently integrate with LangGraph. **LLMs (legacy)**: Older language models that take a string as input and return a string as output. **Managing chat history**: Techniques to maintain and manage the chat history. **OpenAI format**: OpenAI's message format for chat models. **Propagation of RunnableConfig**: Propagating configuration through Runnables. Read if working with python 3.9, 3.10 and async. **rate-limiting**: Client side rate limiting for chat models. **RemoveMessage**: An abstraction used to remove a message from chat history, used primarily in LangGraph. **role**: Represents the role (e.g., user, assistant) of a chat message. **RunnableConfig**: Use to pass run time information to Runnables (e.g.,\nrun_name,\nrun_id,\ntags,\nmetadata,\nmax_concurrency,\nrecursion_limit,\nconfigurable).\n**Standard parameters for chat models**: Parameters such as API key,\ntemperature, and\nmax_tokens.\n**Standard tests**: A defined set of unit and integration tests that all integrations must pass. **stream**: Use to stream output from a Runnable or a graph. **Tokenization**: The process of converting data into tokens and vice versa. **Tokens**: The basic unit that a language model reads, processes, and generates under the hood. **Tool artifacts**: Add artifacts to the output of a tool that will not be sent to the model, but will be available for downstream processing. **Tool binding**: Binding tools to models. **@tool**: Decorator for creating tools in LangChain. **Toolkits**: A collection of tools that can be used together. **ToolMessage**: Represents a message that contains the results of a tool execution. **Unit tests**: Tests that verify the correctness of individual components, run in isolation without access to the Internet. **Vector stores**: Datastores specialized for storing and efficiently searching vector embeddings. **with_structured_output**: A helper method for chat models that natively support tool calling to get structured output matching a given schema specified via Pydantic, JSON schema or a function. **with_types**: Method to overwrite the input and output types of a runnable. Useful when working with complex LCEL chains and deploying with LangServe.\n\n\n"}]